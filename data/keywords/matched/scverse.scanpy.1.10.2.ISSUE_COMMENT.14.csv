id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1030:165,modifiability,scal,scale,165,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:217,modifiability,scal,scale,217,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:165,performance,scale,scale,165,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:217,performance,scale,scale,217,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:533,usability,intuit,intuitive,533,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:551,usability,user,users,551,"Hey! So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:192,availability,error,error,192,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:388,availability,cluster,clustering,388,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:444,availability,cluster,clustering,444,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:604,availability,error,errors,604,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:65,deployability,continu,continuous,65,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:388,deployability,cluster,clustering,388,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:444,deployability,cluster,clustering,444,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:804,deployability,continu,continuous,804,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:192,performance,error,error,192,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:604,performance,error,errors,604,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:192,safety,error,error,192,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:604,safety,error,errors,604,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:192,usability,error,error,192,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:226,usability,user,user-images,226,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:604,usability,error,errors,604,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:641,usability,user,user-images,641,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:854,usability,user,user-images,854,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1029,usability,user,user-images,1029,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:528,availability,cluster,clustering,528,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:584,availability,cluster,clustering,584,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1021,availability,cluster,cluster,1021,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1038,availability,cluster,cluster,1038,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1174,availability,consist,consistent,1174,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1297,availability,cluster,clustering,1297,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:528,deployability,cluster,clustering,528,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:584,deployability,cluster,clustering,584,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1021,deployability,cluster,cluster,1021,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1038,deployability,cluster,cluster,1038,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1297,deployability,cluster,clustering,1297,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:998,integrability,sub,subclustering,998,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1034,integrability,sub,sub-cluster,1034,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1075,integrability,sub,sub,1075,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1324,performance,Perform,Performance,1324,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:241,safety,avoid,avoid,241,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:210,testability,understand,understand,210,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:969,usability,behavi,behavior,969,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1174,usability,consist,consistent,1174,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1324,usability,Perform,Performance,1324,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward? Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this. * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way. * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1319,availability,cluster,cluster,1319," data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1989,availability,cluster,clustering,1989,"arability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2046,availability,cluster,cluster,2046,"thon. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1319,deployability,cluster,cluster,1319," data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1989,deployability,cluster,clustering,1989,"arability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2046,deployability,cluster,cluster,2046,"thon. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2108,deployability,pipelin,pipelines,2108," and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2280,deployability,continu,continuous,2280," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2108,integrability,pipelin,pipelines,2108," and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1349,interoperability,standard,standard,1349,"(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the la",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2331,interoperability,compatib,compatible,2331," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:787,safety,compl,complicated,787,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2627,safety,test,test,2627," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:787,security,compl,complicated,787,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1412,security,access,access,1412,"es = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I wil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2211,security,access,accessible,2211," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:802,testability,understand,understand,802,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2627,testability,test,test,2627," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:118,usability,support,support,118,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:934,usability,learn,learning,934,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1039,usability,tool,tools,1039," I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)). colors = pd.Series(data[:,0], dtype='category'). sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `skle",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1505,usability,learn,learning,1505,"es). ```. I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1852,usability,learn,learning,1852,"se if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://kris",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1899,usability,prefer,preferred,1899,"ax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the imp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1918,usability,tool,tools,1918,"it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2200,usability,tool,tools,2200," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2384,usability,tool,tools,2384," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python. In [1]: '1' < 'a'. Out[1]: True. ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:391,availability,sli,slicing,391,"Hey @dburkhardt,. I'm definitely not perceiving this as a flame war, so no worries ;). It's good to discuss priorities like this. You are however right, that I have a strong opinion about this... and may have been a bit German/direct in how I presented it ^^. To be honest, I'm not a huge fan of the `.loc` and `.iloc` conventions in the first place and I frequently get frustrated with how slicing works in numpy as well. So I have always been quite happy with `AnnData`'s solution here. There should however be a better documentation around these decisions/conventions. My feeling is that people who are familiar with `pandas` and `numpy` conventions won't have a hard time coming to grips with the conventions in `AnnData`, but that won't necessarily be the case if the choices were switched around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:671,performance,time,time,671,"Hey @dburkhardt,. I'm definitely not perceiving this as a flame war, so no worries ;). It's good to discuss priorities like this. You are however right, that I have a strong opinion about this... and may have been a bit German/direct in how I presented it ^^. To be honest, I'm not a huge fan of the `.loc` and `.iloc` conventions in the first place and I frequently get frustrated with how slicing works in numpy as well. So I have always been quite happy with `AnnData`'s solution here. There should however be a better documentation around these decisions/conventions. My feeling is that people who are familiar with `pandas` and `numpy` conventions won't have a hard time coming to grips with the conventions in `AnnData`, but that won't necessarily be the case if the choices were switched around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:391,reliability,sli,slicing,391,"Hey @dburkhardt,. I'm definitely not perceiving this as a flame war, so no worries ;). It's good to discuss priorities like this. You are however right, that I have a strong opinion about this... and may have been a bit German/direct in how I presented it ^^. To be honest, I'm not a huge fan of the `.loc` and `.iloc` conventions in the first place and I frequently get frustrated with how slicing works in numpy as well. So I have always been quite happy with `AnnData`'s solution here. There should however be a better documentation around these decisions/conventions. My feeling is that people who are familiar with `pandas` and `numpy` conventions won't have a hard time coming to grips with the conventions in `AnnData`, but that won't necessarily be the case if the choices were switched around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:522,usability,document,documentation,522,"Hey @dburkhardt,. I'm definitely not perceiving this as a flame war, so no worries ;). It's good to discuss priorities like this. You are however right, that I have a strong opinion about this... and may have been a bit German/direct in how I presented it ^^. To be honest, I'm not a huge fan of the `.loc` and `.iloc` conventions in the first place and I frequently get frustrated with how slicing works in numpy as well. So I have always been quite happy with `AnnData`'s solution here. There should however be a better documentation around these decisions/conventions. My feeling is that people who are familiar with `pandas` and `numpy` conventions won't have a hard time coming to grips with the conventions in `AnnData`, but that won't necessarily be the case if the choices were switched around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1185,availability,cluster,clusters,1185," tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1434,availability,cluster,clustering,1434,"eat to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2650,availability,operat,operation,2650,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:766,deployability,API,APIs,766,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:810,deployability,resourc,resources,810,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1185,deployability,cluster,clusters,1185," tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1434,deployability,cluster,clustering,1434,"eat to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:810,energy efficiency,resourc,resources,810,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2393,energy efficiency,Current,Currently,2393,"want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:508,integrability,filter,filtering,508,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:766,integrability,API,APIs,766,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2063,integrability,coupl,couple,2063," / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more lik",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:766,interoperability,API,APIs,766,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:974,interoperability,incompatib,incompatibilities,974,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2774,interoperability,compatib,compatibility,2774,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3334,interoperability,compatib,compatibility,3334,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2063,modifiability,coupl,couple,2063," / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more lik",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:810,performance,resourc,resources,810,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2637,performance,perform,perform,2637,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2902,performance,time,time,2902,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2951,reliability,doe,doesn,2951,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:810,safety,resourc,resources,810,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2153,safety,valid,valid,2153," using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2159,safety,input,input,2159," numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2807,safety,prevent,prevents,2807,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:412,security,access,accessible,412,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:596,security,access,accessible,596,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1106,security,barrier,barrier,1106,"rcles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as we",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2531,security,modif,modifying,2531,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2807,security,preven,prevents,2807,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3106,security,access,access,3106,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:722,testability,understand,understand,722,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:810,testability,resourc,resources,810,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2063,testability,coupl,couple,2063," / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more lik",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:151,usability,interact,interact,151,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:190,usability,tool,tools,190,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:340,usability,learn,learning,340,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:366,usability,support,supportive,366,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:392,usability,learn,learning,392,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:454,usability,help,helper,454,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:621,usability,document,documentation,621,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:671,usability,tool,tools,671,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:828,usability,learn,learned,828,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:892,usability,help,helpful,892,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:920,usability,user,users,920,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1205,usability,intuit,intuitive,1205,"sis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1602,usability,user,users,1602,"e tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1845,usability,tool,tool,1845,"ram within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people wh",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1894,usability,document,documentation,1894,"hey can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1941,usability,clear,clear,1941,"desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1955,usability,user,users,1955," is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2159,usability,input,input,2159," numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2594,usability,tool,toolkits,2594,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2637,usability,perform,perform,2637,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2941,usability,tool,tool,2941,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2959,usability,interact,interact,2959,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3017,usability,tool,tools,3017,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3048,usability,user,users,3048,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3201,usability,tool,tools,3201,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3254,usability,experien,experience,3254,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3398,usability,help,help,3398,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3437,usability,document,documentation,3437,"learn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work? ```python. import numpy as np. import pandas as pd. import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))). adata = sc.AnnData(data). np.sqrt(adata). ```. Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1150,availability,sli,slicing,1150,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1226,availability,sli,sliced,1226,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,deployability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,deployability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,integrability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,integrability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,interoperability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,interoperability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,modifiability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,modifiability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,reliability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,reliability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1150,reliability,sli,slicing,1150,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1226,reliability,sli,sliced,1226,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,security,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,security,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:6,testability,understand,understand,6,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:62,testability,integr,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:127,testability,understand,understand,127,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:173,testability,integr,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:464,usability,experien,experienced,464,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:484,usability,user,user,484,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:866,usability,clear,clear,866,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:462,availability,sli,slicing,462,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:511,availability,sli,slicing,511,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:583,availability,sli,slicing,583,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:665,availability,sli,slicing,665,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:875,availability,error,error,875,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:900,interoperability,specif,specify,900,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:875,performance,error,error,875,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:462,reliability,sli,slicing,462,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:511,reliability,sli,slicing,511,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:583,reliability,sli,slicing,583,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:665,reliability,sli,slicing,665,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:875,safety,error,error,875,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:790,testability,simpl,simply,790,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:102,usability,minim,minimum,102,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:149,usability,minim,minimum,149,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:790,usability,simpl,simply,790,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:875,usability,error,error,875,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:892,usability,user,user,892,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:303,availability,sli,slicing,303,"So the question is just about passing `adata` or `adata.X` then? This doesn't sound like a particularly difficult problem to solve and wouldn't require structural changes in `AnnData`. Indeed one could check whether an index is in `.obs_names` or `.var_names`. @ivirshup is just busy trying to speed up slicing in `AnnData` though, so maybe he can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:70,reliability,doe,doesn,70,"So the question is just about passing `adata` or `adata.X` then? This doesn't sound like a particularly difficult problem to solve and wouldn't require structural changes in `AnnData`. Indeed one could check whether an index is in `.obs_names` or `.var_names`. @ivirshup is just busy trying to speed up slicing in `AnnData` though, so maybe he can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:303,reliability,sli,slicing,303,"So the question is just about passing `adata` or `adata.X` then? This doesn't sound like a particularly difficult problem to solve and wouldn't require structural changes in `AnnData`. Indeed one could check whether an index is in `.obs_names` or `.var_names`. @ivirshup is just busy trying to speed up slicing in `AnnData` though, so maybe he can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,deployability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,integrability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,interoperability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,modifiability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,reliability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,security,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:114,testability,integr,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:429,testability,simpl,simply,429,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:216,usability,person,personal,216,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:429,usability,simpl,simply,429,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:8,deployability,fail,fail,8,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:350,modifiability,scal,scaling,350,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:8,reliability,fail,fail,8,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:236,testability,simpl,simpler,236,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:106,usability,person,person,106,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:236,usability,simpl,simpler,236,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:909,availability,cluster,cluster,909,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1877,availability,consist,consistent,1877,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2041,availability,sli,slicing,2041,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2082,availability,slo,slot,2082,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:111,deployability,API,API,111,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:909,deployability,cluster,cluster,909,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1888,deployability,API,API,1888,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:111,integrability,API,API,111,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1888,integrability,API,API,1888,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:111,interoperability,API,API,111,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:153,interoperability,incompatib,incompatibility,153,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:836,interoperability,compatib,compatible,836,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1888,interoperability,API,API,1888,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:310,modifiability,pac,packages,310,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1716,modifiability,scal,scaling,1716,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:365,reliability,doe,doesn,365,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:707,reliability,doe,doesn,707,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2041,reliability,sli,slicing,2041,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2082,reliability,slo,slot,2082,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1058,safety,input,input,1058,"ork with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:242,testability,understand,understand,242,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:788,testability,simpl,simple,788,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:180,usability,tool,tools,180,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:418,usability,tool,tools,418,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:693,usability,document,documentation,693,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:788,usability,simpl,simple,788,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:820,usability,tool,tools,820,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:938,usability,Support,Support,938,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conven",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:997,usability,Support,Support,997,"o I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conveni",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1058,usability,input,input,1058,"ork with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1877,usability,consist,consistent,1877,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1896,usability,support,supporting,1896,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2009,usability,efficien,efficient,2009,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2120,usability,help,helpful,2120,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:. 1. Return cluster labels as `ints`. 2. Support non-string indexes (and adopt `loc` vs `iloc`). 3. Support `ufuncs` with `AnnData`. 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:301,availability,cluster,clusters,301,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:550,availability,cluster,clustering,550,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:651,availability,cluster,cluster,651,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:301,deployability,cluster,clusters,301,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:550,deployability,cluster,clustering,550,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:651,deployability,cluster,cluster,651,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1713,deployability,updat,update,1713,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5,energy efficiency,cool,cool,5,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2330,energy efficiency,predict,predict,2330,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2482,energy efficiency,core,core,2482,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2238,integrability,transform,transformers,2238,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2238,interoperability,transform,transformers,2238,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2112,modifiability,layer,layers,2112,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2138,modifiability,layer,layers,2138,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2546,performance,perform,performance,2546,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:728,reliability,Doe,Doesn,728,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1614,reliability,doe,does,1614,"this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1713,safety,updat,update,1713,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2330,safety,predict,predict,2330,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2439,safety,input,input,2439,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1713,security,updat,update,1713,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:321,usability,intuit,intuitive,321,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:743,usability,learn,learn,743,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1234,usability,Support,Support,1234,"s for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1417,usability,person,personally,1417,"compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1580,usability,Support,Support,1580,"e scanpy and sklearn and I want this to ""just work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned cop",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2439,usability,input,input,2439,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2546,usability,perform,performance,2546,"t work"". > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories? <details>. <summary> Example of sklearn working with string categories </summary>. ```python. from sklearn import metrics. import numpy as np. from string import ascii_letters. x = np.random.randint(0, 10, 50). y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y). ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels. > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X? I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python. adata.apply_ufunc(np.log1p, in=""X"", out=""X""). adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")). ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python. clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""). clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""). ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:311,availability,error,errors,311,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:478,availability,operat,operators,478,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:632,availability,cluster,cluster,632,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:736,availability,error,errors,736,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1806,availability,Cluster,Clusters,1806,"r_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2201,availability,cluster,clusters,2201," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2302,availability,error,error,2302," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2394,availability,Cluster,Clusters,2394," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2558,availability,cluster,cluster,2558," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2566,availability,operat,operators,2566," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2698,availability,cluster,cluster,2698," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2779,availability,cluster,cluster,2779," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2862,availability,cluster,clusters,2862," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2902,availability,operat,operators,2902," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:64,deployability,log,logging,64,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:632,deployability,cluster,cluster,632,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1051,deployability,updat,update,1051,"I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1806,deployability,Cluster,Clusters,1806,"r_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2201,deployability,cluster,clusters,2201," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2394,deployability,Cluster,Clusters,2394," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2558,deployability,cluster,cluster,2558," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2698,deployability,cluster,cluster,2698," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2779,deployability,cluster,cluster,2779," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2862,deployability,cluster,clusters,2862," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2427,integrability,sub,subclustering,2427," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2796,integrability,sub,subclusters,2796," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:617,modifiability,deco,decomposition,617,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:847,modifiability,deco,decomposition,847,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2217,modifiability,paramet,parameter,2217," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:311,performance,error,errors,311,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:736,performance,error,errors,736,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1124,performance,perform,performance,1124,"bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2302,performance,error,error,2302," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1662,reliability,doe,doesn,1662,"size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:64,safety,log,logging,64,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:311,safety,error,errors,311,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:736,safety,error,errors,736,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1051,safety,updat,update,1051,"I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2302,safety,error,error,2302," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:64,security,log,logging,64,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1051,security,updat,update,1051,"I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1217,security,modif,modifying,1217,"I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` param",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:64,testability,log,logging,64,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2446,testability,understand,understand,2446," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:255,usability,person,personally,255,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:311,usability,error,errors,311,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:736,usability,error,errors,736,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1124,usability,perform,performance,1124,"bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python. import scanpy as sc. import numpy as np. import pandas as pd. import matplotlib.pyplot as plt. from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)). adata = sc.AnnData(data). # All of the following raise errors. np.sqrt(adata). adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata). ```. To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2089,usability,user,user-images,2089," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2302,usability,error,error,2302," if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. . **2. Requirement to use .var_vector or .obs_vector for single columns**. ```python. # This works as expected. adata[:, adata.var_names[0:3]]. # I wish this did as well. adata[:, adata.var_names[0]]. ```. **3. .var_vector doesn't return a Series**. ```python. pdata = pd.DataFrame(data). # Returns series. pdata[0]. # Returns ndarray. adata.var_vector[0]. ```. **4. Clusters as categories creates confusing scatterplots**. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]). ```. Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**. I would like this to just work. Instead it throws a huge error. ```python. plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']). ```. **6. Clusters as categories frustrate subclustering**. I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators. ```python. sc.pp.neighbors(adata). sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']. sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with . # new names that don't clash with the existing clusters. # However, np.max() and the + operators aren't well defined for . # cateogricals of strings. sub_clusters = sub_clusters + np.max(adata.obs['leiden']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:237,deployability,contain,container,237,"@ivirshup I'd be interested to hear more about the differences between DataFrames and xarrays. The website for xarray makes it sound like they're very similar. >Dataset is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray to the pandas.DataFrame. Is there a dichotomy here? What differentiates AnnData from Datasets?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:296,interoperability,share,shared,296,"@ivirshup I'd be interested to hear more about the differences between DataFrames and xarrays. The website for xarray makes it sound like they're very similar. >Dataset is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray to the pandas.DataFrame. Is there a dichotomy here? What differentiates AnnData from Datasets?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:196,performance,memor,memory,196,"@ivirshup I'd be interested to hear more about the differences between DataFrames and xarrays. The website for xarray makes it sound like they're very similar. >Dataset is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray to the pandas.DataFrame. Is there a dichotomy here? What differentiates AnnData from Datasets?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:196,usability,memor,memory,196,"@ivirshup I'd be interested to hear more about the differences between DataFrames and xarrays. The website for xarray makes it sound like they're very similar. >Dataset is a multi-dimensional, in-memory array database. It is a dict-like container of DataArray objects aligned along any number of shared dimensions, and serves a similar purpose in xarray to the pandas.DataFrame. Is there a dichotomy here? What differentiates AnnData from Datasets?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:52,availability,operat,operators,52,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:690,availability,error,error,690,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1581,availability,state,statements,1581,"python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2708,availability,Cluster,Clusters,2708,"rent evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2991,availability,cluster,clusters,2991,"fferent shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3519,availability,Cluster,Clusters,3519,"ly wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3698,availability,cluster,cluster,3698,"## 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3716,availability,cluster,clusters,3716,"categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4027,availability,cluster,cluster,4027,"Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Functio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4289,availability,cluster,cluster,4289,"or matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstanc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4352,availability,cluster,clustering,4352,"you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5049,availability,cluster,clustering,5049,"ter(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5147,availability,cluster,clustering,5147,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5181,availability,cluster,clusters,5181,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5210,availability,cluster,clustering,5210,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5261,availability,cluster,clustering,5261,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5652,availability,cluster,cluster,5652,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:6003,availability,cluster,cluster,6003,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1117,deployability,updat,update,1117,"ent if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. H",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1258,deployability,updat,update,1258,"ut data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2708,deployability,Cluster,Clusters,2708,"rent evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2991,deployability,cluster,clusters,2991,"fferent shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3444,deployability,continu,continuous,3444,"s as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".j",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3519,deployability,Cluster,Clusters,3519,"ly wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3698,deployability,cluster,cluster,3698,"## 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3716,deployability,cluster,clusters,3716,"categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4027,deployability,cluster,cluster,4027,"Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Functio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4289,deployability,cluster,cluster,4289,"or matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstanc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4352,deployability,cluster,clustering,4352,"you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5049,deployability,cluster,clustering,5049,"ter(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5063,deployability,observ,observations,5063,"_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5147,deployability,cluster,clustering,5147,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5181,deployability,cluster,clusters,5181,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5210,deployability,cluster,clustering,5210,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5261,deployability,cluster,clustering,5261,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5652,deployability,cluster,cluster,5652,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:6003,deployability,cluster,cluster,6003,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3336,energy efficiency,current,current,3336,"more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:354,integrability,wrap,wrapping,354,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:375,integrability,transform,transformers,375,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:798,integrability,sub,subtracted,798,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1581,integrability,state,statements,1581,"python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3552,integrability,sub,subclustering,3552,"gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3605,integrability,sub,subclustering,3605,"as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.ob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4215,integrability,sub,subset,4215,"s[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clusteri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4330,integrability,sub,subset,4330,"ir current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4603,integrability,sub,subset,4603,"ubclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(su",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:4877,integrability,sub,subcluster,4877,", I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5193,integrability,sub,subset,5193,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5393,integrability,sub,subset,5393,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5498,integrability,sub,subset,5498,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5809,integrability,sub,subset,5809,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5980,integrability,sub,subcluster,5980,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:375,interoperability,transform,transformers,375,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3005,modifiability,paramet,parameter,3005,"rom `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. fro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:690,performance,error,error,690,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1190,performance,perform,performance,1190,"rked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:269,reliability,doe,does,269,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2396,reliability,doe,doesn,2396,"ve for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (reg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:258,safety,input,input,258,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:690,safety,error,error,690,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1117,safety,updat,update,1117,"ent if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. H",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1258,safety,updat,update,1258,"ut data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1341,safety,prevent,prevent,1341," of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2423,safety,reme,remeber,2423,"single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's orde",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1117,security,updat,update,1117,"ent if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. H",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1258,security,updat,update,1258,"ut data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1341,security,preven,prevent,1341," of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1363,security,modif,modification,1363,"learn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2110,testability,context,context,2110,"d ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_na",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2358,testability,context,context,2358," modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5063,testability,observ,observations,5063,"_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:5567,testability,assert,assert,5567,"r) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering. . new_clustering = adata.obs[orig_key].copy(). # Make new names. new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str). new_clustering.cat.add_categories(new_clusters, inplace=True). new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata. adata.obs[key_added] = new_clustering. ```. <details>. <summary> Or if you wanted something more generic: </summary>. ```python. from typing import Callable, Collection. from anndata import AnnData. def subcluster(. cluster_func: Callable[[AnnData], Collection],. adata,. orig_key,. orig_clusters,. key_added,. ):. """""". Params. ------. cluster_func. Function that produces a clustering of observations from an anndata object. adata. orig_key. Key in adata.obs for original clustering. orig_clusters. Set of clusters to subset to before clustering. key_added. Key in obs to add resulting clustering to. """""". if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = (. pd.Categorical(cluster_func(subset)). .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)). ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment. new_clustering = (. adata.obs[orig_key].astype(""category"", copy=True). .cat.add_categories(sub_clustering.categories). ). new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata. adata.obs[key_added] = new_clustering. . from functools import partial. subcluster_kmeans = partial(. subcluster,. lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X). ). ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:258,usability,input,input,258,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:690,usability,error,error,690,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1190,usability,perform,performance,1190,"rked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python. adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python. adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,usability,behavi,behaviour,1921,"ar_names[3:6]].X. ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional? This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2909,usability,intuit,intuitive,2909,"be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:3344,usability,behavi,behaviour,3344,"t in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`? ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these? ```python. sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns. sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]). ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? Basically, I'd say do something more like:. ```python. from collections.abc import Iterable. import numpy as np. import pandas as pd. from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):. if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):. orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]. sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1827,availability,error,errors,1827,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,deployability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:322,integrability,event,eventually,322,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:531,integrability,coupl,couple,531,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:840,integrability,interfac,interface,840,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,integrability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:840,interoperability,interfac,interface,840,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1663,interoperability,specif,specific,1663,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,interoperability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:531,modifiability,coupl,couple,531,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:840,modifiability,interfac,interface,840,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,modifiability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1827,performance,error,errors,1827,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:408,reliability,pra,practice,408,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1223,reliability,doe,doesn,1223,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1449,reliability,doe,doesn,1449,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,reliability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1827,safety,error,errors,1827,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,security,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:531,testability,coupl,couple,531,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1921,testability,integr,integrating,1921,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:477,usability,support,support,477,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:640,usability,Efficien,Efficient,640,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1106,usability,support,support,1106,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1163,usability,support,supported,1163,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1311,usability,support,support,1311,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1330,usability,usab,usable,1330,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1381,usability,behavi,behaviour,1381,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1613,usability,behavi,behaviour,1613,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1672,usability,behavi,behaviour,1672,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1827,usability,error,errors,1827,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do. * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:154,availability,cluster,clustering,154,"> I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? I'm not so familiar with the scanpy tutorials, but I do show sub-clustering in the single-cell-tutorial notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:154,deployability,cluster,clustering,154,"> I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? I'm not so familiar with the scanpy tutorials, but I do show sub-clustering in the single-cell-tutorial notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:150,integrability,sub,sub-clustering,150,"> I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD? I'm not so familiar with the scanpy tutorials, but I do show sub-clustering in the single-cell-tutorial notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:198,availability,operat,operators,198,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:494,availability,error,error,494,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:978,availability,sli,slicing,978,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1012,availability,sli,slicing,1012,"ong response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1040,availability,cluster,clusters,1040,"1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1713,availability,down,down,1713,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1040,deployability,cluster,clusters,1040,"1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1104,deployability,contain,container,1104,"return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing fu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1132,deployability,contain,container,1132," the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1474,deployability,api,apis,1474,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1726,deployability,infrastructur,infrastructure,1726,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1895,energy efficiency,model,model,1895,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1079,integrability,transform,transformation,1079,"ata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1160,integrability,transform,transformation,1160,"Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1474,integrability,api,apis,1474,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1079,interoperability,transform,transformation,1079,"ata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1160,interoperability,transform,transformation,1160,"Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1474,interoperability,api,apis,1474,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1694,modifiability,concern,concern,1694,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:494,performance,error,error,494,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1346,performance,overhead,overhead,1346,"`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:978,reliability,sli,slicing,978,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1012,reliability,sli,slicing,1012,"ong response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:494,safety,error,error,494,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:581,security,access,access,581,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:695,security,access,access,695,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1895,security,model,model,1895,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2349,security,access,accessible,2349,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:620,testability,understand,understand,620,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:1694,testability,concern,concern,1694,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:174,usability,support,support,174,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:494,usability,error,error,494,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:901,usability,workflow,workflow,901,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:933,usability,learn,learning,933,"Thanks for the long response @ivirshup! For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2000,usability,feedback,feedback,2000,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2071,usability,tool,tools,2071,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2264,usability,support,supportive,2264,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2313,usability,tool,tools,2313,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1030:2400,usability,user,users,2400,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030
https://github.com/scverse/scanpy/issues/1032:72,integrability,filter,filter,72,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:171,integrability,batch,batch,171,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:43,interoperability,specif,specify,43,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:185,interoperability,specif,specified,185,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:171,performance,batch,batch,171,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:251,performance,time,time,251,"I think the solution here is to be able to specify which key is used to filter features on. @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:269,integrability,batch,batch,269,Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:269,performance,batch,batch,269,Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:547,availability,down,downstream,547,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:454,deployability,contain,contains,454,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:271,integrability,batch,batch,271,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:271,performance,batch,batch,271,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:624,usability,user,user,624,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:641,usability,minim,minimum,641,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1366,availability,error,error,1366,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2000,availability,error,error,2000,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2124,availability,error,error,2124,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:539,deployability,contain,contains,539,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:954,deployability,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:338,integrability,filter,filtering,338,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:608,integrability,batch,batches,608,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:697,integrability,filter,filters,697,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:773,integrability,batch,batches,773,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:954,integrability,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1572,integrability,sub,subset,1572,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:487,interoperability,specif,specifies,487,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:552,modifiability,variab,variable,552,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:637,modifiability,variab,variable,637,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:954,modifiability,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1452,modifiability,variab,variable,1452,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1589,modifiability,variab,variable,1589,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:608,performance,batch,batches,608,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:773,performance,batch,batches,773,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1366,performance,error,error,1366,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2000,performance,error,error,2000,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2124,performance,error,error,2124,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:625,safety,detect,detected,625,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:922,safety,avoid,avoid,922,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:954,safety,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1366,safety,error,error,1366,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2000,safety,error,error,2000,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2124,safety,error,error,2124,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:625,security,detect,detected,625,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1431,security,ident,identified,1431,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2153,security,ident,identified,2153,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:954,testability,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:482,usability,user,user,482,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:911,usability,person,personally,911,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:1366,usability,error,error,1366,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2000,usability,error,error,2000,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:2124,usability,error,error,2124,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(). sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""). adata.var.highly_variable = adata.var.highly_variable_intersection. sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below. ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:74,integrability,filter,filter,74,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:261,integrability,batch,batch,261,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:45,interoperability,specif,specify,45,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:275,interoperability,specif,specified,275,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:261,performance,batch,batch,261,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:341,performance,time,time,341,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:441,reliability,doe,does,441,"> I think the solution here is to be able to specify which key is used to filter features on. > . Yeah, that might also work, but might also be too much flexibility, I'm not sure. > @gokceneraslan, what is `adata.var[""highly_variable""]` supposed to mean if the batch key was specified has been run? I've checked with a few datasets and each time all the values were false. Hmm, can you make a reproducible example? This should be a bug. How does the other fields like adata.var[""highly_variable_nbatch""] and adata.var[""highly_variable_intersection""] look? Maybe a separate issue would be a better place to discuss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:288,availability,avail,available,288,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:191,deployability,build,building,191,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,deployability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,integrability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,interoperability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,modifiability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:59,performance,content,contents,59,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:151,reliability,doe,does,151,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,reliability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:288,reliability,availab,available,288,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:288,safety,avail,available,288,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,security,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:288,security,availab,available,288,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:237,testability,integr,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:348,integrability,batch,batch,348,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:348,performance,batch,batch,348,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:33,usability,clear,clear,33,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:304,usability,document,documentation,304,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:324,integrability,batch,batch,324,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:384,integrability,batch,batch,384,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:449,integrability,batch,batch,449,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:474,integrability,batch,batch,474,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:539,integrability,batch,batch,539,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:324,performance,batch,batch,324,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:384,performance,batch,batch,384,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:449,performance,batch,batch,449,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:474,performance,batch,batch,474,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:539,performance,batch,batch,539,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:182,testability,assert,assert,182,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:393,testability,assert,assert,393,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:548,testability,assert,assert,548,"@gokceneraslan here's a quick example:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(). ```. Alternatively:. ```python. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). pbmc.obs[""batch""] = ""a"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a"". pbmc.obs[""batch""][::2] = ""b"". sc.pp.highly_variable_genes(pbmc, batch_key=""batch""). assert not pbmc.var[""highly_variable""].any(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:792,availability,sli,slightly,792,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,deployability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:377,integrability,batch,batch,377,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,integrability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,interoperability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,modifiability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:377,performance,batch,batch,377,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:792,reliability,sli,slightly,792,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,reliability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,security,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:884,testability,integr,integration,884,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:35,usability,clear,clear,35,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:333,usability,document,documentation,333,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:750,usability,hint,hint,750,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? > . > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031). > . > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:268,safety,test,tested,268,"> @gokceneraslan here's a quick example:. Oh man, just noticed a horrible bug which leads to zero HVGs if batch_key is given but n_top_genes is not  Somehow, highly_variable_genes with batch_key but without n_top_genes (which is the option I always use :) ) is never tested :/ Fixing now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1032:268,testability,test,tested,268,"> @gokceneraslan here's a quick example:. Oh man, just noticed a horrible bug which leads to zero HVGs if batch_key is given but n_top_genes is not  Somehow, highly_variable_genes with batch_key but without n_top_genes (which is the option I always use :) ) is never tested :/ Fixing now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032
https://github.com/scverse/scanpy/issues/1033:69,modifiability,exten,extended,69,"In lines 345 - 348, a list of dtypes was getting appended instead of extended. Fixed in PR #1070 . ```. dtypes.append([. ('highly_variable_nbatches', int),. ('highly_variable_intersection', np.bool_),. ]). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033
https://github.com/scverse/scanpy/issues/1033:19,safety,test,test,19,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here? Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033
https://github.com/scverse/scanpy/issues/1033:19,testability,test,test,19,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here? Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033
https://github.com/scverse/scanpy/issues/1033:40,testability,simpl,simple,40,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here? Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033
https://github.com/scverse/scanpy/issues/1033:40,usability,simpl,simple,40,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here? Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033
https://github.com/scverse/scanpy/issues/1035:56,availability,cluster,cluster,56,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:56,deployability,cluster,cluster,56,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1234,deployability,log,log-normalized,1234,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1279,deployability,scale,scaled,1279,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1413,deployability,log,log-normalization,1413,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1279,energy efficiency,scale,scaled,1279,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:25,integrability,sub,subsetting,25,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1259,integrability,batch,batch,1259,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1370,interoperability,distribut,distributed,1370,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1279,modifiability,scal,scaled,1279,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1259,performance,batch,batch,1259,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1279,performance,scale,scaled,1279,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:184,safety,test,testing,184,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:392,safety,test,test,392,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1019,safety,test,tests,1019,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1062,safety,test,test,1062,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1226,safety,test,test,1226,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1234,safety,log,log-normalized,1234,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1348,safety,test,test,1348,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1413,safety,log,log-normalization,1413,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1234,security,log,log-normalized,1234,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1413,security,log,log-normalization,1413,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:184,testability,test,testing,184,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:392,testability,test,test,392,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1019,testability,test,tests,1019,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1062,testability,test,test,1062,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1226,testability,test,test,1226,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1234,testability,log,log-normalized,1234,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1326,testability,regress,regressed,1326,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1348,testability,test,test,1348,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:1413,testability,log,log-normalization,1413,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```. WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]. sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'). sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'). ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data. In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:218,availability,error,errors,218,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. . Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:218,performance,error,errors,218,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. . Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:218,safety,error,errors,218,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. . Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:165,testability,understand,understand,165,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. . Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:218,usability,error,errors,218,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. . Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:382,deployability,version,version,382,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:382,integrability,version,version,382,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:382,modifiability,version,version,382,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:259,safety,test,test,259,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:413,safety,test,tested,413,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:259,testability,test,test,259,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:413,testability,test,tested,413,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]. sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') . sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'). ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:39,performance,time,time,39,"Hi,. Thank you so much! It worked this time! I got a lot ""Keyerror"" before, but I tried your tested code, it worked perfectly! . I really appreciate it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:93,safety,test,tested,93,"Hi,. Thank you so much! It worked this time! I got a lot ""Keyerror"" before, but I tried your tested code, it worked perfectly! . I really appreciate it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1035:93,testability,test,tested,93,"Hi,. Thank you so much! It worked this time! I got a lot ""Keyerror"" before, but I tried your tested code, it worked perfectly! . I really appreciate it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035
https://github.com/scverse/scanpy/issues/1036:594,availability,FAILUR,FAILURES,594,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:594,deployability,FAIL,FAILURES,594,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2015,deployability,version,versions,2015," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2044,deployability,log,logging,2044," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:9,energy efficiency,Current,Currently,9,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1107,energy efficiency,reduc,reducer,1107,"================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1168,energy efficiency,reduc,reducer,1168,"===============================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1205,energy efficiency,reduc,reducer,1205,"form darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-lear",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1213,integrability,transform,transform,1213,"in -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2015,integrability,version,versions,2015," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:205,interoperability,platform,platform,205,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:262,interoperability,plug,pluggy-,262,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:361,interoperability,plug,plugins,361,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1213,interoperability,transform,transform,1213,"in -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1963,interoperability,coordinat,coordinates,1963," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2015,modifiability,version,versions,2015," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:384,performance,parallel,parallel-,384,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:594,performance,FAILUR,FAILURES,594,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:594,reliability,FAIL,FAILURES,594,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:28,safety,test,test,28,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:130,safety,test,test,130,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:335,safety,test,testpaths,335,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:353,safety,test,tests,353,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:501,safety,test,tests,501,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1577,safety,test,tests,1577," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2044,safety,log,logging,2044," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:135,security,session,session,135,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2044,security,log,logging,2044," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:28,testability,test,test,28,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:130,testability,test,test,130,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:335,testability,test,testpaths,335,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:353,testability,test,tests,353,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:501,testability,test,tests,501,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1231,testability,assert,assert,1231,".6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmode",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1294,testability,assert,assert,1294," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1577,testability,test,tests,1577," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:1603,testability,Assert,AssertionError,1603," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2044,testability,log,logging,2044," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:287,usability,User,Users,287,"@Koncopd Currently breaking test for me:. ```pytb. $ pytest -k test_ingest. ===================================================== test session starts =====================================================. platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0. rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:2205,usability,learn,learn,2205," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/. plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0. collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================. _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():. adata_ref = sc.AnnData(X). adata_new = sc.AnnData(T). . sc.pp.neighbors(. adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0. ). sc.tl.umap(adata_ref, random_state=0). . ing = sc.tl.Ingest(adata_ref). ing.fit(adata_new). ing.map_embedding(method='umap'). . reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4). reducer.fit(X). umap_transformed_t = reducer.transform(T). . > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t). E assert False. E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)). E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError. ---------------------------------------------------- Captured stderr call -----------------------------------------------------. computing neighbors. finished: added to `.uns['neighbors']`. 'distances', distances for each pair of neighbors. 'connectivities', weighted adjacency matrix (0:00:00). computing UMAP. finished: added. 'X_umap', UMAP coordinates (adata.obsm) (0:00:00). ```. With these versions:. ```python. >>> sc.logging.print_versions() . scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:27,availability,error,error,27,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do? Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:27,performance,error,error,27,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do? Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:27,safety,error,error,27,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do? Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:96,testability,understand,understand,96,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do? Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:27,usability,error,error,27,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do? Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:48,availability,down,downgrading,48,"@nahanoo . Hi, there are 3 options for now:. 1. downgrading umap to 0.39. 2. installing scanpy from github. 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:77,deployability,instal,installing,77,"@nahanoo . Hi, there are 3 options for now:. 1. downgrading umap to 0.39. 2. installing scanpy from github. 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:129,deployability,releas,release,129,"@nahanoo . Hi, there are 3 options for now:. 1. downgrading umap to 0.39. 2. installing scanpy from github. 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/issues/1036:33,deployability,Build,Building,33,Thank you for the fast response. Building from source did the job for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036
https://github.com/scverse/scanpy/pull/1038:137,availability,robust,robust,137,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:160,deployability,updat,updates,160,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:137,reliability,robust,robust,137,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:137,safety,robust,robust,137,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:160,safety,updat,updates,160,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:160,security,updat,updates,160,"Nice! But may I ask why youre still importing everything from umap instead of from pynndescent? Id assume if wed do that wed be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:118,deployability,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:118,integrability,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:118,modifiability,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:118,safety,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:118,testability,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:50,deployability,version,version,50,"This looks pretty reasonable for now -- the newer version tried to clean up a lot of t5hese sorts of issues, but backwards compatibility is now an issue I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:50,integrability,version,version,50,"This looks pretty reasonable for now -- the newer version tried to clean up a lot of t5hese sorts of issues, but backwards compatibility is now an issue I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:123,interoperability,compatib,compatibility,123,"This looks pretty reasonable for now -- the newer version tried to clean up a lot of t5hese sorts of issues, but backwards compatibility is now an issue I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:50,modifiability,version,version,50,"This looks pretty reasonable for now -- the newer version tried to clean up a lot of t5hese sorts of issues, but backwards compatibility is now an issue I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/pull/1038:34,deployability,continu,continue,34,"Great, if the new way is going to continue to work, everythings fine!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038
https://github.com/scverse/scanpy/issues/1039:35,security,ident,identical,35,The issue could be fixed by having identical post_adata.var_names and post_adata.raw.var_names. Please ignore this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:145,availability,error,error,145,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:145,performance,error,error,145,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:24,reliability,doe,does,24,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:145,safety,error,error,145,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:145,usability,error,error,145,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:250,availability,mask,mask,250,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:14,deployability,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:355,deployability,log,log-normalized,355,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:14,integrability,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:137,integrability,filter,filtered,137,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:400,integrability,batch,batch,400,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:14,modifiability,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:400,performance,batch,batch,400,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:14,safety,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:355,safety,log,log-normalized,355,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:355,security,log,log-normalized,355,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:14,testability,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:355,testability,log,log-normalized,355,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1039:420,testability,regress,regress,420,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039
https://github.com/scverse/scanpy/issues/1040:9,testability,plan,plan,9,"Sure, we plan to overhaul plotting soon, but all fixes until then are welcome. Should be pretty straightforward. Would you care to do a PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1040
https://github.com/scverse/scanpy/issues/1043:72,energy efficiency,profil,profile,72,"On a related note, would it make sense for an option to call `gprofiler.profile()` with a list of `background` genes for the `sc.queries.enrich(adata)` case. The `background` genes could be all the genes in the adata object? Currently, the `background` is always the full set of protein coding genes for the organism I think, which is quite different if few genes are expressed in the dataset, something like... ```. sc.queries.enrich(adata, group=""0"", gprofiler_kwargs={""background"": list(adata.var.index)}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:225,energy efficiency,Current,Currently,225,"On a related note, would it make sense for an option to call `gprofiler.profile()` with a list of `background` genes for the `sc.queries.enrich(adata)` case. The `background` genes could be all the genes in the adata object? Currently, the `background` is always the full set of protein coding genes for the organism I think, which is quite different if few genes are expressed in the dataset, something like... ```. sc.queries.enrich(adata, group=""0"", gprofiler_kwargs={""background"": list(adata.var.index)}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:72,performance,profil,profile,72,"On a related note, would it make sense for an option to call `gprofiler.profile()` with a list of `background` genes for the `sc.queries.enrich(adata)` case. The `background` genes could be all the genes in the adata object? Currently, the `background` is always the full set of protein coding genes for the organism I think, which is quite different if few genes are expressed in the dataset, something like... ```. sc.queries.enrich(adata, group=""0"", gprofiler_kwargs={""background"": list(adata.var.index)}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:164,usability,person,person,164,"That makes a lot of sense to me. I also would have assumed that you can set the background in the `sc.queries.enrich()` function. But then, maybe I'm not the right person to comment on this as I've never used `sc.tl.filter_rank_genes_groups` and didn't know it existed until just now. I believe @ivirshup worked on this function?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:91,deployability,fail,fails,91,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:477,integrability,filter,filtering,477,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:91,reliability,fail,fails,91,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:138,reliability,doe,does,138,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:69,usability,document,documented,69,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:217,usability,document,documenting,217,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:458,usability,custom,custom,458,"I think both of these things are probably already possible, just not documented. That this fails with `sc.tl. rank_genes_groups_filtered` does seems like a bug. This is partly a problem of not having a convention for documenting dispatched methods (@flying-sheep, do you have recommendations for how to handle this?). `sc.queries.enrich(adata, ...)` calls `sc.get.rank_genes_groups_df`, then the basic `enrich` method. You can pass arguments to both. To use custom cutoffs for filtering:. ```python. sc.queries.enrich(adata, log2fc_min=2, pval_cutoff=.01). ```. The other example, where you pass `gprofiler_kwargs`, should work as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:159,availability,avail,available,159,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:159,reliability,availab,available,159,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:159,safety,avail,available,159,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:159,security,availab,available,159,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/issues/1043:172,usability,custom,custom,172,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043
https://github.com/scverse/scanpy/pull/1047:21,deployability,fail,fails,21,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:117,deployability,artifact,artifacts,117,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:160,deployability,fail,failed-diff,160,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:248,deployability,artifact,artifacts,248,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:21,reliability,fail,fails,21,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:160,reliability,fail,failed-diff,160,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:144,safety,test,tests,144,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:144,testability,test,tests,144,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:233,usability,user,user,233,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml. addons:. artifacts:. paths:. - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""). ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:15,reliability,doe,does,15,"@flying-sheep, does this still need to get merged? It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:67,testability,simpl,simple,67,"@flying-sheep, does this still need to get merged? It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1047:67,usability,simpl,simple,67,"@flying-sheep, does this still need to get merged? It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047
https://github.com/scverse/scanpy/pull/1048:161,deployability,releas,release,161,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/pull/1048:28,integrability,sub,subsetting,28,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/pull/1048:115,modifiability,pac,package,115,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/pull/1048:152,modifiability,pac,packages,152,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/pull/1048:264,safety,test,tests,264,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/pull/1048:264,testability,test,tests,264,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package  . https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048
https://github.com/scverse/scanpy/issues/1051:163,availability,error,error,163,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:169,integrability,messag,message,169,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:169,interoperability,messag,message,169,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:163,performance,error,error,163,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:163,safety,error,error,163,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:79,usability,minim,minimum,79,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:163,usability,error,error,163,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:73,availability,error,error,73,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:73,performance,error,error,73,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:494,reliability,doe,does,494,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:73,safety,error,error,73,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:503,safety,reme,remember,503,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:73,usability,error,error,73,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:489,usability,user,user,489,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python. import numpy as np. import scanpy as sc. import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))). sc.pp.pca(adata). ```. 2. Defaults should work without tuning. 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:49,availability,error,error,49,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:68,deployability,log,logic,68,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:49,performance,error,error,49,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:372,performance,time,time,372,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:78,reliability,doe,does,78,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:49,safety,error,error,49,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:68,safety,log,logic,68,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:68,security,log,logic,68,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:68,testability,log,logic,68,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:49,usability,error,error,49,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:. https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:252,availability,error,error,252,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:292,integrability,compon,components,292,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:270,interoperability,specif,specified,270,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:292,interoperability,compon,components,292,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:292,modifiability,compon,components,292,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:252,performance,error,error,252,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:308,reliability,doe,doesn,308,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:252,safety,error,error,252,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:65,usability,behavi,behaviour,65,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:252,usability,error,error,252,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:265,usability,user,user,265,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python. if n_comps is None:. min_dim = min(adata.n_vars, adata.n_obs). if 50 >= min_dim:. n_comps = min_dim - 1. else:. n_comps = 50. ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:26,integrability,filter,filtering,26,"Ha, actually we implement filtering for `highly_variable_genes` as taking a subset of the anndata object, so it is ` min(adata.n_vars, adata.n_obs)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1051:76,integrability,sub,subset,76,"Ha, actually we implement filtering for `highly_variable_genes` as taking a subset of the anndata object, so it is ` min(adata.n_vars, adata.n_obs)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051
https://github.com/scverse/scanpy/issues/1053:15,deployability,updat,update,15,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,deployability,api,api,172,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:446,deployability,modul,modularity,446,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,integrability,api,api,172,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:446,integrability,modular,modularity,446,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,interoperability,api,api,172,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:446,modifiability,modul,modularity,446,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:475,modifiability,paramet,parameter,475,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:326,performance,multiplex,multiplex,326,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:377,performance,perform,performance,377,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:15,safety,updat,update,15,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:446,safety,modul,modularity,446,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:15,security,updat,update,15,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:446,testability,modula,modularity,446,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:93,usability,user,users,93,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:377,usability,perform,performance,377,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg? * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:82,deployability,modul,modularity,82,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:82,integrability,modular,modularity,82,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:82,modifiability,modul,modularity,82,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:121,modifiability,paramet,parameters,121,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:616,modifiability,pac,package,616,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:265,performance,perform,performant,265,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:361,performance,multiplex,multiplex,361,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:547,performance,multiplex,multiplex,547,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:315,reliability,doe,does,315,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:82,safety,modul,modularity,82,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,security,sign,significance,430,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:82,testability,modula,modularity,82,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:265,usability,perform,performant,265,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:58,deployability,modul,modularity,58,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:58,integrability,modular,modularity,58,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:58,modifiability,modul,modularity,58,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:183,performance,perform,performance,183,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:58,safety,modul,modularity,58,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:207,safety,test,test,207,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:58,testability,modula,modularity,58,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:207,testability,test,test,207,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:183,usability,perform,performance,183,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:87,deployability,modul,modularity,87,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:87,integrability,modular,modularity,87,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:87,modifiability,modul,modularity,87,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:10,performance,perform,performed,10,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:171,performance,network,networks,171,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:87,safety,modul,modularity,87,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:98,safety,detect,detection,98,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:361,safety,detect,detection,361,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:98,security,detect,detection,98,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:171,security,network,networks,171,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:361,security,detect,detection,361,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:87,testability,modula,modularity,87,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:10,usability,perform,performed,10,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:. ```. Running Leiden 0.7.0.post1+71.g14ba1e4.dirty. Running igraph 0.8.0. Read graph (n=63731,m=817035), starting community detection. leidenalg: t=8.048258741036989, m=0.6175825273363675. igraph community_leiden: t=1.159165252931416, m=0.6298702028415605. ```. This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:70,availability,cluster,clustering,70,"I have a large single cell set, where i'd love to speed up the Leiden clustering. . Is it the igraph version of leiden implemented in scanpy? Just wondering if this might be a potential area for speed gains. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:70,deployability,cluster,clustering,70,"I have a large single cell set, where i'd love to speed up the Leiden clustering. . Is it the igraph version of leiden implemented in scanpy? Just wondering if this might be a potential area for speed gains. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:101,deployability,version,version,101,"I have a large single cell set, where i'd love to speed up the Leiden clustering. . Is it the igraph version of leiden implemented in scanpy? Just wondering if this might be a potential area for speed gains. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:101,integrability,version,version,101,"I have a large single cell set, where i'd love to speed up the Leiden clustering. . Is it the igraph version of leiden implemented in scanpy? Just wondering if this might be a potential area for speed gains. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:101,modifiability,version,version,101,"I have a large single cell set, where i'd love to speed up the Leiden clustering. . Is it the igraph version of leiden implemented in scanpy? Just wondering if this might be a potential area for speed gains. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:189,performance,perform,performance,189,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:99,usability,document,documentation,99,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:160,usability,behavi,behavior,160,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:189,usability,perform,performance,189,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:85,availability,cluster,clustering,85,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:378,availability,cluster,clustering,378,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:518,availability,cluster,clustering,518,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:718,availability,cluster,clustering,718,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:825,availability,cluster,cluster,825,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1142,availability,Cluster,Clustering-the-neighborhood-graph,1142,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1255,availability,cluster,cluster,1255,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1361,availability,cluster,cluster,1361,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:85,deployability,cluster,clustering,85,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:378,deployability,cluster,clustering,378,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,deployability,modul,modularity,430,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:518,deployability,cluster,clustering,518,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:718,deployability,cluster,clustering,718,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:825,deployability,cluster,cluster,825,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1142,deployability,Cluster,Clustering-the-neighborhood-graph,1142,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1255,deployability,cluster,cluster,1255,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1361,deployability,cluster,cluster,1361,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,integrability,modular,modularity,430,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,modifiability,modul,modularity,430,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1189,reliability,pra,practically,1189,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,safety,modul,modularity,430,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:430,testability,modula,modularity,430,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:936,usability,user,user-images,936,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1405,usability,user,user-images,1405,"So there are definite speed gains to be had with the igraph implementation of Leiden clustering. But the results are not exactly the same. I have run igraph straight from adata by piggy backing scanpy utility functions with the following code... ```. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', resolution_parameter=0.15). adata_analyse.obs['leiden_igraph'] = pd.Series(clustering.membership, dtype='category', index=adata_analyse.obs.index). ```. On my spatial data (only mention this to explain the poor cell-cell separation in the results!) igraph is 5.86x faster at clustering a 185,000 cell dataset vs. `sc.tl.leiden(adata, resolution=0.15)`. The methods output different cluster numbers, but the results broadly make sense for both outputs biologically speaking. . ![image](https://user-images.githubusercontent.com/25825809/153923576-499b85c3-f521-460e-88d7-be99a4bf055b.png). On the [PBMC 3k dataset from scanpy tutorials,](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html#Clustering-the-neighborhood-graph) there is no practically speed difference (too small a dateset to matter). The cluster number output from both methods is the same for both methods. There are a few differences in cell cluster assignment though. ![image](https://user-images.githubusercontent.com/25825809/153925230-1f0becf1-e024-4968-bf3a-410144acecb7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:579,availability,avail,available,579,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:127,deployability,releas,release,127,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:155,deployability,version,version,155,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:208,deployability,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:155,integrability,version,version,155,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:208,integrability,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:296,integrability,sub,substantially,296,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:155,modifiability,version,version,155,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:208,modifiability,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:234,modifiability,pac,packages,234,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:338,modifiability,pac,package,338,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:417,modifiability,extens,extensive,417,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:454,modifiability,pac,package,454,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:579,reliability,availab,available,579,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:208,safety,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:579,safety,avail,available,579,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:579,security,availab,available,579,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:208,testability,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:353,testability,simpl,simpler,353,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:353,usability,simpl,simpler,353,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:152,availability,sli,slightly,152,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:356,availability,replic,replicate,356,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:183,modifiability,paramet,parameter,183,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:84,performance,time,times,84,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:166,performance,time,time,166,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:231,performance,time,time,231,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:152,reliability,sli,slightly,152,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:20,safety,accid,accidentally,20,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:392,usability,clear,clear,392,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:88,deployability,modul,modularity,88,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:447,deployability,modul,modularity,447,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:88,integrability,modular,modularity,88,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:447,integrability,modular,modularity,447,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:88,modifiability,modul,modularity,88,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:447,modifiability,modul,modularity,447,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:88,safety,modul,modularity,88,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:447,safety,modul,modularity,447,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:88,testability,modula,modularity,88,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:447,testability,modula,modularity,447,"Good to see the large speed gains! Do note that `g.community_leiden(objective_function='modularity', resolution_parameter=0.15)` won't use any edge weights by default. Possibly, `sc.tl.leiden()`, might use edge weights? This might explain some of the discrepancies you might see. Another difference is that `community_leiden` only runs on undirected graphs, while `leidenalg` will also work on directed graphs, which makes a difference when using modularity. This could perhaps also explain some of the discrepancies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:136,availability,sli,slightly,136,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:367,deployability,api,api,367,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:367,integrability,api,api,367,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:367,interoperability,api,api,367,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:452,interoperability,standard,standard,452,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:167,modifiability,paramet,parameter,167,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:68,performance,time,times,68,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:150,performance,time,time,150,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:215,performance,time,time,215,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:136,reliability,sli,slightly,136,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:4,safety,accid,accidentally,4,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:433,testability,simpl,simply,433,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:433,usability,simpl,simply,433,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:8,security,ident,identical,8,"Getting identical output from two runs without using the same seed is unlikely in many cases indeed. In some cases, you really do want to get identical output, so then setting a seed could be useful. In addition, it would be good to ensure that at least the approach is conceptually the same for both the `python-igraph` implementation and `sc.tl.leiden`. But the suggestion of taking multiple runs and compare them makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:142,security,ident,identical,142,"Getting identical output from two runs without using the same seed is unlikely in many cases indeed. In some cases, you really do want to get identical output, so then setting a seed could be useful. In addition, it would be good to ensure that at least the approach is conceptually the same for both the `python-igraph` implementation and `sc.tl.leiden`. But the suggestion of taking multiple runs and compare them makes sense!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:81,interoperability,plug,plugin,81,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:16,modifiability,pac,package,16,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:368,modifiability,pac,package,368,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:587,performance,time,time,587,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:167,usability,efficien,efficiently,167,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:286,usability,efficien,efficient,286,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:384,usability,efficien,efficient,384,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:472,usability,efficien,efficient,472,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:223,availability,cluster,clustering,223,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:360,availability,cluster,clustering,360,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:454,availability,cluster,clustering,454,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:223,deployability,cluster,clustering,223,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:275,deployability,modul,modularity,275,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:360,deployability,cluster,clustering,360,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:454,deployability,cluster,clustering,454,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:275,integrability,modular,modularity,275,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:275,modifiability,modul,modularity,275,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:696,performance,time,times,696,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:668,reliability,doe,does,668,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:11,safety,test,test,11,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:275,safety,modul,modularity,275,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:11,testability,test,test,11,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:275,testability,modula,modularity,275,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:542,usability,user,user-images,542,"For the 3k test dataset, introducing edge weights with. ```. import random. random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(objective_function='modularity', weights='weight'). adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). ```. Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible. ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:147,deployability,modul,modularity,147,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:271,deployability,modul,modularity,271,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:147,integrability,modular,modularity,147,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:271,integrability,modular,modularity,271,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:147,modifiability,modul,modularity,147,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:271,modifiability,modul,modularity,271,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:244,reliability,doe,does,244,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:413,reliability,doe,does,413,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:147,safety,modul,modularity,147,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:271,safety,modul,modularity,271,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:147,testability,modula,modularity,147,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:271,testability,modula,modularity,271,"I suspect a lot of the runtime increase may come from the number of iterations. . Using the default value, `n_interations=2`, `g.community_leiden(""modularity"", weights=""weight"")` took ~2.5 minutes. Using `n_iterations=-1`, which is what scanpy does, `g.community_leiden(""modularity"", weights=""weight"", n_iterations=-1)` took ~1.5 hours. I am unsure why we set `n_iterations=-1`, since that's not what `leidenalg` does by default. I'm not seeing much discussion of why it's done this way in the issue (#350) or PR (#361)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:201,security,ident,identical,201,"Ah right, you meant those iterations, of course. Yes, that definitely can make a large difference! When comparing the speed of both implementations, indeed the number of iterations should of course be identical.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:154,availability,sli,slightly,154,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:372,availability,replic,replicate,372,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:185,modifiability,paramet,parameter,185,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:86,performance,time,times,86,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:168,performance,time,time,168,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:233,performance,time,time,233,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:154,reliability,sli,slightly,154,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:587,reliability,doe,does,587,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:22,safety,accid,accidentally,22,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:408,usability,clear,clear,408,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe? > . > . > . > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. > . > . > . > Thanks! > . > . > . > Thanks! As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:905,availability,cluster,clusters,905,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:514,deployability,modul,modularity,514,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:564,deployability,updat,update,564,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:905,deployability,cluster,clusters,905,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:953,energy efficiency,optim,optimization,953,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:116,integrability,coupl,couple,116,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:514,integrability,modular,modularity,514,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:63,modifiability,paramet,parameter,63,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:116,modifiability,coupl,couple,116,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:514,modifiability,modul,modularity,514,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:999,modifiability,maintain,maintained,999,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1054,modifiability,maintain,maintained,1054,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:953,performance,optimiz,optimization,953,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:514,safety,modul,modularity,514,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:564,safety,updat,update,564,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:999,safety,maintain,maintained,999,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1054,safety,maintain,maintained,1054,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:564,security,updat,update,564,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:116,testability,coupl,couple,116,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:514,testability,modula,modularity,514,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:223,usability,close,closer,223,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python. def iterativley_cluster(. g: igraph.Graph,. *,. n_iterations: int = 10,. random_state: int = 0,. leiden_kwargs: dict = {}. ) -> list:. import random. random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}. _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]. for _ in range(n_iterations-1):. partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs). steps.append(partition). return steps. ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:290,integrability,sub,subsampled,290,"Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. The average of 4 leiden runs on my 185,000 cell subsampled dataset:. `sc.tl.leiden`, 11.5 minutes. `g.community_leiden`, 9.5 minutes . 1 leiden run on my 1,850,000 cell subsampled dataset:. `sc.tl.leiden`, 11 hours, 26 minutes . `g.community_leiden`, 7 hours, 30 minutes .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:411,integrability,sub,subsampled,411,"Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. The average of 4 leiden runs on my 185,000 cell subsampled dataset:. `sc.tl.leiden`, 11.5 minutes. `g.community_leiden`, 9.5 minutes . 1 leiden run on my 1,850,000 cell subsampled dataset:. `sc.tl.leiden`, 11 hours, 26 minutes . `g.community_leiden`, 7 hours, 30 minutes .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:72,performance,time,time,72,"Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. The average of 4 leiden runs on my 185,000 cell subsampled dataset:. `sc.tl.leiden`, 11.5 minutes. `g.community_leiden`, 9.5 minutes . 1 leiden run on my 1,850,000 cell subsampled dataset:. `sc.tl.leiden`, 11 hours, 26 minutes . `g.community_leiden`, 7 hours, 30 minutes .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:190,performance,time,times,190,"Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. The average of 4 leiden runs on my 185,000 cell subsampled dataset:. `sc.tl.leiden`, 11.5 minutes. `g.community_leiden`, 9.5 minutes . 1 leiden run on my 1,850,000 cell subsampled dataset:. `sc.tl.leiden`, 11 hours, 26 minutes . `g.community_leiden`, 7 hours, 30 minutes .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:177,availability,slo,slower,177,"@mezwick, I don't fully understand something. The timings you report seem to be lower for `sc.tl.leiden` than for `g.community_leiden`, suggesting that `g.community_leiden` is *slower* than `sc.tl.leiden`, but in your comments you say that `g.community_leiden` appears faster. Am I misunderstanding something? Could you perhaps clarify?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:177,reliability,slo,slower,177,"@mezwick, I don't fully understand something. The timings you report seem to be lower for `sc.tl.leiden` than for `g.community_leiden`, suggesting that `g.community_leiden` is *slower* than `sc.tl.leiden`, but in your comments you say that `g.community_leiden` appears faster. Am I misunderstanding something? Could you perhaps clarify?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:24,testability,understand,understand,24,"@mezwick, I don't fully understand something. The timings you report seem to be lower for `sc.tl.leiden` than for `g.community_leiden`, suggesting that `g.community_leiden` is *slower* than `sc.tl.leiden`, but in your comments you say that `g.community_leiden` appears faster. Am I misunderstanding something? Could you perhaps clarify?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:108,deployability,updat,updated,108,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:555,integrability,sub,subsampled,555,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:679,integrability,sub,subsampled,679,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:37,performance,time,time,37,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:331,performance,time,time,331,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:449,performance,time,times,449,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:108,safety,updat,updated,108,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:108,security,updat,updated,108,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:153,usability,clear,clear,153,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster. > . > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes. > . > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:60,availability,cluster,clustering,60,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1094,availability,cluster,clustering,1094,"on cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1268,availability,cluster,clustering,1268,"th weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1404,availability,cluster,clustering,1404,"name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1472,availability,cluster,clustering,1472,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2273,availability,cluster,cluster,2273,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:60,deployability,cluster,clustering,60,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:561,deployability,modul,module,561,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1094,deployability,cluster,clustering,1094,"on cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1148,deployability,modul,modularity,1148,"ython-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1268,deployability,cluster,clustering,1268,"th weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1404,deployability,cluster,clustering,1404,"name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1472,deployability,cluster,clustering,1472,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1588,deployability,modul,module,1588,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2203,deployability,manag,managed,2203,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2273,deployability,cluster,cluster,2273,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2203,energy efficiency,manag,managed,2203,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2331,energy efficiency,power,power,2331,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:206,integrability,sub,subsets,206,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1148,integrability,modular,modularity,1148,"ython-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2143,integrability,sub,subsample,2143,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:561,modifiability,modul,module,561,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:700,modifiability,pac,packages,700,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1148,modifiability,modul,modularity,1148,"ython-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1588,modifiability,modul,module,1588,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1690,modifiability,pac,packages,1690,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1869,modifiability,pac,packages,1869,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2309,performance,memor,memory,2309,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:561,safety,modul,module,561,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1148,safety,modul,modularity,1148,"ython-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1588,safety,modul,module,1588,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2120,safety,compl,completely,2120,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2203,safety,manag,managed,2203,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2120,security,compl,completely,2120,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:441,testability,trace,traceback,441,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:457,testability,Trace,Traceback,457,"Hi @vtraag @ivirshup,. I have been unable to get the leiden clustering to run on a large 18.5 million cell dataset with either `sc.tl.leiden()` or the `python-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1148,testability,modula,modularity,1148,"ython-igraph` implementation outlined above. Smaller subsets of the data run fine. I think the problem lies somewhere with weights. . When i run . ```. obs_name = f""leiden {leiden_suffix}"". sc.tl.leiden(adata, resolution=0.1, n_iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1376,testability,trace,traceback,1376,"iterations=2, key_added=obs_name). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1484,testability,Trace,Traceback,1484,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:1706,usability,tool,tools,1706,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:2309,usability,memor,memory,2309,"ame). ```. I get the following traceback. ```. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. When i run the `python-igraph` leiden implementation like so... ```. obs_name = f""leiden {leiden_suffix}"". g = sc._utils.get_igraph_from_adjacency(adjacency). clustering = g.community_leiden(. objective_function=""modularity"", . resolution_parameter=0.1, . weights = 'weight',. n_iterations=2. ). adata.obs[obs_name] = (. pd.Series(. clustering.membership, . dtype='category', . index=adata.obs.index. ). ). ```. I get the following, similar traceback. ```. UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473. running Leiden clustering. Traceback (most recent call last):. File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>. obs_name = f""leiden {leiden_suffix}"". File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden. g = _utils.get_igraph_from_adjacency(adjacency, directed=directed). File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency. g.es['weight'] = weights. SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:28,performance,memor,memory,28,"This issue appears due to a memory overflow on the number of edges in my graph. Fixing will probably need to wait till igraph 0.10, which should handle 64-bit integers, and so be able to handle far larger graphs . https://github.com/igraph/python-igraph/issues/531#issuecomment-1102792526",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:28,usability,memor,memory,28,"This issue appears due to a memory overflow on the number of edges in my graph. Fixing will probably need to wait till igraph 0.10, which should handle 64-bit integers, and so be able to handle far larger graphs . https://github.com/igraph/python-igraph/issues/531#issuecomment-1102792526",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:25,deployability,version,version,25,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:41,deployability,releas,released,41,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:67,deployability,releas,release,67,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:99,deployability,build,building,99,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:120,deployability,version,version,120,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,deployability,updat,update,172,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:306,deployability,build,builds,306,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:11,energy efficiency,core,core,11,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:25,integrability,version,version,25,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:89,integrability,interfac,interface,89,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:120,integrability,version,version,120,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:89,interoperability,interfac,interface,89,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:25,modifiability,version,version,25,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:89,modifiability,interfac,interface,89,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:120,modifiability,version,version,120,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,safety,updat,update,172,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:172,security,updat,update,172,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:151,usability,support,support,151,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:217,deployability,log,logic,217,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:401,modifiability,paramet,parameter,401,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:270,reliability,doe,does,270,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:217,safety,log,logic,217,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:629,safety,test,test,629,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:217,security,log,logic,217,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:217,testability,log,logic,217,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1053:629,testability,test,test,629,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`. 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`. 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053
https://github.com/scverse/scanpy/issues/1059:29,interoperability,coordinat,coordinate,29,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:87,interoperability,coordinat,coordinate,87,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:207,interoperability,coordinat,coordinates,207,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:310,interoperability,coordinat,coordinates,310,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:362,modifiability,paramet,parameter,362,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:432,modifiability,paramet,parameter,432,"It kinda is yes, as an image coordinate system is differs in that way from normal plot coordinate system:. - Images are addressed as pixel row  pixel column, with pixel 1,1 in the top left corner, i.e. the coordinates mean  then . - Plots are done xy, with position 0,0 to the bottom left of 1,1, i.e. the coordinates mean  then . I assume we should add a parameter to vertically flip plots, and make `sc.pl.spatial` set this parameter by default. A further idea: without an image we should probably plot the spots as hexagonal tiles.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:52,modifiability,paramet,parameter,52,"Thanks for clarifying. . > I assume we should add a parameter to vertically flip plots, and make sc.pl.spatial set this parameter by default. I think this would be useful. I am alternating between visualising with and without the tissue image in the background and the flipping can cause quite some confusion, esp for ""square"" tissue sections. . > A further idea: without an image we should probably plot the spots as hexagonal tiles. or set a dark background.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:120,modifiability,paramet,parameter,120,"Thanks for clarifying. . > I assume we should add a parameter to vertically flip plots, and make sc.pl.spatial set this parameter by default. I think this would be useful. I am alternating between visualising with and without the tissue image in the background and the flipping can cause quite some confusion, esp for ""square"" tissue sections. . > A further idea: without an image we should probably plot the spots as hexagonal tiles. or set a dark background.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:197,usability,visual,visualising,197,"Thanks for clarifying. . > I assume we should add a parameter to vertically flip plots, and make sc.pl.spatial set this parameter by default. I think this would be useful. I am alternating between visualising with and without the tissue image in the background and the flipping can cause quite some confusion, esp for ""square"" tissue sections. . > A further idea: without an image we should probably plot the spots as hexagonal tiles. or set a dark background.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:51,usability,user,user-images,51,"You can just put `alpha_img = 0`. ![image](https://user-images.githubusercontent.com/25887487/74839801-2bfbb400-5326-11ea-95c1-1a95b40766cf.png). ![image](https://user-images.githubusercontent.com/25887487/74839776-21411f00-5326-11ea-8b4e-f4098a7df079.png). But yeah it's a functionality that should be supported, will have a look at it. Also, I like the hexagonals @flying-sheep !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:163,usability,user,user-images,163,"You can just put `alpha_img = 0`. ![image](https://user-images.githubusercontent.com/25887487/74839801-2bfbb400-5326-11ea-95c1-1a95b40766cf.png). ![image](https://user-images.githubusercontent.com/25887487/74839776-21411f00-5326-11ea-8b4e-f4098a7df079.png). But yeah it's a functionality that should be supported, will have a look at it. Also, I like the hexagonals @flying-sheep !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1059:303,usability,support,supported,303,"You can just put `alpha_img = 0`. ![image](https://user-images.githubusercontent.com/25887487/74839801-2bfbb400-5326-11ea-95c1-1a95b40766cf.png). ![image](https://user-images.githubusercontent.com/25887487/74839776-21411f00-5326-11ea-8b4e-f4098a7df079.png). But yeah it's a functionality that should be supported, will have a look at it. Also, I like the hexagonals @flying-sheep !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1059
https://github.com/scverse/scanpy/issues/1061:184,safety,test,test,184,"Hi, thank you for the report. However, you deleted. > Put a minimal reproducible example that reproduces the bug in the code block below. Please do that, so we can create a regression test from it:. ```py. import numpy as np. import scanpy as sc. ad = AnnData(np.array([...])). sc.tl.rank_genes_groups(ad) # this line crashes without the fix. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:173,testability,regress,regression,173,"Hi, thank you for the report. However, you deleted. > Put a minimal reproducible example that reproduces the bug in the code block below. Please do that, so we can create a regression test from it:. ```py. import numpy as np. import scanpy as sc. ad = AnnData(np.array([...])). sc.tl.rank_genes_groups(ad) # this line crashes without the fix. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:184,testability,test,test,184,"Hi, thank you for the report. However, you deleted. > Put a minimal reproducible example that reproduces the bug in the code block below. Please do that, so we can create a regression test from it:. ```py. import numpy as np. import scanpy as sc. ad = AnnData(np.array([...])). sc.tl.rank_genes_groups(ad) # this line crashes without the fix. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:60,usability,minim,minimal,60,"Hi, thank you for the report. However, you deleted. > Put a minimal reproducible example that reproduces the bug in the code block below. Please do that, so we can create a regression test from it:. ```py. import numpy as np. import scanpy as sc. ad = AnnData(np.array([...])). sc.tl.rank_genes_groups(ad) # this line crashes without the fix. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:181,integrability,wrap,wrapper,181,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:102,interoperability,specif,specific,102,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:181,interoperability,wrapper,wrapper,181,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:261,safety,test,test,261,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:250,testability,regress,regression,250,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:261,testability,test,test,261,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:13,safety,test,test,13,"Uh, we dont test on windows at the moment so it wont. To create a data set, you could just have created one using some numpy random function or so. But as it is we should probably just fix it the way you propose. Care to do a PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/issues/1061:13,testability,test,test,13,"Uh, we dont test on windows at the moment so it wont. To create a data set, you could just have created one using some numpy random function or so. But as it is we should probably just fix it the way you propose. Care to do a PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061
https://github.com/scverse/scanpy/pull/1062:81,usability,behavi,behavior,81,"Using `/ 12` instead of `*(1/12)` works too, right? Or is there some weird numpy behavior here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1062
https://github.com/scverse/scanpy/pull/1062:57,availability,operat,operations,57,But I am not sure about the readability and the order of operations. `a/b*c` is equal to `a/(b*c)` or `(a/b)*c`? . that why I did it with parenthesis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1062
https://github.com/scverse/scanpy/pull/1063:25,modifiability,deco,decorator,25,"Actually, why is there a decorator at all? let me do this real quick: 37e7782",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:16,safety,test,test,16,"hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out ce06987",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:77,security,trust,trusting,77,"hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out ce06987",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:16,testability,test,test,16,"hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out ce06987",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:18,safety,test,test,18,"> hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out [ce06987](https://github.com/theislab/scanpy/commit/ce06987e6824471d0fe22c5cc7f9faf8840bf5da). sorry about that forgot to do this with the last changes I made!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:79,security,trust,trusting,79,"> hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out [ce06987](https://github.com/theislab/scanpy/commit/ce06987e6824471d0fe22c5cc7f9faf8840bf5da). sorry about that forgot to do this with the last changes I made!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:18,testability,test,test,18,"> hmm, you didnt test with the new changes. please do so for the next PR, Im trusting you with these! Check out [ce06987](https://github.com/theislab/scanpy/commit/ce06987e6824471d0fe22c5cc7f9faf8840bf5da). sorry about that forgot to do this with the last changes I made!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:47,usability,support,support,47,"> Looks good now, thank you! Thank you for the support and help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1063:59,usability,help,help,59,"> Looks good now, thank you! Thank you for the support and help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063
https://github.com/scverse/scanpy/pull/1066:501,deployability,scale,scaled,501,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:501,energy efficiency,scale,scaled,501,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:61,integrability,coupl,couple,61,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:91,integrability,sub,submit,91,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:438,integrability,compon,components,438,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:486,integrability,compon,components,486,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:438,interoperability,compon,components,438,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:486,interoperability,compon,components,486,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:61,modifiability,coupl,couple,61,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:438,modifiability,compon,components,438,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:486,modifiability,compon,components,486,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:501,modifiability,scal,scaled,501,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:282,performance,time,time,282,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:288,performance,memor,memory,288,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:380,performance,memor,memory,380,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:427,performance,time,times,427,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:501,performance,scale,scaled,501,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:640,performance,network,network,640,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:640,security,network,network,640,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:61,testability,coupl,couple,61,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:113,usability,learn,learn,113,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:288,usability,memor,memory,288,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:380,usability,memor,memory,380,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting. * This should work with other solvers from scipy, like `lobpcg`, right? * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that? ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:933,availability,slo,slower,933,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:90,integrability,compon,components,90,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:173,integrability,Sub,Submitting,173,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:319,integrability,topic,topic,319,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:482,integrability,event,eventually,482,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:843,integrability,compon,components,843,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:90,interoperability,compon,components,90,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:843,interoperability,compon,components,843,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:73,modifiability,scal,scaling,73,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:90,modifiability,compon,components,90,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:843,modifiability,compon,components,843,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:340,performance,time,time,340,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:357,reliability,doe,does,357,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:413,reliability,doe,doesn,413,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:477,reliability,doe,does,477,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:642,reliability,doe,does,642,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:933,reliability,slo,slower,933,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:517,safety,input,inputs,517,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:919,security,sign,significantly,919,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:397,usability,support,support,397,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:517,usability,input,inputs,517,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:763,usability,learn,learn,763,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:776,usability,learn,learn,776,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:799,availability,sli,slight,799,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:895,availability,sli,slightly,895,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:914,availability,cluster,cluster,914,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:58,deployability,contain,containing,58,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:914,deployability,cluster,cluster,914,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:881,integrability,translat,translates,881,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:881,interoperability,translat,translates,881,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:263,performance,memor,memory,263,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:322,performance,time,time,322,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:327,performance,time,time,327,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:433,performance,time,time,433,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:438,performance,time,time,438,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:503,performance,memor,memory,503,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:594,performance,time,time,594,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:599,performance,time,time,599,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:667,performance,time,time,667,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:672,performance,time,time,672,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:737,performance,memor,memory,737,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:799,reliability,sli,slight,799,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:895,reliability,sli,slightly,895,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:263,usability,memor,memory,263,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:503,usability,memor,memory,503,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:737,usability,memor,memory,737,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook. [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,. ```. %%memit. t=time.time(). sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True). print(str(time.time()-t)+' seconds'). ```. ```. 6.122049570083618 seconds. peak memory: 1332.33 MiB, increment: 1047.04 MiB. ```. With `pca_sparse=True`,. ```. %%memit. t=time.time(). sc.tl.pca(adata2,pca_sparse=True,random_state=0). print(str(time.time()-t)+' seconds'). ```. ```. 2.373802423477173 seconds. peak memory: 401.17 MiB, increment: 56.26 MiB. ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:980,deployability,releas,release,980,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:36,integrability,topic,topic,36,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:57,performance,time,time,57,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1214,performance,perform,performance,1214,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1235,performance,time,time,1235,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1244,performance,memor,memory,1244,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1425,performance,multi-thread,multi-threaded,1425,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:74,reliability,doe,does,74,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:130,reliability,doe,doesn,130,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:114,usability,support,support,114,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:357,usability,support,support,357,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:614,usability,support,support,614,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:863,usability,prefer,preference,863,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1214,usability,perform,performance,1214,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1244,usability,memor,memory,1244,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1549,usability,close,closer,1549,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you? Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing! Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)). * Implicit centering, densifying centering, no centering. * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:29,energy efficiency,profil,profiler,29,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:269,energy efficiency,profil,profile,269,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:339,energy efficiency,profil,profile,339,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:392,energy efficiency,profil,profile,392,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:22,performance,memor,memory-profiler,22,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:269,performance,profil,profile,269,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:339,performance,profil,profile,339,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:392,performance,profil,profile,392,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:22,usability,memor,memory-profiler,22,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:679,usability,user,user-images,679,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>. <summary> `sparse_pca.py` </summary>. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). sc.pp.log1p(pbmc). @profile. def implicit_mean_pca():. sc.pp.pca(pbmc, pca_sparse=True). @profile. def explicit_mean_pca():. sc.pp.pca(pbmc). @profile. def nomean_pca():. sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":. implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh. $ mprof run --interval=0.01 ./sparse_pca.py. ... $ mprof plot. ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:240,deployability,releas,release,240,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested! > * Datasets size (one small, one large (>50k cells)). > * Implicit centering, densifying centering, no centering. > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:484,performance,multi-thread,multi-threaded,484,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested! > * Datasets size (one small, one large (>50k cells)). > * Implicit centering, densifying centering, no centering. > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:619,performance,multi-thread,multi-threaded,619,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested! > * Datasets size (one small, one large (>50k cells)). > * Implicit centering, densifying centering, no centering. > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:123,usability,prefer,preference,123,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested! > * Datasets size (one small, one large (>50k cells)). > * Implicit centering, densifying centering, no centering. > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:236,performance,memor,memory,236,I used the 68k pbmc dataset from 10x genomics for the large dataset. Jupyter notebook with residuals:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4234730/benchmarks_PR1066_residuals.ipynb.zip). The memory and timing benchmarks:. ![large](https://user-images.githubusercontent.com/16548075/75012333-97cd4200-5436-11ea-883a-94512bac16a4.png). ![small](https://user-images.githubusercontent.com/16548075/75012334-97cd4200-5436-11ea-9393-696a00b884f8.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:236,usability,memor,memory,236,I used the 68k pbmc dataset from 10x genomics for the large dataset. Jupyter notebook with residuals:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4234730/benchmarks_PR1066_residuals.ipynb.zip). The memory and timing benchmarks:. ![large](https://user-images.githubusercontent.com/16548075/75012333-97cd4200-5436-11ea-883a-94512bac16a4.png). ![small](https://user-images.githubusercontent.com/16548075/75012334-97cd4200-5436-11ea-9393-696a00b884f8.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:284,usability,user,user-images,284,I used the 68k pbmc dataset from 10x genomics for the large dataset. Jupyter notebook with residuals:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4234730/benchmarks_PR1066_residuals.ipynb.zip). The memory and timing benchmarks:. ![large](https://user-images.githubusercontent.com/16548075/75012333-97cd4200-5436-11ea-883a-94512bac16a4.png). ![small](https://user-images.githubusercontent.com/16548075/75012334-97cd4200-5436-11ea-9393-696a00b884f8.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:396,usability,user,user-images,396,I used the 68k pbmc dataset from 10x genomics for the large dataset. Jupyter notebook with residuals:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4234730/benchmarks_PR1066_residuals.ipynb.zip). The memory and timing benchmarks:. ![large](https://user-images.githubusercontent.com/16548075/75012333-97cd4200-5436-11ea-883a-94512bac16a4.png). ![small](https://user-images.githubusercontent.com/16548075/75012334-97cd4200-5436-11ea-9393-696a00b884f8.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:146,reliability,doe,does,146,"By the way, I was curious why nomean was so much faster than implicit mean centering. I noticed that if `zero_center=False` then `TruncatedSVD` does not accept the `svd_solver` argument and defaults to the randomized solver (line 533):. `pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state)`. So the speed difference is due to differences in the solvers (arpack vs randomized). Is the omission of `svd_solver` in the above line intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:610,energy efficiency,CPU,CPU,610,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:706,energy efficiency,CPU,CPU,706,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:893,energy efficiency,CPU,CPU,893,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:988,energy efficiency,CPU,CPU,988,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:232,modifiability,variab,variable,232,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:106,performance,multi-thread,multi-threaded,106,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:570,performance,time,time,570,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:610,performance,CPU,CPU,610,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:614,performance,time,times,614,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:668,performance,time,time,668,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:683,performance,time,time,683,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:706,performance,CPU,CPU,706,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:710,performance,time,times,710,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:763,performance,time,time,763,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:853,performance,time,time,853,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:893,performance,CPU,CPU,893,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:897,performance,time,times,897,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:950,performance,time,time,950,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:965,performance,time,time,965,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:988,performance,CPU,CPU,988,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:992,performance,time,times,992,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1048,performance,time,time,1048,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:1135,reliability,doe,does,1135,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:621,usability,user,user,621,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:717,usability,user,user,717,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:904,usability,user,user,904,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:999,usability,user,user,999,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit? The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python. import os. os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas. os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas. ```. Using sc.datasets.pbmc3k:. <details>. <summary> Single threaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s. Wall time: 4.43 s. %time sc.pp.pca(pbmc) . CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s. Wall time: 15.8 s. ```. </details>. <details>. <summary> Multithreaded </summary>. ```python. %time sc.pp.pca(pbmc, pca_sparse=True) . CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s. Wall time: 2.39 s. %time sc.pp.pca(pbmc) . CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s. Wall time: 9.92 s. ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:133,energy efficiency,cpu,cpu,133,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:182,energy efficiency,core,core,182,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:133,performance,cpu,cpu,133,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:60,reliability,doe,doesn,60,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:352,usability,user,user-images,352,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:503,usability,user,user-images,503,"That's bizarre. Somehow, using a single thread on my system doesn't actually increase the runtime by that much. I kept an eye on the cpu usage to make sure that I was just using one core. It's actually faster to do implicit mean centering on the small and large datasets using a single thread. . Small dataset, one thread:. ![one_thread_small](https://user-images.githubusercontent.com/16548075/75087997-fdcfcd00-54fb-11ea-92fc-8ec52a10a035.png). Large dataset, one thread:. ![one_thread_large](https://user-images.githubusercontent.com/16548075/75087995-fb6d7300-54fb-11ea-9ebb-326b0a7fa407.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:325,energy efficiency,measur,measurements,325,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:86,performance,time,time,86,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:193,performance,perform,performance,193,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:193,usability,perform,performance,193,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:75,integrability,messag,message,75,I included the notebook with the residuals above. I'll reattach it to this message:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4242812/benchmarks_PR1066_residuals.ipynb.zip).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:75,interoperability,messag,message,75,I included the notebook with the residuals above. I'll reattach it to this message:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4242812/benchmarks_PR1066_residuals.ipynb.zip).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:266,availability,state,states,266,"Ah, I had totally missed that, sorry! Hm, it looks like the residuals is scaling with the number of cells. I think this has to do with floating point precision, since using 64bit floats up seems to remove the effect for me. Could you show comparisons between random states within the sparse and dense method so we can be sure? I.e. if you run each method twice with the different seeds, how different are the results? Also, the numpy random state should be set (with the `random_state` argument) before `_pca_with_sparse` calls the svd solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:441,availability,state,state,441,"Ah, I had totally missed that, sorry! Hm, it looks like the residuals is scaling with the number of cells. I think this has to do with floating point precision, since using 64bit floats up seems to remove the effect for me. Could you show comparisons between random states within the sparse and dense method so we can be sure? I.e. if you run each method twice with the different seeds, how different are the results? Also, the numpy random state should be set (with the `random_state` argument) before `_pca_with_sparse` calls the svd solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:266,integrability,state,states,266,"Ah, I had totally missed that, sorry! Hm, it looks like the residuals is scaling with the number of cells. I think this has to do with floating point precision, since using 64bit floats up seems to remove the effect for me. Could you show comparisons between random states within the sparse and dense method so we can be sure? I.e. if you run each method twice with the different seeds, how different are the results? Also, the numpy random state should be set (with the `random_state` argument) before `_pca_with_sparse` calls the svd solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:441,integrability,state,state,441,"Ah, I had totally missed that, sorry! Hm, it looks like the residuals is scaling with the number of cells. I think this has to do with floating point precision, since using 64bit floats up seems to remove the effect for me. Could you show comparisons between random states within the sparse and dense method so we can be sure? I.e. if you run each method twice with the different seeds, how different are the results? Also, the numpy random state should be set (with the `random_state` argument) before `_pca_with_sparse` calls the svd solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:73,modifiability,scal,scaling,73,"Ah, I had totally missed that, sorry! Hm, it looks like the residuals is scaling with the number of cells. I think this has to do with floating point precision, since using 64bit floats up seems to remove the effect for me. Could you show comparisons between random states within the sparse and dense method so we can be sure? I.e. if you run each method twice with the different seeds, how different are the results? Also, the numpy random state should be set (with the `random_state` argument) before `_pca_with_sparse` calls the svd solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:29,deployability,updat,update,29,If we have arpack i can also update the PR with randomized svd approach. Is it needed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:29,safety,updat,update,29,If we have arpack i can also update the PR with randomized svd approach. Is it needed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:29,security,updat,update,29,If we have arpack i can also update the PR with randomized svd approach. Is it needed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:57,performance,perform,performance,57,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:44,usability,satisfa,satisfactory,44,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:57,usability,perform,performance,57,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:101,integrability,sub,submitting,101,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:24,performance,time,time,24,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:419,performance,perform,performance,419,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:184,reliability,stabil,stability,184,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:405,reliability,stabil,stability,405,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:211,usability,help,help,211,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:419,usability,perform,performance,419,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:537,usability,behavi,behaviour,537,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want. * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file? * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf? -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:86,integrability,compon,components,86,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:259,integrability,compon,components,259,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:349,integrability,compon,components,349,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:86,interoperability,compon,components,86,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:259,interoperability,compon,components,259,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:349,interoperability,compon,components,349,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:86,modifiability,compon,components,86,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:259,modifiability,compon,components,259,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:349,modifiability,compon,components,349,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:162,safety,hot,hot,162,"Oh crud, good catch. I was normalizing by the total variance of the top `n` principal components, instead of by the total variance. Easy fix, will commit it in a hot sec. Edit: Scratch that, I lied. I'm confused now. `svds` only outputs the top `k` principal components -- how can I find out the total variance for all `min(n,m)` possible principal components? Edit-edit: Apparently the total variance is equal to the total variance of all the original features. Edit-edit-edit: Fixed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:363,energy efficiency,CPU,CPU,363,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:567,energy efficiency,CPU,CPU,567,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:19,performance,perform,performance,19,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:285,performance,time,time,285,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:363,performance,CPU,CPU,363,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:367,performance,time,times,367,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:422,performance,time,time,422,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:443,performance,memor,memory,443,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:488,performance,time,time,488,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:567,performance,CPU,CPU,567,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:571,performance,time,times,571,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:634,performance,time,time,634,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:657,performance,memor,memory,657,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:922,reliability,doe,does,922,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:680,testability,assert,assert,680,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:748,testability,assert,assert,748,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:19,usability,perform,performance,19,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:374,usability,user,user,374,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:443,usability,memor,memory,443,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:578,usability,user,user,578,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:657,usability,memor,memory,657,"@atarashansky, the performance is looking very very good:. ```python. import scanpy as sc. import numpy as np. from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data. # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64). %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True). # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s. # Wall time: 2.93 s. # Peak memory (including dataset) is about 770 MB. %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True). # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s. # Wall time: 7min 43s. # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]). assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]). ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does? Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:677,availability,error,error,677,"That's odd. sklearn calculates the explained variance and variance ratio as follows:. ```python. # Calculate explained variance & explained variance ratio. X_transformed = U * Sigma. self.explained_variance_ = exp_var = np.var(X_transformed, axis=0). if sp.issparse(X):. _, full_var = mean_variance_axis(X, axis=0). full_var = full_var.sum(). else:. full_var = np.var(X, axis=0).sum(). self.explained_variance_ratio_ = exp_var / full_var. ```. I do it in the same way:. ```python. X_pca = (u * s)[:, idx] # sort PCs in decreasing order. ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(). ev_ratio = ev / total_var. ```. I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:677,performance,error,error,677,"That's odd. sklearn calculates the explained variance and variance ratio as follows:. ```python. # Calculate explained variance & explained variance ratio. X_transformed = U * Sigma. self.explained_variance_ = exp_var = np.var(X_transformed, axis=0). if sp.issparse(X):. _, full_var = mean_variance_axis(X, axis=0). full_var = full_var.sum(). else:. full_var = np.var(X, axis=0).sum(). self.explained_variance_ratio_ = exp_var / full_var. ```. I do it in the same way:. ```python. X_pca = (u * s)[:, idx] # sort PCs in decreasing order. ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(). ev_ratio = ev / total_var. ```. I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:677,safety,error,error,677,"That's odd. sklearn calculates the explained variance and variance ratio as follows:. ```python. # Calculate explained variance & explained variance ratio. X_transformed = U * Sigma. self.explained_variance_ = exp_var = np.var(X_transformed, axis=0). if sp.issparse(X):. _, full_var = mean_variance_axis(X, axis=0). full_var = full_var.sum(). else:. full_var = np.var(X, axis=0).sum(). self.explained_variance_ratio_ = exp_var / full_var. ```. I do it in the same way:. ```python. X_pca = (u * s)[:, idx] # sort PCs in decreasing order. ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(). ev_ratio = ev / total_var. ```. I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:667,testability,assert,assertion,667,"That's odd. sklearn calculates the explained variance and variance ratio as follows:. ```python. # Calculate explained variance & explained variance ratio. X_transformed = U * Sigma. self.explained_variance_ = exp_var = np.var(X_transformed, axis=0). if sp.issparse(X):. _, full_var = mean_variance_axis(X, axis=0). full_var = full_var.sum(). else:. full_var = np.var(X, axis=0).sum(). self.explained_variance_ratio_ = exp_var / full_var. ```. I do it in the same way:. ```python. X_pca = (u * s)[:, idx] # sort PCs in decreasing order. ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(). ev_ratio = ev / total_var. ```. I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:677,usability,error,error,677,"That's odd. sklearn calculates the explained variance and variance ratio as follows:. ```python. # Calculate explained variance & explained variance ratio. X_transformed = U * Sigma. self.explained_variance_ = exp_var = np.var(X_transformed, axis=0). if sp.issparse(X):. _, full_var = mean_variance_axis(X, axis=0). full_var = full_var.sum(). else:. full_var = np.var(X, axis=0).sum(). self.explained_variance_ratio_ = exp_var / full_var. ```. I do it in the same way:. ```python. X_pca = (u * s)[:, idx] # sort PCs in decreasing order. ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(). ev_ratio = ev / total_var. ```. I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:168,modifiability,deco,decomposition,168,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python. self.n_samples_, self.n_features_ = n_samples, n_features. self.components_ = V. self.n_components_ = n_components. # Get variance explained by singular values. self.explained_variance_ = (S ** 2) / (n_samples - 1). total_var = np.var(X, ddof=1, axis=0). self.explained_variance_ratio_ = \. self.explained_variance_ / total_var.sum(). self.singular_values_ = S.copy() # Store the singular values. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:447,security,ddo,ddof,447,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python. self.n_samples_, self.n_features_ = n_samples, n_features. self.components_ = V. self.n_components_ = n_components. # Get variance explained by singular values. self.explained_variance_ = (S ** 2) / (n_samples - 1). total_var = np.var(X, ddof=1, axis=0). self.explained_variance_ratio_ = \. self.explained_variance_ / total_var.sum(). self.singular_values_ = S.copy() # Store the singular values. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:95,usability,learn,learn,95,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python. self.n_samples_, self.n_features_ = n_samples, n_features. self.components_ = V. self.n_components_ = n_components. # Get variance explained by singular values. self.explained_variance_ = (S ** 2) / (n_samples - 1). total_var = np.var(X, ddof=1, axis=0). self.explained_variance_ratio_ = \. self.explained_variance_ / total_var.sum(). self.singular_values_ = S.copy() # Store the singular values. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:108,usability,learn,learn,108,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python. self.n_samples_, self.n_features_ = n_samples, n_features. self.components_ = V. self.n_components_ = n_components. # Get variance explained by singular values. self.explained_variance_ = (S ** 2) / (n_samples - 1). total_var = np.var(X, ddof=1, axis=0). self.explained_variance_ratio_ = \. self.explained_variance_ / total_var.sum(). self.singular_values_ = S.copy() # Store the singular values. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:157,deployability,modul,module,157,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:229,integrability,transform,transform,229,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:488,integrability,Sub,Submitting,488,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:690,integrability,sub,sub,690,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:229,interoperability,transform,transform,229,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:157,modifiability,modul,module,157,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:471,modifiability,maintain,maintainers,471,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:614,reliability,doe,does,614,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:157,safety,modul,module,157,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:370,safety,input,inputs,370,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:471,safety,maintain,maintainers,471,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:370,usability,input,inputs,370,"> Also, any thoughts on making a PR there? Tbh, it's just less hoops to jump through to write a pca function that can fit snugly in your preprocessing utils module compared to writing a new class on top of their base class with `transform`, `fit`, `fit_transform` methods, and the whole shebang. I think the stark computational advantage of using this method for sparse inputs justifies its immediate inclusion into scanpy (of course, this is at the discretion of scanpy maintainers :D). Submitting a PR to sklearn is perhaps something I'd be willing to do later on. Also, as discussed previously, if sklearn ever does come out with an implementation, it should be quite straightforward to sub my function call with theirs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:272,availability,error,error,272,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:175,modifiability,deco,decomposition,175,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:272,performance,error,error,272,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:272,safety,error,error,272,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:262,testability,assert,assertion,262,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:133,usability,learn,learn,133,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:146,usability,learn,learn,146,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:272,usability,error,error,272,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:11,safety,test,test,11,"Here's the test I ran for commit 82e3a59b5905. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). pbmc.X = pbmc.X.astype(np.float64). sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True). explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]). assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:11,testability,test,test,11,"Here's the test I ran for commit 82e3a59b5905. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). pbmc.X = pbmc.X.astype(np.float64). sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True). explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]). assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:332,testability,assert,assert,332,"Here's the test I ran for commit 82e3a59b5905. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). pbmc.X = pbmc.X.astype(np.float64). sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True). explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]). assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:422,testability,assert,assert,422,"Here's the test I ran for commit 82e3a59b5905. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). pbmc.X = pbmc.X.astype(np.float64). sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True). explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]). assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:137,integrability,compon,components,137,"As per your suggestion, I switched to calculating eigenvalue from the singular values as opposed to taking the variance of the principal components. Now the eigenvalues are almost exactly the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:137,interoperability,compon,components,137,"As per your suggestion, I switched to calculating eigenvalue from the singular values as opposed to taking the variance of the principal components. Now the eigenvalues are almost exactly the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:137,modifiability,compon,components,137,"As per your suggestion, I switched to calculating eigenvalue from the singular values as opposed to taking the variance of the principal components. Now the eigenvalues are almost exactly the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:256,integrability,sub,submission,256,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:72,modifiability,responsibil,responsibility,72,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:90,modifiability,maintain,maintain,90,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:497,reliability,doe,does,497,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:503,reliability,Doe,Does,503,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:90,safety,maintain,maintain,90,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:678,safety,test,tests,678,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:107,testability,simpl,simplifies,107,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:678,testability,test,tests,678,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:107,usability,simpl,simplifies,107,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:467,usability,learn,learn,467,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:480,usability,learn,learn,480,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct. * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:14,integrability,sub,submission,14,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:287,integrability,sub,submit,287,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:227,reliability,doe,does,227,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:246,reliability,Doe,Doesn,246,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:202,usability,learn,learn,202,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:215,usability,learn,learn,215,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:74,deployability,releas,release,74,"Hey @atarashansky, what's your status with this? We're going for a larger release soon, and I'd really like to have this PR in it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:31,usability,statu,status,31,"Hey @atarashansky, what's your status with this? We're going for a larger release soon, and I'd really like to have this PR in it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:138,safety,test,tests,138,"Sorry! I've been preoccupied with some stuff on my end (and also super distracted by this coronavirus hullabaloo). I'll add the requested tests, soon. . EDIT: Done!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:138,testability,test,tests,138,"Sorry! I've been preoccupied with some stuff on my end (and also super distracted by this coronavirus hullabaloo). I'll add the requested tests, soon. . EDIT: Done!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:252,deployability,releas,release,252,"Great! Yeah, coronavirus is pretty distracting. This last week has definitely felt like a month for me. So still to do:. * This should be rebased on master. * This should be the default, and this should be noted in the documentation. * Add this to the release notes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:219,usability,document,documentation,219,"Great! Yeah, coronavirus is pretty distracting. This last week has definitely felt like a month for me. So still to do:. * This should be rebased on master. * This should be the default, and this should be noted in the documentation. * Add this to the release notes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:368,integrability,sub,submit,368,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:178,reliability,doe,does,178,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:184,reliability,Doe,Does,184,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:327,reliability,Doe,Doesn,327,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:153,usability,learn,learn,153,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:166,usability,learn,learn,166,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:523,usability,support,support,523,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)? > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:62,availability,sla,slammed,62,Sorry about going mia! You can open the PR. Ive been totally slammed working on a publication.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:83,integrability,pub,publication,83,Sorry about going mia! You can open the PR. Ive been totally slammed working on a publication.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:62,reliability,sla,slammed,62,Sorry about going mia! You can open the PR. Ive been totally slammed working on a publication.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:9,usability,close,close,9,Shall we close #393 and #403?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:827,availability,error,error,827,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:148,deployability,modul,module,148,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:914,deployability,version,version,914,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:925,deployability,depend,dependency,925,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:453,integrability,wrap,wrapper,453,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:914,integrability,version,version,914,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:925,integrability,depend,dependency,925,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:453,interoperability,wrapper,wrapper,453,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:148,modifiability,modul,module,148,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:185,modifiability,pac,packages,185,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:535,modifiability,pac,packages,535,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:914,modifiability,version,version,914,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:925,modifiability,depend,dependency,925,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:132,performance,time,timed,132,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:827,performance,error,error,827,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:148,safety,modul,module,148,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:827,safety,error,error,827,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:925,safety,depend,dependency,925,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:96,testability,Trace,Traceback,96,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:925,testability,depend,dependency,925,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/pull/1066:827,usability,error,error,827,"```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 201 ). 202 . --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver). 204 # this is just a wrapper for the results. 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state). 293 return XHmat(x) - mhmat(ones(x)). 294 . --> 295 XL = LinearOperator(. 296 matvec=matvec,. 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'. ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066
https://github.com/scverse/scanpy/issues/1067:0,deployability,Upgrad,Upgrade,0,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1067:27,deployability,version,version,27,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1067:47,deployability,contain,contains,47,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1067:27,integrability,version,version,27,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1067:0,modifiability,Upgrad,Upgrade,0,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1067:27,modifiability,version,version,27,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067
https://github.com/scverse/scanpy/issues/1068:13,availability,avail,available,13,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:114,availability,avail,available,114,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,integrability,wrap,wrapper,62,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,interoperability,wrapper,wrapper,62,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:13,reliability,availab,available,13,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:114,reliability,availab,available,114,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:13,safety,avail,available,13,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:114,safety,avail,available,114,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:13,security,availab,available,13,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:114,security,availab,available,114,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:291,availability,avail,available,291,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:391,availability,avail,available,391,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:340,integrability,wrap,wrapper,340,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:543,integrability,sub,subscribed,543,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:340,interoperability,wrapper,wrapper,340,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:291,reliability,availab,available,291,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:391,reliability,availab,available,391,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:291,safety,avail,available,291,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:391,safety,avail,available,391,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:291,security,availab,available,291,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:391,security,availab,available,391,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:914,security,auth,auth,914,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:22,usability,usab,usable,22,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly. the vst R function at this address to make it work. https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <. notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for. > it via rpy2 and anndata2ri which is available here:. >. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:67,interoperability,specif,specify,67,Yup.. with the function I linked to it's even easier as no need to specify the commands even ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:79,usability,command,commands,79,Yup.. with the function I linked to it's even easier as no need to specify the commands even ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:226,interoperability,specif,specify,226,"Smart way to do it. Now you inspired me to reorganize my other rpy2 scripts. :). sn. 23. feb. 2020 9.10 PM skrev MalteDLuecken <notifications@github.com>:. > Yup.. with the function I linked to it's even easier as no need to specify. > the commands even ;). >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66ULT7GUJ42DF3DP6Q2DRELJ3VA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMWF6OI#issuecomment-590110521>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ5BJ7IYNEUYGXYGOTRELJ3VANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:659,security,auth,auth,659,"Smart way to do it. Now you inspired me to reorganize my other rpy2 scripts. :). sn. 23. feb. 2020 9.10 PM skrev MalteDLuecken <notifications@github.com>:. > Yup.. with the function I linked to it's even easier as no need to specify. > the commands even ;). >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66ULT7GUJ42DF3DP6Q2DRELJ3VA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMWF6OI#issuecomment-590110521>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ5BJ7IYNEUYGXYGOTRELJ3VANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:241,usability,command,commands,241,"Smart way to do it. Now you inspired me to reorganize my other rpy2 scripts. :). sn. 23. feb. 2020 9.10 PM skrev MalteDLuecken <notifications@github.com>:. > Yup.. with the function I linked to it's even easier as no need to specify. > the commands even ;). >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66ULT7GUJ42DF3DP6Q2DRELJ3VA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMWF6OI#issuecomment-590110521>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACC66UJ5BJ7IYNEUYGXYGOTRELJ3VANCNFSM4KZJFJPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:81,integrability,wrap,wrappers,81,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:81,interoperability,wrapper,wrappers,81,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:136,usability,workflow,workflows,136,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:88,usability,tool,tools,88,"@LuckyMD, that sounds like it would be really useful. A few questions:. * What methods/ tools? * How would you handle R depencies? * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:564,availability,error,error,564,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,deployability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,deployability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:413,deployability,instal,installed,413,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:670,deployability,instal,installable,670,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:682,deployability,modul,module,682,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:751,deployability,instal,install,751,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:761,deployability,depend,dependencies,761,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1130,deployability,depend,depending,1130,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,integrability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,integrability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:761,integrability,depend,dependencies,761,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1130,integrability,depend,depending,1130,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,interoperability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,interoperability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,modifiability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,modifiability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:445,modifiability,pac,packages,445,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:487,modifiability,pac,package,487,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:682,modifiability,modul,module,682,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:761,modifiability,depend,dependencies,761,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1130,modifiability,depend,depending,1130,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:564,performance,error,error,564,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,reliability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,reliability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:564,safety,error,error,564,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:682,safety,modul,module,682,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:761,safety,depend,dependencies,761,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1130,safety,depend,depending,1130,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,security,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,security,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:80,testability,integr,integration,80,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:161,testability,integr,integration,161,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:593,testability,plan,plan,593,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:761,testability,depend,dependencies,761,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:981,testability,understand,understand,981,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1130,testability,depend,depending,1130,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:23,usability,tool,tools,23,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:564,usability,error,error,564,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1241,usability,help,help,1241,"Hey! > * What methods/ tools? I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies? So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer? This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:53,deployability,instal,install,53,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:63,deployability,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:121,deployability,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:272,deployability,instal,install,272,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:239,energy efficiency,current,currently,239,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:63,integrability,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:121,integrability,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:332,integrability,wrap,wrapped,332,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:655,integrability,wrap,wrappers,655,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:655,interoperability,wrapper,wrappers,655,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:63,modifiability,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:99,modifiability,pac,package,99,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:121,modifiability,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:300,modifiability,pac,packages,300,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:556,performance,memor,memory,556,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:685,performance,memor,memory,685,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:63,safety,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:121,safety,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:63,testability,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:121,testability,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:556,usability,memor,memory,556,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:685,usability,memor,memory,685,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:223,deployability,instal,installed,223,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:272,deployability,instal,installed,272,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:287,deployability,instal,install,287,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:55,modifiability,pac,packages,55,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:127,modifiability,maintain,maintain,127,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:148,modifiability,pac,package,148,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:214,modifiability,pac,packages,214,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:263,modifiability,pac,packages,263,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:295,modifiability,pac,packages,295,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:423,performance,memor,memory,423,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:127,safety,maintain,maintain,127,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:423,usability,memor,memory,423,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:365,availability,slo,slow,365,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:147,deployability,instal,install,147,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:29,modifiability,maintain,maintain,29,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:48,modifiability,pac,package,48,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:155,modifiability,pac,packages,155,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:135,reliability,doe,does,135,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:365,reliability,slo,slow,365,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:29,safety,maintain,maintain,29,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case? Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2? Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:117,availability,slo,slow,117,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:122,availability,down,down,122,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:176,deployability,instal,install,176,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:302,deployability,instal,install,302,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:340,deployability,instal,install,340,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:184,modifiability,pac,packages,184,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:316,modifiability,pac,packages,316,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:117,reliability,slo,slow,117,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:164,reliability,doe,does,164,"> That's fair. Might be worth asking the `conos` developers in this case? Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:200,deployability,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:306,deployability,instal,installation,306,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:351,deployability,instal,installed,351,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:420,deployability,instal,install,420,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:430,deployability,instal,installation,430,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:533,deployability,instal,installation,533,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:655,deployability,instal,installed,655,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:489,energy efficiency,reduc,reduce,489,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:607,energy efficiency,reduc,reduced,607,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:930,energy efficiency,reduc,reduced,930,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:200,integrability,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:200,modifiability,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:286,modifiability,pac,packages,286,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1105,modifiability,pac,packages,1105,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:408,performance,cach,cached,408,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:520,performance,cach,caching,520,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:886,performance,overhead,overhead,886,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:985,performance,time,time,985,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1181,performance,cach,cached,1181,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1230,performance,cach,cache,1230,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:1243,performance,overhead,overhead,1243,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:200,safety,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:792,safety,test,test,792,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:881,safety,test,test,881,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:980,safety,test,test,980,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:416,security,apt,apt-install,416,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:200,testability,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:792,testability,test,test,792,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:881,testability,test,test,881,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:980,testability,test,test,980,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:375,usability,user,user,375,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:15,availability,avail,available,15,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:116,availability,avail,available,116,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:289,availability,error,error,289,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:342,availability,Error,Error,342,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:432,availability,slo,slot,432,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:575,availability,slo,slot,575,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:718,availability,slo,slot,718,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:863,availability,slo,slot,863,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:64,integrability,wrap,wrapper,64,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:252,integrability,wrap,wrapper,252,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:64,interoperability,wrapper,wrapper,64,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:252,interoperability,wrapper,wrapper,252,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:306,interoperability,convers,conversion,306,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:495,modifiability,exten,extend,495,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:638,modifiability,exten,extend,638,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:783,modifiability,exten,extend,783,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:926,modifiability,exten,extend,926,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:289,performance,error,error,289,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:342,performance,Error,Error,342,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:15,reliability,availab,available,15,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:116,reliability,availab,available,116,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:432,reliability,slo,slot,432,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:575,reliability,slo,slot,575,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:718,reliability,slo,slot,718,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:863,reliability,slo,slot,863,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:15,safety,avail,available,15,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:116,safety,avail,available,116,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:289,safety,error,error,289,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:342,safety,Error,Error,342,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:351,safety,valid,validObject,351,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:15,security,availab,available,15,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:116,security,availab,available,116,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:289,usability,error,error,289,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:342,usability,Error,Error,342,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:. > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : . invalid class dgCMatrix object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer"". invalid class dgCMatrix object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:101,availability,down,downgrading,101,Hey! I think this is probably related to https://github.com/theislab/anndata2ri/issues/63. Maybe try downgrading your `anndata2ri` version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:131,deployability,version,version,131,Hey! I think this is probably related to https://github.com/theislab/anndata2ri/issues/63. Maybe try downgrading your `anndata2ri` version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:131,integrability,version,version,131,Hey! I think this is probably related to https://github.com/theislab/anndata2ri/issues/63. Maybe try downgrading your `anndata2ri` version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:131,modifiability,version,version,131,Hey! I think this is probably related to https://github.com/theislab/anndata2ri/issues/63. Maybe try downgrading your `anndata2ri` version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,availability,error,error,62,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:84,availability,Error,Error,84,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:31,integrability,wrap,wrapper,31,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:31,interoperability,wrapper,wrapper,31,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,performance,error,error,62,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:84,performance,Error,Error,84,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,safety,error,error,62,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:84,safety,Error,Error,84,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:62,usability,error,error,62,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/issues/1068:84,usability,Error,Error,84,"Hello,. I am trying to use the wrapper class and I am getting error. RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : . undefined columns selected. Could you please suggest me what should I do. Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'). Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068
https://github.com/scverse/scanpy/pull/1069:4,deployability,updat,updates,4,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069
https://github.com/scverse/scanpy/pull/1069:4,safety,updat,updates,4,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069
https://github.com/scverse/scanpy/pull/1069:4,security,updat,updates,4,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069
https://github.com/scverse/scanpy/pull/1069:147,security,modif,modify,147,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069
https://github.com/scverse/scanpy/pull/1070:35,safety,test,test,35,Thanks for the PR! Could you add a test to make sure it works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:35,testability,test,test,35,Thanks for the PR! Could you add a test to make sure it works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:21,safety,test,test,21,"Do you mean to add a test for TravisCI (sorry, I'm new to this)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:21,testability,test,test,21,"Do you mean to add a test for TravisCI (sorry, I'm new to this)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:499,interoperability,specif,specify,499,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:75,safety,test,tests,75,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:124,safety,test,tests,124,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:405,safety,test,tests,405,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:428,safety,test,tests,428,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:513,safety,test,tests,513,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:587,safety,test,test,587,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:75,testability,test,tests,75,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:124,testability,test,tests,124,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:405,testability,test,tests,405,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:428,testability,test,tests,428,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:513,testability,test,tests,513,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/pull/1070:587,testability,test,test,587,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070
https://github.com/scverse/scanpy/issues/1072:19,safety,input,input,19,Hi. Thanks for the input. scVI can definitely be used for quite a lot of tasks. Do you have a recommendation for how it should be talked about on the ecosystem page? (a PR would be welcome of course ).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072
https://github.com/scverse/scanpy/issues/1072:19,usability,input,input,19,Hi. Thanks for the input. scVI can definitely be used for quite a lot of tasks. Do you have a recommendation for how it should be talked about on the ecosystem page? (a PR would be welcome of course ).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072
https://github.com/scverse/scanpy/issues/1072:50,integrability,sub,submit,50,@romain-lopez @adamgayoso could one of you please submit a PR for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072
https://github.com/scverse/scanpy/issues/1072:43,usability,tool,tools,43,not sure this is necessary anymore as scvi-tools is on the ecosystem page,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072
https://github.com/scverse/scanpy/issues/1074:388,deployability,contain,contain,388,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:490,deployability,manag,manage,490,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:490,energy efficiency,manag,manage,490,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:170,reliability,doe,doesn,170,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:490,safety,manag,manage,490,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:543,usability,help,help,543,"1. This warning is produced by loompy. That means that either Seurat made a mistake writing the file or loompy has a bug regarding reading it. 2. Its just a warning, it doesnt necessarily mean things dont work. But if youre worried, file a bug at loompy. 3. No idea why the warning is suppressed in scvelo. @VolkerBergen any idea? PS: thank you for your excellent bug reports. Theyd contain everything wed need to know if we were in the position to fix them. Few people new to Python manage to do that! Im sure the people at loompy can help you quickly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:117,testability,assert,assert,117,Especially weird for since (at least for `scvelo 0.1.25`):. ```python. import scanpy; import scvelo; import anndata. assert scvelo.read_loom == scanpy.read_loom == anndata.read_loom. assert scvelo.read == scanpy.read. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:183,testability,assert,assert,183,Especially weird for since (at least for `scvelo 0.1.25`):. ```python. import scanpy; import scvelo; import anndata. assert scvelo.read_loom == scanpy.read_loom == anndata.read_loom. assert scvelo.read == scanpy.read. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:72,testability,simpl,simply,72,At some point we moved scvelo's loom reading to anndata/scanpy and then simply called that one from within scvelo. . `scvelo.read` and `scanpy.read` are ecaxtly the same. And `read_loom` is called internally within `read`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1074:72,usability,simpl,simply,72,At some point we moved scvelo's loom reading to anndata/scanpy and then simply called that one from within scvelo. . `scvelo.read` and `scanpy.read` are ecaxtly the same. And `read_loom` is called internally within `read`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074
https://github.com/scverse/scanpy/issues/1077:44,availability,cluster,clusters,44,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:44,deployability,cluster,clusters,44,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:6,energy efficiency,heat,heatmap,6,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:64,modifiability,exten,extended,64,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:28,availability,cluster,clustermap,28,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:100,availability,cluster,clustermap,100,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:126,availability,cluster,clustermap,126,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:28,deployability,cluster,clustermap,28,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:86,deployability,api,api,86,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:100,deployability,cluster,clustermap,100,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:126,deployability,cluster,clustermap,126,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:86,integrability,api,api,86,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/issues/1077:86,interoperability,api,api,86,For this you can use `sc.pl.clustermap`. See: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.clustermap.html#scanpy.pl.clustermap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077
https://github.com/scverse/scanpy/pull/1078:14,safety,test,test,14,Added a quick test,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:14,testability,test,test,14,Added a quick test,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:91,modifiability,scal,scaling,91,"The point of making it in `sc.pl.embedding` is that if there is no image, then there is no scaling factor and it really becomes just a scatterplot. I could use another point size, but then I think it becomes yet another different behavior. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:240,reliability,Doe,Does,240,"The point of making it in `sc.pl.embedding` is that if there is no image, then there is no scaling factor and it really becomes just a scatterplot. I could use another point size, but then I think it becomes yet another different behavior. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:230,usability,behavi,behavior,230,"The point of making it in `sc.pl.embedding` is that if there is no image, then there is no scaling factor and it really becomes just a scatterplot. I could use another point size, but then I think it becomes yet another different behavior. Does it make sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:55,interoperability,specif,specific,55,"Sure, only problem is that theres now so much spatial-specific code in `embedding`. But we can fix that after this PR. Id still like to see this change though:. > Its good to have [a test], but it should of course also make sure that the flipping works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:186,safety,test,test,186,"Sure, only problem is that theres now so much spatial-specific code in `embedding`. But we can fix that after this PR. Id still like to see this change though:. > Its good to have [a test], but it should of course also make sure that the flipping works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1078:186,testability,test,test,186,"Sure, only problem is that theres now so much spatial-specific code in `embedding`. But we can fix that after this PR. Id still like to see this change though:. > Its good to have [a test], but it should of course also make sure that the flipping works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078
https://github.com/scverse/scanpy/pull/1079:246,availability,consist,consistency,246,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:53,deployability,version,version,53,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:196,deployability,version,version,196,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:53,integrability,version,version,53,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:196,integrability,version,version,196,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:53,modifiability,version,version,53,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:196,modifiability,version,version,196,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1079:246,usability,consist,consistency,246,> Thanks for the PR! Could you also add the relevant version restrictions [here](https://github.com/theislab/scanpy/blob/28498953092dc7cbecd0bd67380b1b060367d639/setup.py#L33)? Sure! I have added version restriction for cuml and cudf as well for consistency,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1079
https://github.com/scverse/scanpy/pull/1080:27,safety,review,review,27,"Hi @flying-sheep ,. Please review and let me know if anything needs to be done here. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:27,testability,review,review,27,"Hi @flying-sheep ,. Please review and let me know if anything needs to be done here. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:36,deployability,fail,fails,36,"@awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. Could you check please? This is certainly related to scipy 1.5. With scipy 1.4 the test works fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:36,reliability,fail,fails,36,"@awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. Could you check please? This is certainly related to scipy 1.5. With scipy 1.4 the test works fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:179,safety,test,test,179,"@awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. Could you check please? This is certainly related to scipy 1.5. With scipy 1.4 the test works fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:179,testability,test,test,179,"@awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. Could you check please? This is certainly related to scipy 1.5. With scipy 1.4 the test works fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:215,availability,error,error,215,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:38,deployability,fail,fails,38,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:283,deployability,releas,release,283,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:338,deployability,version,version-,338,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:386,deployability,fail,fail,386,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:415,deployability,releas,release,415,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:429,deployability,instal,install,429,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:338,integrability,version,version-,338,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:338,modifiability,version,version-,338,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:215,performance,error,error,215,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:38,reliability,fail,fails,38,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:377,reliability,doe,does,377,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:386,reliability,fail,fail,386,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:185,safety,test,test,185,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:215,safety,error,error,215,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:185,testability,test,test,185,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:215,usability,error,error,215,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`. > Could you check please? > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:69,deployability,api,api,69,"@awnimo ok, i see, thanks. Also for some reason this pr deletes docs/api/scanpy.external.rst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:69,integrability,api,api,69,"@awnimo ok, i see, thanks. Also for some reason this pr deletes docs/api/scanpy.external.rst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:69,interoperability,api,api,69,"@awnimo ok, i see, thanks. Also for some reason this pr deletes docs/api/scanpy.external.rst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:50,availability,restor,restore,50,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:127,deployability,build,building,127,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:167,deployability,api,api,167,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:167,integrability,api,api,167,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:167,interoperability,api,api,167,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:50,reliability,restor,restore,50,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:95,safety,test,test,95,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:95,testability,test,test,95,@Koncopd . This is weird never noticed. How can I restore it?? I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:28,availability,restor,restore,28,"@awnimo i'm not sure how to restore it, there were so many commits so git reset won't help. Hm, you can try copying this file back and commiting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:28,reliability,restor,restore,28,"@awnimo i'm not sure how to restore it, there were so many commits so git reset won't help. Hm, you can try copying this file back and commiting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:86,usability,help,help,86,"@awnimo i'm not sure how to restore it, there were so many commits so git reset won't help. Hm, you can try copying this file back and commiting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:29,deployability,api,api,29,@Koncopd . I added back docs/api/scanpy.external.rst. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:29,integrability,api,api,29,@Koncopd . I added back docs/api/scanpy.external.rst. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1080:29,interoperability,api,api,29,@Koncopd . I added back docs/api/scanpy.external.rst. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080
https://github.com/scverse/scanpy/pull/1081:94,deployability,log,logg,94,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:94,safety,log,logg,94,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:146,safety,test,test,146,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:232,safety,test,test,232,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:281,safety,test,test,281,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:347,safety,test,tests,347,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:442,safety,test,tests,442,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:94,security,log,logg,94,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:94,testability,log,logg,94,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:146,testability,test,test,146,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:232,testability,test,test,232,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:281,testability,test,test,281,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:347,testability,test,tests,347,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:442,testability,test,tests,442,"Thank you! This looks great! What do you think, @ivirshup? Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value? Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much! Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:27,safety,test,tests,27,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:93,safety,test,tests,93,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:180,safety,test,tests,180,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:27,testability,test,tests,27,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:93,testability,test,tests,93,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:180,testability,test,tests,180,Also there are 2 numerical tests here. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py. https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:226,performance,time,time,226,"Guys, now that you're dissecting rank_genes_groups, do you mind adding additional info to the results which is the fraction of cells expressing the genes (similar to what we have in dotplots) People calculate it manually ever time and it can be painful for those who are not familiar with pandas.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:5,deployability,fail,fails,5,Test fails seem unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:5,reliability,fail,fails,5,Test fails seem unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:0,safety,Test,Test,0,Test fails seem unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:0,testability,Test,Test,0,Test fails seem unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:94,usability,user,user-images,94,From [Seurat tutorial](https://satijalab.org/seurat/v3.0/de_vignette.html):. ![image](https://user-images.githubusercontent.com/1140359/76091681-09a3a080-5f8c-11ea-9c21-3c75c71f23cd.png). pct.1 and pct.2 are the ones I mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:152,safety,avoid,avoid,152,"@ivirshup thanks for catching these things, i'll fix them. . In addition to some splitting and cleaning i also want to improve wilcoxon implementation (avoid densification at least, i have some code for it). But this should be another step, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:125,safety,compl,completely,125,This is still very messy and very inefficient with this separate calculation of pts. I should work on it more to restructure completely...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/pull/1081:125,security,compl,completely,125,This is still very messy and very inefficient with this separate calculation of pts. I should work on it more to restructure completely...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081
https://github.com/scverse/scanpy/issues/1082:68,availability,down,download,68,"Oops, didn't mean to close this initially. The datasets still don't download right on master. I've opened #1102 to fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1082
https://github.com/scverse/scanpy/issues/1082:21,usability,close,close,21,"Oops, didn't mean to close this initially. The datasets still don't download right on master. I've opened #1102 to fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1082
https://github.com/scverse/scanpy/issues/1083:145,modifiability,paramet,parameter,145,check `adata.raw.X`. That's probably what `sc.pl.highest_expr_genes() is using by default. You should be able to turn this off via the `use_raw` parameter.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083
https://github.com/scverse/scanpy/issues/1083:147,modifiability,paramet,parameter,147,"> check `adata.raw.X`. That's probably what `sc.pl.highest_expr_genes() is using by default. You should be able to turn this off via the `use_raw` parameter. My question actually is: . After I ran sc.pp.filter_genes() and sc.pp.filter_cells(), why are there still zero rows or columns in the datasets, so that print(np.any(adata.X.sum(axis=0) == 0)) returns true? Shouldn't these zero row/columns/ be eliminated during the pp.filter_X steps? (since they are inplace by default.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083
https://github.com/scverse/scanpy/issues/1083:110,integrability,filter,filtering,110,"my guess is that if you first genes and then cells, some gene may now have zero counts. First do all the cell filtering and then do the gene filtering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083
https://github.com/scverse/scanpy/issues/1083:141,integrability,filter,filtering,141,"my guess is that if you first genes and then cells, some gene may now have zero counts. First do all the cell filtering and then do the gene filtering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083
https://github.com/scverse/scanpy/issues/1083:8,integrability,filter,filtering,8,"Yes, by filtering cells first, the problem gets solved. Thank you all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1083
https://github.com/scverse/scanpy/pull/1085:20,deployability,updat,updates,20,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/pull/1085:20,safety,updat,updates,20,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/pull/1085:31,safety,review,reviewing,31,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/pull/1085:10,security,team,team,10,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/pull/1085:20,security,updat,updates,20,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/pull/1085:31,testability,review,reviewing,31,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085
https://github.com/scverse/scanpy/issues/1086:31,availability,slo,slow,31,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:131,energy efficiency,model,modeling,131,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:256,energy efficiency,model,model,256,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:268,energy efficiency,model,model,268,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:31,reliability,slo,slow,31,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:70,safety,test,test,70,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:131,security,model,modeling,131,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:256,security,model,model,256,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:268,security,model,model,268,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:70,testability,test,test,70,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:104,deployability,releas,release,104,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:163,interoperability,specif,specifically,163,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:57,modifiability,pac,package,57,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/issues/1086:95,modifiability,pac,packages,95,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086
https://github.com/scverse/scanpy/pull/1088:768,availability,down,down,768,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:804,availability,sli,slices,804,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1064,availability,sli,slices,1064,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:524,deployability,observ,observations,524,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1044,deployability,version,version,1044,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1044,integrability,version,version,1044,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1044,modifiability,version,version,1044,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:339,reliability,doe,does,339,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:804,reliability,sli,slices,804,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1064,reliability,sli,slices,1064,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:524,testability,observ,observations,524,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:750,testability,plan,plans,750,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:783,usability,support,support,783,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1071,usability,support,support,1071,"That looks great Isaac, thanks a lot! > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ? > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:278,availability,sli,slices,278,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:793,availability,sli,slices,793,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1043,availability,sli,slices,1043,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1092,availability,sli,slides,1092,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1120,availability,sli,slides,1120,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1444,availability,sli,slides,1444,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:258,deployability,version,version,258,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:579,deployability,log,logic,579,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:690,deployability,Updat,Update,690,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:761,deployability,version,version,761,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1270,deployability,API,API,1270,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:335,energy efficiency,current,currently,335,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:258,integrability,version,version,258,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:600,integrability,transform,transformed,600,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:639,integrability,abstract,abstracted,639,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:761,integrability,version,version,761,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1270,integrability,API,API,1270,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:600,interoperability,transform,transformed,600,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:612,interoperability,coordinat,coordinates,612,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1270,interoperability,API,API,1270,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:258,modifiability,version,version,258,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:639,modifiability,abstract,abstracted,639,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:761,modifiability,version,version,761,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1456,performance,time,time,1456,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:278,reliability,sli,slices,278,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:793,reliability,sli,slices,793,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1043,reliability,sli,slices,1043,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1092,reliability,sli,slides,1092,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1120,reliability,sli,slides,1120,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1444,reliability,sli,slides,1444,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:579,safety,log,logic,579,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:690,safety,Updat,Update,690,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:579,security,log,logic,579,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:690,security,Updat,Update,690,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:579,testability,log,logic,579,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:285,usability,support,support,285,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:772,usability,support,support,772,"> what about `X_coords` ? Ha, I was mostly just trying to get rid of the `X_`! > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:324,availability,sli,slices,324,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1240,availability,sli,slices,1240,"ns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1502,availability,sli,slices,1502,"ther PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1551,availability,sli,slides,1551,"0x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1579,availability,sli,slides,1579,"ibrary_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1816,availability,sli,slices,1816,"ts for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1967,availability,sli,slide,1967,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2702,availability,sli,slides,2702,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:304,deployability,version,version,304,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:631,deployability,log,logic,631,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:991,deployability,pipelin,pipelines,991,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1124,deployability,Updat,Update,1124,"What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1195,deployability,version,version,1195,"I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1729,deployability,API,API,1729,"to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2007,deployability,contain,containing,2007,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,deployability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2235,deployability,updat,update,2235,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:387,energy efficiency,current,currently,387,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:304,integrability,version,version,304,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:652,integrability,transform,transformed,652,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:691,integrability,abstract,abstracted,691,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:766,integrability,transform,transformed,766,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:991,integrability,pipelin,pipelines,991,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1195,integrability,version,version,1195,"I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1729,integrability,API,API,1729,"to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,integrability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:652,interoperability,transform,transformed,652,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:664,interoperability,coordinat,coordinates,664,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:766,interoperability,transform,transformed,766,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:778,interoperability,coordinat,coordinates,778,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1729,interoperability,API,API,1729,"to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,interoperability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:304,modifiability,version,version,304,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:691,modifiability,abstract,abstracted,691,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1195,modifiability,version,version,1195,"I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,modifiability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2865,modifiability,inherit,inherits,2865,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2714,performance,time,time,2714,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:324,reliability,sli,slices,324,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1240,reliability,sli,slices,1240,"ns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1502,reliability,sli,slices,1502,"ther PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1551,reliability,sli,slides,1551,"0x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1579,reliability,sli,slides,1579,"ibrary_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structur",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1816,reliability,sli,slices,1816,"ts for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1967,reliability,sli,slide,1967,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,reliability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2702,reliability,sli,slides,2702,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:631,safety,log,logic,631,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:815,safety,input,inputs,815,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1124,safety,Updat,Update,1124,"What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2235,safety,updat,update,2235,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2536,safety,compl,completely,2536,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:631,security,log,logic,631,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1124,security,Updat,Update,1124,"What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,security,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2235,security,updat,update,2235,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2536,security,compl,completely,2536,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:631,testability,log,logic,631,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:800,testability,understand,understand,800,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2112,testability,integr,integration,2112,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:331,usability,support,support,331,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:815,usability,input,inputs,815,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:934,usability,support,support,934,"> > what about `X_coords` ? > . > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me! > . > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1219,usability,support,support,1219," to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. > . > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (wh",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1795,usability,support,support,1795," understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.emb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:2256,usability,support,supports,2256,"ou are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. . Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged. > Update: heard back, the `library_id` should be fine, at least for this version. > . good ! > > support for multiple slices should be first. > . > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. > . > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:. * most people don't work with one slide. * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure). . > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:348,availability,cluster,cluster,348,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:369,availability,cluster,cluster,369,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:348,deployability,cluster,cluster,348,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:369,deployability,cluster,cluster,369,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1663,deployability,API,API,1663,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:22,integrability,transform,transformed,22,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:684,integrability,sub,subgraph,684,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1072,integrability,compon,components,1072,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1663,integrability,API,API,1663,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:22,interoperability,transform,transformed,22,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:34,interoperability,coordinat,coordinates,34,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:94,interoperability,coordinat,coordinates,94,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:177,interoperability,coordinat,coordinates,177,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1072,interoperability,compon,components,1072,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1663,interoperability,API,API,1663,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:71,modifiability,scal,scaling,71,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1072,modifiability,compon,components,1072,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:1563,usability,behavi,behaviour,1563,"> What do you mean by transformed coordinates? Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? > I am very interested to see the applications of spatial connectivities you think can be useful. A few uses I can think of:. * Select all wells which are in cluster 1 and border cluster 0. <details>. <summary> Example code </summary>. ```python. g = sc._utils.get_igraph_from_adjacency(adata.obsp[""spatial_connectivity""]). g.vs[""obs_names""] = adata.obs_names. g.vs[""leiden""] = adata.obs[""leiden""]. (. g.es. .select(. _between=(. g.vs.select(leiden_eq=""0""),. g.vs.select(leiden_eq=""1""). ). ). .subgraph().vs. .select(leiden_eq=""1""). [""obs_names""]. ). # Alternative, but equivalent:. adata.obs_names[. (. adata.obsp[""spatial_connectivity""] @. (adata.obs[""leiden""] == ""0""). ) & . (adata.obs[""leiden""] == ""1""). ]. ```. </details>. * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). <details>. <summary> Example code </summary>. From #915. ```python. gearys_c(adata.obsp[""spatial_connectivity""], adata.X.T). ```. </details>. * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). --------------. For multiple spatial objects, I think that makes sense. Having the ability to concatenate makes sense as a priority, I was thinking more of functions that are aware of multiple tissues and having special behaviour for that w.r.t. their images. That's the stuff where I'd like have more confidence in the API before it's put in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:331,availability,cluster,cluster,331,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:352,availability,cluster,cluster,352,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:813,availability,cluster,clustering,813,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:331,deployability,cluster,cluster,331,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:352,deployability,cluster,cluster,352,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:813,deployability,cluster,clustering,813,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:160,energy efficiency,current,currently,160,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:539,integrability,compon,components,539,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:49,interoperability,coordinat,coordinates,49,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:132,interoperability,coordinat,coordinates,132,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:539,interoperability,compon,components,539,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:26,modifiability,scal,scaling,26,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/pull/1088:539,modifiability,compon,components,539,"> Don't you have to use a scaling factor for the coordinates to map them onto the image? And the maybe flip either the image or the coordinates? . Yes, this is currently done in `sc.pl.spatial` because it requires first to select the type of image to be plotted. > A few uses I can think of:. > . > * Select all wells which are in cluster 1 and border cluster 0. > . > Example code. > * Feature selection for genes which have more similar values in their neighbouring wells than would be expected by random (this works pretty well for NMF components too). > . > Example code. > * Select wells where some feature is expressed in a gradient (not sure what the elegant way to do this one is). > . Thanks, all really interesting applications indeed. I guess I was more thinking of analysis on the spatial graph (e.g. clustering). Ok so just to double-check, I will wait for this to get merge and then go on with `uns` and `sc.pl.spatial` fixes, makes sense?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088
https://github.com/scverse/scanpy/issues/1089:366,deployability,scale,scaled,366,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:366,energy efficiency,scale,scaled,366,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:112,integrability,transform,transformations,112,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:112,interoperability,transform,transformations,112,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:5,modifiability,layer,layers,5,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:225,modifiability,layer,layers,225,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:366,modifiability,scal,scaled,366,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:366,performance,scale,scaled,366,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:40,usability,close,closest,40,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python. adata.layers[""counts""] = adata.X.copy(). sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,deployability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:138,energy efficiency,reduc,reduce,138,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:59,integrability,filter,filtering,59,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,integrability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:363,integrability,batch,batch,363,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,interoperability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:251,modifiability,scal,scaling,251,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,modifiability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:363,performance,batch,batch,363,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,reliability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,security,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:351,testability,integr,integration,351,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:334,usability,help,helpful,334,"Hi @chansigit,. If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:88,availability,cluster,clustering,88,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:88,deployability,cluster,clustering,88,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:121,deployability,scale,scaled,121,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:229,deployability,scale,scaled,229,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:312,deployability,scale,scale,312,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:121,energy efficiency,scale,scaled,121,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:137,energy efficiency,heat,heatmaps,137,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:229,energy efficiency,scale,scaled,229,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:312,energy efficiency,scale,scale,312,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:56,modifiability,scal,scaling,56,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:121,modifiability,scal,scaled,121,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:207,modifiability,layer,layer,207,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:229,modifiability,scal,scaled,229,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:266,modifiability,layer,layer,266,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:273,modifiability,paramet,parameter,273,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:312,modifiability,scal,scale,312,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:121,performance,scale,scaled,121,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:229,performance,scale,scaled,229,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:312,performance,scale,scale,312,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/issues/1089:16,usability,help,help,16,"Thanks for your help. I double-checked my data and find scaling is not so important for clustering. But I do hope to use scaled data for heatmaps, it would be much cleaner. To achieve this, I hope to keep a layer that stores the scaled data. Could you please add a 'layer' parameter for sc.pp.log1p and sc.pp.scale so that we could stores them and compute them without affecting the adata.X?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089
https://github.com/scverse/scanpy/pull/1090:117,deployability,fail,failing,117,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/pull/1090:117,reliability,fail,failing,117,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/pull/1090:23,safety,review,review,23,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/pull/1090:111,safety,test,tests,111,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/pull/1090:23,testability,review,review,23,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/pull/1090:111,testability,test,tests,111,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090
https://github.com/scverse/scanpy/issues/1092:73,deployability,updat,update,73,"Hi, @sarajimenez . there is no ingest in scanpy 1.4.4.post1. You need to update scanpy to 1.4.5 and anndata to 0.7 to use ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:73,safety,updat,update,73,"Hi, @sarajimenez . there is no ingest in scanpy 1.4.4.post1. You need to update scanpy to 1.4.5 and anndata to 0.7 to use ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:73,security,updat,update,73,"Hi, @sarajimenez . there is no ingest in scanpy 1.4.4.post1. You need to update scanpy to 1.4.5 and anndata to 0.7 to use ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:167,availability,error,error,167,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:95,deployability,updat,updated,95,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:354,deployability,modul,module,354,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1297,deployability,fail,fails,1297,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:173,integrability,messag,message,173,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1097,integrability,transform,transform,1097,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1232,integrability,transform,transform,1232,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:173,interoperability,messag,message,173,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1097,interoperability,transform,transform,1097,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1232,interoperability,transform,transform,1232,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:354,modifiability,modul,module,354,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:453,modifiability,pac,packages,453,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:718,modifiability,pac,packages,718,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:974,modifiability,pac,packages,974,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1206,modifiability,pac,packages,1206,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:167,performance,error,error,167,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:1297,reliability,fail,fails,1297,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:95,safety,updat,updated,95,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:167,safety,error,error,167,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:327,safety,input,input-,327,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:354,safety,modul,module,354,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:95,security,updat,updated,95,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:283,testability,Trace,Traceback,283,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:167,usability,error,error,167,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:327,usability,input,input-,327,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:469,usability,tool,tools,469,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:734,usability,tool,tools,734,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:990,usability,tool,tools,990,"Hi, . I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain'). I have updated my Scanpy 1.4.6 and anndata to 0.7.1. I'm getting the following error message. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-71-27e22cc8f823> in <module>. ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs). 119 . 120 for method in embedding_method:. --> 121 ing.map_embedding(method). 122 . 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method). 407 """""". 408 if method == 'umap':. --> 409 self._obsm['X_umap'] = self._umap_transform(). 410 elif method == 'pca':. 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self). 396 . 397 def _umap_transform(self):. --> 398 return self._umap.transform(self._obsm['rep']). 399 . 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X). 2006 try:. 2007 # sklearn pairwise_distances fails for callable metric on sparse data. -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func. 2009 dmat = pairwise_distances(. 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'. ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:18,availability,Down,Downgrading,18,"Hi, @vikram0010 . Downgrading umap to 0.39 should help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1092:50,usability,help,help,50,"Hi, @vikram0010 . Downgrading umap to 0.39 should help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092
https://github.com/scverse/scanpy/issues/1094:28,deployability,version,version,28,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:61,deployability,instal,installed,61,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:78,deployability,version,version,78,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:94,deployability,version,version,94,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:121,deployability,version,version,121,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:28,integrability,version,version,28,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:78,integrability,version,version,78,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:94,integrability,version,version,94,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:121,integrability,version,version,121,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:28,modifiability,version,version,28,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:78,modifiability,version,version,78,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:94,modifiability,version,version,94,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:121,modifiability,version,version,121,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:40,performance,network,networkx,40,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:40,security,network,networkx,40,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1094:49,availability,down,downgrade,49,I read some forum threads saying that we need to downgrade matplotlib,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094
https://github.com/scverse/scanpy/issues/1098:24,deployability,releas,release,24,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:183,deployability,version,versions,183,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:183,integrability,version,versions,183,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:183,modifiability,version,versions,183,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:244,deployability,version,version,244,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:589,deployability,instal,installed,589,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:293,energy efficiency,heat,heatmap,293,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:244,integrability,version,version,244,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:244,modifiability,version,version,244,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:144,usability,learn,learn,144,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:375,usability,user,user-images,375,"When I import Scanpy, I go this output:. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:. sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:. ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png). GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:70,deployability,upgrad,upgrading,70,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:128,deployability,instal,install,128,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:293,deployability,releas,release,293,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:195,energy efficiency,heat,heatmaps,195,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:70,modifiability,upgrad,upgrading,70,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1098:26,deployability,instal,installed,26,"It worked! I guess when I installed pyvdj, I got 3.1.1. Many thx!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098
https://github.com/scverse/scanpy/issues/1103:90,testability,understand,understand,90,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. . Could you please let me know what kind of solution will be good for this issue? Thanks! . <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">. <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1103:55,usability,learn,learning,55,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. . Could you please let me know what kind of solution will be good for this issue? Thanks! . <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">. <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1103:336,usability,user,user-images,336,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. . Could you please let me know what kind of solution will be good for this issue? Thanks! . <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">. <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1103:503,usability,user,user-images,503,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. . Could you please let me know what kind of solution will be good for this issue? Thanks! . <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">. <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1103:98,deployability,instal,install,98,This is being fixed in #1210. for the time being you can try to set `dendrogram=False` or you can install the branch with the fix.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1103:38,performance,time,time,38,This is being fixed in #1210. for the time being you can try to set `dendrogram=False` or you can install the branch with the fix.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103
https://github.com/scverse/scanpy/issues/1104:140,deployability,updat,update,140,"Hi @cartal! I believe the spatial branch has been merged into master now. So you no longer need the `@spatial` in there. . @giovp could you update this in the tutorial, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104
https://github.com/scverse/scanpy/issues/1104:140,safety,updat,update,140,"Hi @cartal! I believe the spatial branch has been merged into master now. So you no longer need the `@spatial` in there. . @giovp could you update this in the tutorial, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104
https://github.com/scverse/scanpy/issues/1104:140,security,updat,update,140,"Hi @cartal! I believe the spatial branch has been merged into master now. So you no longer need the `@spatial` in there. . @giovp could you update this in the tutorial, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104
https://github.com/scverse/scanpy/issues/1104:21,security,modif,modify,21,Sorry for that. Will modify it soon but wanted to include new changes first.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104
https://github.com/scverse/scanpy/pull/1105:597,availability,mask,masked,597,"Thanks for the H/T @ivirshup ! I was able to find the library ids and I also added the metadata as suggested. This should do for now. I have also changed the plotting function to make it work with the new `uns` structure. The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:871,availability,mask,masked,871,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:1073,availability,mask,masked,1073,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:332,energy efficiency,current,current,332,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:1186,integrability,sub,subtypes,1186,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:421,interoperability,specif,specified,421,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:1015,interoperability,specif,specify,1015,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:323,modifiability,Maintain,Maintain,323,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:323,safety,Maintain,Maintain,323,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:436,security,ident,identical,436,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:340,usability,behavi,behaviour,340,"No problem! > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging. * `""unique""`: Only keep values which are uniquely specified. * `""identical""`: Only keep values which are the same in all objects. * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be? I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:79,availability,error,errors,79,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:119,availability,Error,Error,119,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:21,deployability,fail,failing,21,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:79,performance,error,errors,79,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:119,performance,Error,Error,119,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:21,reliability,fail,failing,21,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:5,safety,test,tests,5,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:79,safety,error,errors,79,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:119,safety,Error,Error,119,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:5,testability,test,tests,5,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:111,testability,assert,assert,111,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:79,usability,error,errors,79,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:119,usability,Error,Error,119,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:. ```. assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:147,interoperability,coordinat,coordinates,147,"@giovp or @flying-sheep, any idea how to get travis to run this? I'd like to see what's happening there. Also @giovp, were you going to change the coordinates `obsm` entry key `""coords""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:210,availability,consist,consistent,210,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:78,interoperability,specif,specified,78,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
