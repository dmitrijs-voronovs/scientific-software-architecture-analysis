id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/689:274,usability,perform,perform,274,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:700,usability,perform,perform,700,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7717,usability,Command,Command,7717,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7729,usability,Error,Error,7729,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/690:365,integrability,filter,filter,365,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:410,integrability,filter,filter,410,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:601,integrability,event,event,601,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:801,integrability,filter,filter,801,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:833,integrability,filter,filter,833,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:177,interoperability,standard,standards,177,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:220,interoperability,standard,standard,220,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:636,modifiability,paramet,parameters,636,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:592,safety,detect,detected,592,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/690:592,security,detect,detected,592,"How to get pileup images; Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690
https://github.com/google/deepvariant/issues/691:238,integrability,filter,filter,238,"Variant detected in VCF but not in bam ; Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files? Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38. Chr15:41570158 T>C. The depth is 14,9 from the VCF. This is the region in the bam:. ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase! Thanks!! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:8,safety,detect,detected,8,"Variant detected in VCF but not in bam ; Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files? Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38. Chr15:41570158 T>C. The depth is 14,9 from the VCF. This is the region in the bam:. ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase! Thanks!! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:114,safety,detect,detected,114,"Variant detected in VCF but not in bam ; Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files? Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38. Chr15:41570158 T>C. The depth is 14,9 from the VCF. This is the region in the bam:. ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase! Thanks!! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:8,security,detect,detected,8,"Variant detected in VCF but not in bam ; Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files? Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38. Chr15:41570158 T>C. The depth is 14,9 from the VCF. This is the region in the bam:. ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase! Thanks!! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/691:114,security,detect,detected,114,"Variant detected in VCF but not in bam ; Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files? Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38. Chr15:41570158 T>C. The depth is 14,9 from the VCF. This is the region in the bam:. ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase! Thanks!! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691
https://github.com/google/deepvariant/issues/692:39,modifiability,Pac,PacBio,39,"Somatic Variant Calling for Long Reads PacBio; Hi,. I used DeepVariant for PacBio Germline variant calling, Just wondering if there is separate method for Somatic Variant Call ( Tumor/Normal or Tumor Only like in Mutect2/Strelka ) in DeepVariant? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/692
https://github.com/google/deepvariant/issues/692:75,modifiability,Pac,PacBio,75,"Somatic Variant Calling for Long Reads PacBio; Hi,. I used DeepVariant for PacBio Germline variant calling, Just wondering if there is separate method for Somatic Variant Call ( Tumor/Normal or Tumor Only like in Mutect2/Strelka ) in DeepVariant? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/692
https://github.com/google/deepvariant/issues/693:228,integrability,pub,pub,228,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:687,integrability,sub,subtract,687,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:12,interoperability,coordinat,coordinate,12,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:23,interoperability,incompatib,incompatibility,23,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:559,interoperability,coordinat,coordinates,559,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:620,interoperability,coordinat,coordinate,620,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:713,interoperability,coordinat,coordinate,713,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:749,performance,time,time,749,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/693:758,usability,help,help,758,"GFF3 to BED coordinate incompatibility in DV RNA-seq case study; Hi, . I am trying to run DV using the RNA-seq case study as an example before my actual runs. I found that at some point you used:. `curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \. awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \. awk '!dup[$0]++' > data/chr20_CDS.bed`. To get the CDS coordinates from the GFF3 file. . - But isn't GFF3 a 1-based coordinate system while BED is a 0-based one? - Do we just need to subtract 1 from the start coordinate right? . Thanks for your time and help",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/693
https://github.com/google/deepvariant/issues/695:587,availability,Operat,Operating,587,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:251,deployability,version,version,251,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:290,deployability,contain,contains,290,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:659,deployability,version,version,659,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:677,deployability,Instal,Installation,677,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:991,deployability,contain,containing,991,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1141,deployability,contain,contains,1141,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1175,deployability,observ,observe,1175,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:251,integrability,version,version,251,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:659,integrability,version,version,659,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:0,modifiability,Deco,Decoy,0,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:251,modifiability,version,version,251,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:264,modifiability,deco,decoys,264,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:302,modifiability,deco,decoys,302,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:488,modifiability,deco,decoy,488,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:659,modifiability,version,version,659,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:924,modifiability,deco,decoys,924,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1002,modifiability,deco,decoys,1002,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1076,modifiability,paramet,parameters,1076,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1150,modifiability,deco,decoy,1150,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1234,reliability,Doe,Does,1234,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1031,safety,test,test,1031,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1255,safety,test,test,1255,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1291,safety,test,test,1291,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:768,testability,instrument,instrument,768,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1031,testability,test,test,1031,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1175,testability,observ,observe,1175,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1255,testability,test,test,1255,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1291,testability,test,test,1291,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/695:1466,testability,context,context,1466,"Decoy Reads are added to the wrong contig for hs37d5_decoy.fasta; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**. - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. . 2. Run deepvariant with default parameters to call the file. 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y. 4. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695
https://github.com/google/deepvariant/issues/696:176,availability,down,download,176,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:461,energy efficiency,GPU,GPU,461,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:461,performance,GPU,GPU,461,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:534,performance,memor,memory,534,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:52,safety,test,testing,52,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:316,safety,test,tested,316,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:353,safety,test,test,353,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:403,safety,test,test,403,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:52,testability,test,testing,52,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:316,testability,test,tested,316,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:353,testability,test,test,353,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:403,testability,test,test,403,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:159,usability,guid,guide,159,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:271,usability,guid,guideline,271,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:534,usability,memor,memory,534,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/696:718,usability,tool,tool,718,"Question about running DeepVariant on AWS; Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696
https://github.com/google/deepvariant/issues/697:29,interoperability,specif,specifying,29,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1308,reliability,doe,doesn,1308,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:69,safety,test,test,69,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:511,safety,input,input,511,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:659,safety,input,input,659,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:693,safety,input,input,693,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:893,safety,test,test,893,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:69,testability,test,test,69,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:893,testability,test,test,893,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:511,usability,input,input,511,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:659,usability,input,input,659,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:693,usability,input,input,693,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/697:1178,usability,command,command,1178,"Generating unusual output by specifying a region.; Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0"". . INPUT_DIR=""${PWD}/test_input"". OUTPUT_DIR=""${PWD}/test_output"". mkdir -p ""${OUTPUT_DIR}"". . docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.fasta \. --reads=/input/my_test.bam \. --regions chx:xxxxx-xxxxxx \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:. ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163). I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697
https://github.com/google/deepvariant/issues/698:448,modifiability,paramet,parameters,448,"retraining DeepVariant; I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? . Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:285,safety,valid,validation,285,"retraining DeepVariant; I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? . Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:788,safety,valid,validation,788,"retraining DeepVariant; I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? . Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:285,security,validat,validation,285,"retraining DeepVariant; I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? . Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/698:788,security,validat,validation,788,"retraining DeepVariant; I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? . Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/698
https://github.com/google/deepvariant/issues/699:116,usability,tool,tool,116,"question about DeepTrio; Hi,. I want to ask if DeepTrio provides an option to generate denovo variants. . I ran the tool using exact files provided in the tutorial, receive 3 vcf and 3gvcf but that's about it. How can I get the denovo variant from the child vcf output? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/699
https://github.com/google/deepvariant/issues/700:113,deployability,version,version,113,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:131,deployability,version,version,131,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:147,deployability,build,build,147,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:113,integrability,version,version,113,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:131,integrability,version,version,131,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:113,modifiability,version,version,113,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:131,modifiability,version,version,131,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:578,security,soc,social,578,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/700:408,usability,command,command,408,"When running, the community docker will pull deeptrio-1.5.0, but I want to run ordinary deepvariant; `$ docker --version`. `Docker version 24.0.5, build ced0996`. `$ sudo docker images`. `google/deepvariant deeptrio-1.5.0 aff53ed783a7 5 months ago 5.99GB`. `google/deepvariant 1.5.0 45f6c7767ff0 5 months ago 6.98GB`. Although I have pulled the ordinary DV, it will output when I run it with the recommended command，""unable to find image 'google/deepvariant:1.5.0' locally 1.5.0:Pulling from google/deepvariant"". Now the task is running. Should I terminate the task, delete the social docker, and reinstall it? Or wait for it to finish running? Dear developers, what do you think?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/700
https://github.com/google/deepvariant/issues/701:673,availability,Operat,Operating,673,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:68,deployability,observ,observing,68,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:715,deployability,version,version,715,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:755,deployability,Instal,Installation,755,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:746,energy efficiency,model,model,746,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:715,integrability,version,version,715,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:715,modifiability,version,version,715,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:746,security,model,model,746,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:68,testability,observ,observing,68,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:228,testability,coverag,coverage,228,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:851,testability,instrument,instrument,851,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/701:220,usability,minim,minimum,220,"Weird genotype calls based on RNAseq; **Describe the issue:**. I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | . | ------------- | ------------- | ------------- | ------------- | ------------- |. | 1/1 | 117,86 | 58,42,0 | 42 | 42. | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)? From what I've read, the AD values is calculated by chunks. **Setup**. - Operating system: CentOS 8. - DeepVariant version: 1.5.0, with 1.4.0 RNA model. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/701
https://github.com/google/deepvariant/issues/702:152,safety,valid,validation,152,"The principle of genotype interpretation; The principle of PL, and sometimes the genotype and mutation ratio may not match, and after one generation of validation, some genotype results are incorrect.How should I handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/702:152,security,validat,validation,152,"The principle of genotype interpretation; The principle of PL, and sometimes the genotype and mutation ratio may not match, and after one generation of validation, some genotype results are incorrect.How should I handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/702
https://github.com/google/deepvariant/issues/703:326,deployability,updat,update,326,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:4,energy efficiency,model,model,4,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:281,energy efficiency,current,current,281,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:293,energy efficiency,model,model,293,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:341,energy efficiency,model,model,341,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:159,performance,content,content,159,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:226,performance,perform,perform,226,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:209,reliability,doe,does,209,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:326,safety,updat,update,326,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:4,security,model,model,4,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:293,security,model,model,293,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:326,security,updat,update,326,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:341,security,model,model,341,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/703:226,usability,perform,perform,226,"WES model for Element AVITI™ System; Hello,. Deepvariant is reported to work well with WGS data from the Element AVITI™ System. #623 . https://www.biorxiv.org/content/10.1101/2023.08.11.553043v1.full.pdf. How does Deepvariant perform with AVITI WES data? Is it possible to use the current WES model or is it still required to update the WES model? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/703
https://github.com/google/deepvariant/issues/704:802,availability,Operat,Operating,802,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:94,deployability,pipelin,pipeline,94,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:841,deployability,version,version,841,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:881,deployability,Instal,Installation,881,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:914,deployability,version,version,914,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:94,integrability,pipelin,pipeline,94,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:841,integrability,version,version,841,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:914,integrability,version,version,914,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:841,modifiability,version,version,841,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:914,modifiability,version,version,914,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:403,performance,perform,perform,403,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:695,performance,time,time,695,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:546,reliability,doe,does,546,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:385,security,team,team,385,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:754,security,team,team,754,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:131,usability,guidanc,guidance,131,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/704:403,usability,perform,perform,403,"Question on trio analysis using DeepVariant and Deeptrio; . I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions! **Setup**. - Operating system: linux. - DeepVariant version: 1.5.0 (in Deeptrio as well). - Installation method: Singularity version. - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/704
https://github.com/google/deepvariant/issues/705:0,reliability,Doe,Does,0,"Does DeepVariant support scRNA-seq data, exp: 10x genome data？; Does DeepVariant support scRNA-seq data, exp: 10x genome data？",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:64,reliability,Doe,Does,64,"Does DeepVariant support scRNA-seq data, exp: 10x genome data？; Does DeepVariant support scRNA-seq data, exp: 10x genome data？",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:17,usability,support,support,17,"Does DeepVariant support scRNA-seq data, exp: 10x genome data？; Does DeepVariant support scRNA-seq data, exp: 10x genome data？",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/705:81,usability,support,support,81,"Does DeepVariant support scRNA-seq data, exp: 10x genome data？; Does DeepVariant support scRNA-seq data, exp: 10x genome data？",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/705
https://github.com/google/deepvariant/issues/706:453,deployability,log,log,453,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:910,deployability,log,log,910,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:563,energy efficiency,gpu,gpu,563,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:92,interoperability,specif,specifically,92,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:382,performance,time,time,382,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:414,performance,parallel,parallel,414,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:563,performance,gpu,gpu,563,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:453,safety,log,log,453,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:910,safety,log,log,910,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:453,security,log,log,453,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:910,security,log,log,910,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:148,testability,coverag,coverages,148,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:332,testability,coverag,coverages,332,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:453,testability,log,log,453,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:910,testability,log,log,910,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:168,usability,command,command,168,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/706:208,usability,command,command,208,"training on multiple samples; Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here? `( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once? Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/706
https://github.com/google/deepvariant/issues/707:19,interoperability,FORMAT,FORMAT,19,"question about vcf FORMAT; Hello! . I would like to know if DeepVariant can provide strand bias information. For example, by adding additional parameters to output Read depth for each allele on the forward strand and the reverse strand in the VCF file. Looking for your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/707:143,modifiability,paramet,parameters,143,"question about vcf FORMAT; Hello! . I would like to know if DeepVariant can provide strand bias information. For example, by adding additional parameters to output Read depth for each allele on the forward strand and the reverse strand in the VCF file. Looking for your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/707
https://github.com/google/deepvariant/issues/708:180,performance,content,content,180,"Force Genotyping; Sometimes in variant calling, it's necessary to force genotype all known sites regardless if a variant is found. Illumina has it:. - https://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/ForceGenotyping_fDG_dtSW.htm. GATK also has a similar option. How to do it in DeepVariant? I followed a DeepVariant tutorial at:. - https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Unfortunately it didn't look like working for me. For example, `chr1:783006` was in the `force_calling.candidates.vcf.gz` file but the site is not reported in the two output files: `HG003.output.g.vcf.gz` and `HG003.output.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:265,performance,Content,Content,265,"Force Genotyping; Sometimes in variant calling, it's necessary to force genotype all known sites regardless if a variant is found. Illumina has it:. - https://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/ForceGenotyping_fDG_dtSW.htm. GATK also has a similar option. How to do it in DeepVariant? I followed a DeepVariant tutorial at:. - https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Unfortunately it didn't look like working for me. For example, `chr1:783006` was in the `force_calling.candidates.vcf.gz` file but the site is not reported in the two output files: `HG003.output.g.vcf.gz` and `HG003.output.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:159,usability,support,support,159,"Force Genotyping; Sometimes in variant calling, it's necessary to force genotype all known sites regardless if a variant is found. Illumina has it:. - https://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/ForceGenotyping_fDG_dtSW.htm. GATK also has a similar option. How to do it in DeepVariant? I followed a DeepVariant tutorial at:. - https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Unfortunately it didn't look like working for me. For example, `chr1:783006` was in the `force_calling.candidates.vcf.gz` file but the site is not reported in the two output files: `HG003.output.g.vcf.gz` and `HG003.output.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:201,usability,support,support,201,"Force Genotyping; Sometimes in variant calling, it's necessary to force genotype all known sites regardless if a variant is found. Illumina has it:. - https://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/ForceGenotyping_fDG_dtSW.htm. GATK also has a similar option. How to do it in DeepVariant? I followed a DeepVariant tutorial at:. - https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Unfortunately it didn't look like working for me. For example, `chr1:783006` was in the `force_calling.candidates.vcf.gz` file but the site is not reported in the two output files: `HG003.output.g.vcf.gz` and `HG003.output.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/708:209,usability,help,help,209,"Force Genotyping; Sometimes in variant calling, it's necessary to force genotype all known sites regardless if a variant is found. Illumina has it:. - https://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/ForceGenotyping_fDG_dtSW.htm. GATK also has a similar option. How to do it in DeepVariant? I followed a DeepVariant tutorial at:. - https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Unfortunately it didn't look like working for me. For example, `chr1:783006` was in the `force_calling.candidates.vcf.gz` file but the site is not reported in the two output files: `HG003.output.g.vcf.gz` and `HG003.output.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/708
https://github.com/google/deepvariant/issues/709:120,performance,content,content,120,"Regarding the haplotagging paper; Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,. Paul.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:40,security,Team,Team,40,"Regarding the haplotagging paper; Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,. Paul.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:267,usability,clear,clear,267,"Regarding the haplotagging paper; Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,. Paul.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/709:323,usability,help,help,323,"Regarding the haplotagging paper; Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,. Paul.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/709
https://github.com/google/deepvariant/issues/710:152,availability,error,error,152,"merge gvcf file; I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/710:152,performance,error,error,152,"merge gvcf file; I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/710:152,safety,error,error,152,"merge gvcf file; I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/710:152,usability,error,error,152,"merge gvcf file; I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/710:294,usability,help,help,294,"merge gvcf file; I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/710
https://github.com/google/deepvariant/issues/711:240,availability,error,error,240,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:215,deployability,releas,release,215,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:246,integrability,messag,message,246,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:246,interoperability,messag,message,246,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:240,performance,error,error,240,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:240,safety,error,error,240,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:139,usability,tool,tool,139,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:240,usability,error,error,240,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/711:343,usability,custom,custom,343,"run deepvariant using ultima data; Hello,. I'm trying to run DeepVariant using ultima data (cram file). I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page. But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/711
https://github.com/google/deepvariant/issues/712:485,availability,state,statement,485,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:205,deployability,version,version,205,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:205,integrability,version,version,205,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:485,integrability,state,statement,485,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:205,modifiability,version,version,205,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:464,performance,time,time,464,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:867,reliability,Doe,Does,867,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:86,testability,understand,understand,86,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/712:584,usability,guidanc,guidance,584,"Newbie advice; Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly. I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus? Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/712
https://github.com/google/deepvariant/issues/713:135,modifiability,pac,pacbio,135,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:253,modifiability,paramet,parameter,253,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:271,modifiability,PAC,PACBIO,271,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:468,modifiability,paramet,parameter,468,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:491,modifiability,PAC,PACBIO,491,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/713:360,testability,understand,understanding,360,"calling variant; I'm sorry to bother you again, but I have a few more questions for you. . At present, the sequencing data we used are pacbio clr and ont r9.4. Can deepvariant be used to call varinat? If so, what should be selected for the --model_type parameter (HIFI / PACBIO ONT_R104). There is another question mentioned above. According to your reply, my understanding is as follows: When merging gvcf with glnexus, regardless of what deepvatriant's --model_type parameter selects (WGS,PACBIO,HIFI, etc.), glnexus's --config can select DeepVariantWGS. If not, please correct. Thank you again and look forward to your reply",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/713
https://github.com/google/deepvariant/issues/714:793,availability,sli,slightly,793,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:793,reliability,sli,slightly,793,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:874,security,sign,significant,874,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:126,usability,support,support,126,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:143,usability,document,documents,143,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:297,usability,intuit,intuitive,297,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:346,usability,document,document,346,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:901,usability,clear,clear,901,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/714:1068,usability,document,documenting,1068,"[question] interpreting PL in gVCF reference bands; RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/714
https://github.com/google/deepvariant/issues/715:39,deployability,updat,updated,39,"DeepTrio ONT; Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ? Thank you, regards.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/715:39,safety,updat,updated,39,"DeepTrio ONT; Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ? Thank you, regards.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/715:39,security,updat,updated,39,"DeepTrio ONT; Hello,. Will DeepTrio be updated for use with Oxford Nanopore data ? Thank you, regards.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/715
https://github.com/google/deepvariant/issues/716:1555,deployability,fail,fails,1555," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:490,energy efficiency,power,powerful,490,"Retraining DeepVariant using a long read truth set, can it be done?; Hello, . I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1624,energy efficiency,model,model,1624," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1792,energy efficiency,GPU,GPU,1792," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1939,energy efficiency,adapt,adapted,1939," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1939,integrability,adapt,adapted,1939," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1939,interoperability,adapt,adapted,1939," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1939,modifiability,adapt,adapted,1939," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1167,performance,time,time,1167,"lumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a tru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1792,performance,GPU,GPU,1792," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:975,reliability,doe,doesn,975,"Retraining DeepVariant using a long read truth set, can it be done?; Hello, . I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1555,reliability,fail,fails,1555," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1704,reliability,doe,does,1704," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:931,security,trust,trust,931,"Retraining DeepVariant using a long read truth set, can it be done?; Hello, . I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1624,security,model,model,1624," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1775,security,access,access,1775," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:158,testability,coverag,coverage,158,"Retraining DeepVariant using a long read truth set, can it be done?; Hello, . I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:720,testability,understand,understand,720,"Retraining DeepVariant using a long read truth set, can it be done?; Hello, . I think the answer is ""yes I could"". To be quick ... I have 25 datasets of high-coverage illumina data. But mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1337,testability,coverag,coverage,1337," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1442,testability,coverag,coverage,1442," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1488,testability,understand,understand,1488," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1998,testability,understand,understand,1998," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:2136,testability,understand,understand,2136," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:1182,usability,help,help,1182,"t mapping them on the rotifer genomes is a nightmare, it seems there are many mismappings. But ... we finally start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/716:2089,usability,clear,clear,2089," start to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant. I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right? EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/716
https://github.com/google/deepvariant/issues/717:1748,availability,Error,Error,1748,test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I t,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2948,availability,operat,operations,2948,"s.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2994,availability,operat,operations,2994,"auncher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4550,availability,cluster,cluster,4550,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:251,deployability,version,version,251,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:285,deployability,Instal,Installation,285,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3890,deployability,fail,failed,3890,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4550,deployability,cluster,cluster,4550,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2761,energy efficiency,core,core,2761,"f applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2827,energy efficiency,optim,optimized,2827,"exas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2907,energy efficiency,CPU,CPU,2907,"k 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:251,integrability,version,version,251,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:529,interoperability,bind,bind,529,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1146,interoperability,bind,bind,1146,":**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2119,interoperability,bind,bind,2119,"6 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2766,interoperability,platform,platform,2766,"cable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_sha",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3183,interoperability,bind,bind,3183,"t/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:251,modifiability,version,version,251,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:529,modifiability,bind,bind,529,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1146,modifiability,bind,bind,1146,":**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2119,modifiability,bind,bind,2119,"6 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3183,modifiability,bind,bind,3183,"t/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4203,modifiability,interm,intermediate,4203,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4367,modifiability,interm,intermediate,4367,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:20,performance,parallel,parallel,20,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1748,performance,Error,Error,1748,test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I t,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2827,performance,optimiz,optimized,2827,"exas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2861,performance,Network,Network,2861," Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2907,performance,CPU,CPU,2907,"k 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2927,performance,perform,performance-critical,2927,"2.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4518,performance,parallel,parallelly,4518,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3890,reliability,fail,failed,3890,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:756,safety,test,test,756,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:842,safety,test,test,842,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:926,safety,test,test,926,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1012,safety,test,test,1012,"riant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1090,safety,test,test,1090,"epvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.ut",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1373,safety,test,test,1373,"e genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsM",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1459,safety,test,test,1459,. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1543,safety,test,test,1543,ome1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1629,safety,test,test,1629,model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermedia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1707,safety,test,test,1707,asta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1748,safety,Error,Error,1748,test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I t,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1872,safety,compl,completed,1872, --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2014,safety,compl,completed,2014,"tput_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild Tenso",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2346,safety,test,test,2346,"***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2432,safety,test,test,2432,"***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2516,safety,test,test,2516,"***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2602,safety,test,test,2602,"***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2680,safety,test,test,2680,"***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3078,safety,compl,completed,3078,"6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3410,safety,test,test,3410,"**/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3496,safety,test,test,3496,"/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3580,safety,test,test,3580,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3666,safety,test,test,3666,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3744,safety,test,test,3744,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3800,safety,compl,completed,3800,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4027,safety,compl,completed,4027,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4261,safety,test,test,4261,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4425,safety,test,test,4425,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1872,security,compl,completed,1872, --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2014,security,compl,completed,2014,"tput_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild Tenso",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2861,security,Network,Network,2861," Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3078,security,compl,completed,3078,"6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3800,security,compl,completed,3800,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4027,security,compl,completed,4027,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:355,testability,instrument,instrument,355,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:756,testability,test,test,756,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:842,testability,test,test,842,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:926,testability,test,test,926,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1012,testability,test,test,1012,"riant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1090,testability,test,test,1090,"epvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.ut",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1373,testability,test,test,1373,"e genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsM",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1459,testability,test,test,1459,. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1543,testability,test,test,1543,ome1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1629,testability,test,test,1629,model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermedia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1707,testability,test,test,1707,asta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1754,testability,trace,trace,1754,est/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorf,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2346,testability,test,test,2346,"***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2432,testability,test,test,2432,"***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2516,testability,test,test,2516,"***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2602,testability,test,test,2602,"***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2680,testability,test,test,2680,"***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3410,testability,test,test,3410,"**/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3496,testability,test,test,3496,"/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3580,testability,test,test,3580,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3666,testability,test,test,3666,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:3744,testability,test,test,3744,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4261,testability,test,test,4261,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4425,testability,test,test,4425,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:4460,testability,context,context,4460,"gging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory. Launcher: Job 8 completed in 0 seconds. Launcher: Task 0 done. Exiting. Launcher: Task 1 done. Exiting. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:156,usability,clear,clear,156,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:465,usability,Command,Command,465,"Running deepvariant parallel runs ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Slurm based . - DeepVariant version: deepvariant_1.5.0.sif. - Installation method : singularity image . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command:. projDir=/home1/***/***/deepvaraint/. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). La",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:1748,usability,Error,Error,1748,test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash). Launcher: Job 1 completed in 0 seconds. Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I t,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/717:2927,usability,perform,performance-critical,2927,"2.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/). Launcher: Job 2 completed in 0 seconds. Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Launcher: Job 6 completed in 0 seconds. Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1). Launcher: Job 7 completed in 0 seconds. FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/717
https://github.com/google/deepvariant/issues/718:190,availability,error,error,190,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:67,deployability,version,version,67,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:67,integrability,version,version,67,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:196,integrability,messag,message,196,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:196,interoperability,messag,message,196,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:52,modifiability,Pac,PacBio,52,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:67,modifiability,version,version,67,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:221,modifiability,paramet,parameter,221,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:109,performance,perform,performed,109,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:190,performance,error,error,190,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:45,safety,test,tested,45,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:190,safety,error,error,190,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:45,testability,test,tested,45,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:109,usability,perform,performed,109,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/718:190,usability,error,error,190,"question about deepTrio1.5 ; Hello, . I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this? `docker pull google/deepvariant:deeptrio-1.5.0`. ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/718
https://github.com/google/deepvariant/issues/719:0,integrability,Filter,Filtering,0,"Filtering variants out if they show strand bias ; Not an issue! Just a question. Hello,. I was wondering if you knew of any ways to filter out variants based on strand bias? Is there a method to this - I can look visually at the BAM files but I have a lot of samples so wondered if anyone had done anything like this before - or whether there were any stats in the VCF outputs that would allow me to assess this? No worries if not! I thought I would just ask :). Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:132,integrability,filter,filter,132,"Filtering variants out if they show strand bias ; Not an issue! Just a question. Hello,. I was wondering if you knew of any ways to filter out variants based on strand bias? Is there a method to this - I can look visually at the BAM files but I have a lot of samples so wondered if anyone had done anything like this before - or whether there were any stats in the VCF outputs that would allow me to assess this? No worries if not! I thought I would just ask :). Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:400,security,assess,assess,400,"Filtering variants out if they show strand bias ; Not an issue! Just a question. Hello,. I was wondering if you knew of any ways to filter out variants based on strand bias? Is there a method to this - I can look visually at the BAM files but I have a lot of samples so wondered if anyone had done anything like this before - or whether there were any stats in the VCF outputs that would allow me to assess this? No worries if not! I thought I would just ask :). Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/719:213,usability,visual,visually,213,"Filtering variants out if they show strand bias ; Not an issue! Just a question. Hello,. I was wondering if you knew of any ways to filter out variants based on strand bias? Is there a method to this - I can look visually at the BAM files but I have a lot of samples so wondered if anyone had done anything like this before - or whether there were any stats in the VCF outputs that would allow me to assess this? No worries if not! I thought I would just ask :). Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/719
https://github.com/google/deepvariant/issues/720:114,availability,error,error,114,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:61,deployability,version,version,61,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:176,deployability,fail,failed,176,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:61,integrability,version,version,61,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:120,integrability,messag,message,120,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1387,integrability,sub,subprocess,1387,"_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1480,integrability,sub,subprocess,1480,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1561,integrability,sub,subprocess,1561,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1645,integrability,buffer,buffer,1645,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:120,interoperability,messag,message,120,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:46,modifiability,Pac,PacBio,46,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:61,modifiability,version,version,61,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1145,modifiability,pac,packages,1145,"inutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1245,modifiability,pac,packages,1245," hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:114,performance,error,error,114,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:157,performance,parallel,parallel,157,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1601,performance,time,time,1601,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1617,performance,parallel,parallel,1617,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:176,reliability,fail,failed,176,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:39,safety,test,tested,39,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:114,safety,error,error,114,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:39,testability,test,tested,39,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:990,testability,Trace,Traceback,990,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:114,usability,error,error,114,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:892,usability,user,user,892,"question about deepTrio1.4; hello, . I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:. /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1409,usability,command,command,1409,"amples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:1592,usability,Command,Command,1592,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/720:2393,usability,statu,status,2393,"s.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s. user 112m32.542s. sys 0m37.407s. I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG004"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz)"" --parse_sam_aux_fields --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 247. Fri Oct 20 05:16:03 UTC 2023 end. What went wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/720
https://github.com/google/deepvariant/issues/721:194,deployability,contain,contains,194,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:19,energy efficiency,model,model,19,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:152,energy efficiency,model,model,152,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:19,security,model,model,19,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:152,security,model,model,152,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:49,usability,tool,tool,49,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/721:76,usability,document,documentation,76,"training an hybrid model; Hi,. thank you for the tool and the very detailed documentation of it. I was wondering if there are details on how the hybrid model was trained. Is there any page that contains these details? thank you in advance,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/721
https://github.com/google/deepvariant/issues/722:0,availability,Error,Error,0,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:124,availability,error,error,124,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:979,availability,mainten,maintenance,979,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1063,availability,down,downstream,1063,"train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1228,availability,error,error,1228,"ream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2032,availability,checkpoint,checkpoints,2032," May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2155,availability,checkpoint,checkpoints,2155,". Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2249,availability,Checkpoint,Checkpoint,2249," saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2475,availability,Checkpoint,Checkpoint,2475,"84 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2486,availability,restor,restorefor,2486,"tils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_op",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2545,availability,restor,restore,2545,"etrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint coul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2601,availability,checkpoint,checkpoint,2601," be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2666,availability,Checkpoint,Checkpoint,2666,"66536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2892,availability,Checkpoint,Checkpoint,2892,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2903,availability,restor,restorefor,2903,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2962,availability,restor,restore,2962,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3008,availability,checkpoint,checkpoint,3008,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3045,availability,restor,restored,3045,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3123,availability,checkpoint,checkpoint,3123,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3151,availability,checkpoint,checkpoint,3151,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3188,availability,restor,restored,3188,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3256,availability,checkpoint,checkpoint,3256,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3293,availability,restor,restored,3293,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3386,availability,checkpoint,checkpoint,3386,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3414,availability,checkpoint,checkpoint,3414,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3451,availability,restor,restored,3451,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3534,availability,checkpoint,checkpoint,3534,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3571,availability,restor,restored,3571,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3667,availability,checkpoint,checkpoint,3667,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3695,availability,checkpoint,checkpoint,3695,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3732,availability,restor,restored,3732,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:756,deployability,instal,installed,756,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:995,deployability,releas,release,995,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1092,deployability,depend,dependencies,1092,"est 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2319,deployability,log,logs,2319,"but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2736,deployability,log,logs,2736," _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:290,energy efficiency,load,load,290,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:678,energy efficiency,GPU,GPU,678,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1243,energy efficiency,model,model,1243,"latform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1307,energy efficiency,load,loaded,1307," 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1314,energy efficiency,model,model,1314,"infer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the followin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1369,energy efficiency,model,model,1369,"en shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1438,energy efficiency,model,model,1438,"sr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1517,energy efficiency,load,loaded,1517,":00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the statu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1524,energy efficiency,model,model,1524,"064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1579,energy efficiency,model,model,1579,"s.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.9602",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1648,energy efficiency,model,model,1648,"u would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1972,energy efficiency,load,loading,1972,"al maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore functi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2231,energy efficiency,model,model,2231,". Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNIN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2648,energy efficiency,model,model,2648,"el. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3069,energy efficiency,optim,optimizer,3069,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3212,energy efficiency,optim,optimizer,3212,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3317,energy efficiency,optim,optimizer,3317,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3475,energy efficiency,optim,optimizer,3475,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3595,energy efficiency,optim,optimizer,3595,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3756,energy efficiency,optim,optimizer,3756,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:130,integrability,messag,messages,130,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1092,integrability,depend,dependencies,1092,"est 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1116,integrability,repositor,repositories,1116,"e error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] As",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:130,interoperability,messag,messages,130,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:245,interoperability,platform,platform,245,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:375,interoperability,share,shared,375,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1116,interoperability,repositor,repositories,1116,"e error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] As",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1267,interoperability,format,format,1267,"er.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2332,interoperability,specif,specific,2332,"d metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2749,interoperability,specif,specific,2749,"convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:806,modifiability,pac,packages,806,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1092,modifiability,depend,dependencies,1092,"est 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:0,performance,Error,Error,0,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:124,performance,error,error,124,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:290,performance,load,load,290,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:678,performance,GPU,GPU,678,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1228,performance,error,error,1228,"ream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1307,performance,load,loaded,1307," 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1517,performance,load,loaded,1517,":00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the statu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1972,performance,load,loading,1972,"al maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore functi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3069,performance,optimiz,optimizer,3069,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3212,performance,optimiz,optimizer,3212,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3317,performance,optimiz,optimizer,3317,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3475,performance,optimiz,optimizer,3475,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3595,performance,optimiz,optimizer,3595,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3756,performance,optimiz,optimizer,3756,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:979,reliability,mainten,maintenance,979,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2032,reliability,checkpoint,checkpoints,2032," May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2155,reliability,checkpoint,checkpoints,2155,". Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2249,reliability,Checkpoint,Checkpoint,2249," saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2475,reliability,Checkpoint,Checkpoint,2475,"84 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2486,reliability,restor,restorefor,2486,"tils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_op",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2545,reliability,restor,restore,2545,"etrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint coul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2601,reliability,checkpoint,checkpoint,2601," be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2666,reliability,Checkpoint,Checkpoint,2666,"66536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2892,reliability,Checkpoint,Checkpoint,2892,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2903,reliability,restor,restorefor,2903,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2962,reliability,restor,restore,2962,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3008,reliability,checkpoint,checkpoint,3008,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3045,reliability,restor,restored,3045,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3123,reliability,checkpoint,checkpoint,3123,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3151,reliability,checkpoint,checkpoint,3151,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3188,reliability,restor,restored,3188,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3256,reliability,checkpoint,checkpoint,3256,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3293,reliability,restor,restored,3293,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3386,reliability,checkpoint,checkpoint,3386,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3414,reliability,checkpoint,checkpoint,3414,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3451,reliability,restor,restored,3451,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3534,reliability,checkpoint,checkpoint,3534,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3571,reliability,restor,restored,3571,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3667,reliability,checkpoint,checkpoint,3667,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3695,reliability,checkpoint,checkpoint,3695,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:3732,reliability,restor,restored,3732,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:0,safety,Error,Error,0,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:124,safety,error,error,124,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1092,safety,depend,dependencies,1092,"est 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1228,safety,error,error,1228,"ream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2203,safety,Detect,Detecting,2203,"nishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2319,safety,log,logs,2319,"but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2620,safety,Detect,Detecting,2620," train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.moment",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2736,safety,log,logs,2736," _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1056,security,modif,modify,1056,"rying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1243,security,model,model,1243,"latform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1314,security,model,model,1314,"infer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the followin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1369,security,model,model,1369,"en shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1438,security,model,model,1438,"sr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1524,security,model,model,1524,"064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1579,security,model,model,1579,"s.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.9602",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1648,security,model,model,1648,"u would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2203,security,Detect,Detecting,2203,"nishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2231,security,model,model,2231,". Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNIN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2319,security,log,logs,2319,"but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2620,security,Detect,Detecting,2620," train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.moment",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2648,security,model,model,2648,"el. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2736,security,log,logs,2736," _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1016,testability,plan,planned,1016,"g with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /hom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1092,testability,depend,dependencies,1092,"est 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2319,testability,log,logs,2319,"but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2736,testability,log,logs,2736," _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:0,usability,Error,Error,0,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:124,usability,error,error,124,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:858,usability,User,UserWarning,858,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:971,usability,minim,minimal,971,"Error during training with V1.6.0; Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Ass",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:1228,usability,error,error,1228,"ream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format. WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2393,usability,statu,status,2393," be empty until you train or evaluate the model. W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2515,usability,statu,status,2515,"oaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2810,usability,statu,status,2810,"convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/722:2932,usability,statu,status,2932,"lution_op while saving (showing 5 of 94). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets. I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets. WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter. W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay. WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum. ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/722
https://github.com/google/deepvariant/issues/723:133,performance,disk,disk,133,"Emit PASSED Variants only; Dear Sir or Madame, . is it possible to emit only passed Variants to the output VCF file? This would save disk space and runtime for me. Bests. Stefan.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/723
https://github.com/google/deepvariant/issues/724:13,availability,fault,fault,13,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:223,availability,fault,fault,223,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:243,availability,Operat,Operating,243,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:439,availability,down,downloaded,439,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:465,availability,down,downloads,465,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:493,availability,down,downloads,493,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7337,availability,Error,Error,7337,"Un_KI270376v1 1136 0 0. chrUn_KI270374v1 2656 0 0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7385,availability,error,error,7385,"0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7405,availability,fault,fault,7405,"1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:311,deployability,version,version,311,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:328,deployability,Instal,Installation,328,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8409,deployability,modul,module,8409,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:13,energy efficiency,fault,fault,13,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:223,energy efficiency,fault,fault,223,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7405,energy efficiency,fault,fault,7405,"1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7412,energy efficiency,Current,Current,7412,". chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8764,energy efficiency,model,model,8764,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:311,integrability,version,version,311,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:781,integrability,sub,subsampled,781,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:311,modifiability,version,version,311,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8409,modifiability,modul,module,8409,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:13,performance,fault,fault,13,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:223,performance,fault,fault,223,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:1680,performance,content,content,1680,"[GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 622 0. chr11 135086622 538 0. chr12 133275309 478 0. chr13 114364328 409 0. chr14 107043718 340 0. chr15 101991189 379 0. chr16 90338345 409 0. chr17 83257441 432 0. chr18 80373285 308 0. chr19 58617616 223 0. chr20 64444167 330 0. chr21 46709983 284 0. chr22 50818468 203 0. chrX 156040895 290 0. chrY 57227415 292 0. chrM 16569 11 0. chr1_KI270706v1_random 175055 4 0. chr1_KI270707v1_random 32032 1 0. chr1_KI270708v1_random 127682 1 0. chr1_KI270709v1_random 66860 28 0. chr1_KI270710v1_random 40176 1 0. chr1_KI270711v1_random 42210 0 0. chr1_KI270712v1_random 176043 3 0. chr1_KI270713v1_random 40745 0 0. chr1_KI270714v1_random 41717 0 0. chr2_KI270715v1_random 161471 3 0. chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7337,performance,Error,Error,7337,"Un_KI270376v1 1136 0 0. chrUn_KI270374v1 2656 0 0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7385,performance,error,error,7385,"0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7405,performance,fault,fault,7405,"1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:13,reliability,fault,fault,13,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:223,reliability,fault,fault,223,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7405,reliability,fault,fault,7405,"1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8425,reliability,Doe,Does,8425,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:13,safety,fault,fault,13,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:223,safety,fault,fault,223,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7337,safety,Error,Error,7337,"Un_KI270376v1 1136 0 0. chrUn_KI270374v1 2656 0 0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7385,safety,error,error,7385,"0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7405,safety,fault,fault,7405,"1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8409,safety,modul,module,8409,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8446,safety,test,test,8446,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8482,safety,test,test,8482,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8764,security,model,model,8764,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:575,testability,instrument,instrument,575,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7343,testability,trace,trace,7343,"70376v1 1136 0 0. chrUn_KI270374v1 2656 0 0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8446,testability,test,test,8446,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8482,testability,test,test,8482,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:8710,testability,context,context,8710,"KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Unfortunately I cannot run Docker on my environment. **Any additional context:**. The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:850,usability,Command,Command,850,"Segmentation fault DeepTrio v1.6 for ONT duo; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. Yes. **Describe the issue:**. DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**. - Operating system:. Linux 3.10.0-1160.81.1.el7.x86_64. - DeepVariant version:. 1.6. - Installation method (Docker, built from source, etc.):. Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**. - Command:. ```. local args=(). args+=(""--model_type"" ""ONT""). args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""). args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""). args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""). args+=(""--sample_name_child"" ""HG002""). args+=(""--sample_name_parent1"" ""HG002_copy""). args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""). args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""). args+=(""--num_shards"" ""6""). args+=(""--regions"" ""regions_chunk_8.bed""). args+=(""--intermediate_results_dir"" ""intermediate_results""). args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""). args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}"". ```. content of .bed file:. ```. $ cat regions_chunk_8.bed. chr9 0 138394717. ```. stats of .bam file:. ```. chr1 248956422 1319 0. chr2 242193529 929 0. chr3 198295559 749 0. chr4 190214555 1042 0. chr5 181538259 649 0. chr6 170805979 667 0. chr7 159345973 613 0. chr8 145138636 622 0. chr9 138394717 586 0. chr10 133797422 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7337,usability,Error,Error,7337,"Un_KI270376v1 1136 0 0. chrUn_KI270374v1 2656 0 0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/724:7385,usability,error,error,7385,"0. chrUn_KI270372v1 1650 0 0. chrUn_KI270373v1 1451 0 0. chrUn_KI270375v1 2378 0 0. chrUn_KI270371v1 2805 0 0. chrUn_KI270448v1 7992 0 0. chrUn_KI270521v1 7642 0 0. chrUn_GL000195v1 182896 4 0. chrUn_GL000219v1 179198 2 0. chrUn_GL000220v1 161802 37 0. chrUn_GL000224v1 179693 6 0. chrUn_KI270741v1 157432 1 0. chrUn_GL000226v1 15008 48 0. chrUn_GL000213v1 164239 1 0. chrUn_KI270743v1 210658 3 0. chrUn_KI270744v1 168472 13 0. chrUn_KI270745v1 41891 0 0. chrUn_KI270746v1 66486 2 0. chrUn_KI270747v1 198735 2 0. chrUn_KI270748v1 93321 2 0. chrUn_KI270749v1 158759 0 0. chrUn_KI270750v1 148850 0 0. chrUn_KI270751v1 150742 4 0. chrUn_KI270752v1 27745 0 0. chrUn_KI270753v1 62944 1 0. chrUn_KI270754v1 40191 0 0. chrUn_KI270755v1 36723 0 0. chrUn_KI270756v1 79590 3 0. chrUn_KI270757v1 71251 2 0. chrUn_GL000214v1 137718 9 0. chrUn_KI270742v1 186739 9 0. chrUn_GL000216v2 176608 17 0. chrUn_GL000218v1 161147 3 0. chrEBV 171823 10 0. * 0 0 0. ```. - Error trace: (if applicable). ```. Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/724
https://github.com/google/deepvariant/issues/725:120,availability,error,error,120,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:590,availability,Error,Error,590,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:839,availability,checkpoint,checkpoint,839,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1086,availability,mainten,maintenance,1086," keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1170,availability,down,downstream,1170,"latform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Trac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3980,availability,error,error,3980,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4010,availability,error,error,4010,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1102,deployability,releas,release,1102,"the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1199,deployability,depend,dependencies,1199,"you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2328,deployability,modul,module,2328,"tps://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4117,deployability,modul,module,4117,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:56,energy efficiency,model,model,56,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:553,energy efficiency,model,model,553,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:851,energy efficiency,model,model,851,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1904,energy efficiency,model,model,1904,"8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/wor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2015,energy efficiency,model,model,2015,"lopment and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2866,energy efficiency,model,model,2866,"40 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 12",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2874,energy efficiency,model,modeling,2874,"ariants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3205,energy efficiency,model,model,3205,"ile ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:126,integrability,messag,message,126,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:596,integrability,messag,message,596,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1199,integrability,depend,dependencies,1199,"you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1223,integrability,repositor,repositories,1223,"went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3761,integrability,wrap,wrapper,3761,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3832,integrability,wrap,wrapper,3832,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3986,integrability,messag,message,3986,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:126,interoperability,messag,message,126,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:174,interoperability,platform,platform,174,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:596,interoperability,messag,message,596,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1223,interoperability,repositor,repositories,1223,"went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3761,interoperability,wrapper,wrapper,3761,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3832,interoperability,wrapper,wrapper,3832,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3986,interoperability,messag,message,3986,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:914,modifiability,pac,packages,914,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1199,modifiability,depend,dependencies,1199,"you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1948,modifiability,pac,packages,1948,"ol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2328,modifiability,modul,module,2328,"tps://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3279,modifiability,pac,packages,3279,"deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packag",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3431,modifiability,pac,packages,3431,", in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3586,modifiability,pac,packages,3586,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4117,modifiability,modul,module,4117,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4177,modifiability,pac,packages,4177,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4277,modifiability,pac,packages,4277,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:120,performance,error,error,120,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:590,performance,Error,Error,590,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:644,performance,time,time,644,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3921,performance,synch,synchronously,3921,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3958,performance,lock,lock,3958,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3980,performance,error,error,3980,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4010,performance,error,error,4010,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:839,reliability,checkpoint,checkpoint,839,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1086,reliability,mainten,maintenance,1086," keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:42,safety,test,tested,42,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:120,safety,error,error,120,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:151,safety,test,test,151,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:343,safety,Input,Input,343,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:590,safety,Error,Error,590,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1199,safety,depend,dependencies,1199,"you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1651,safety,input,input,1651,"deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1694,safety,input,input,1694,"tput/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1803,safety,input,input,1803,"ake_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2044,safety,input,input,2044,"ew features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2105,safety,input,input,2105,"e mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2328,safety,modul,module,2328,"tps://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3980,safety,error,error,3980,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3997,safety,Input,Input,3997,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4010,safety,error,error,4010,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4117,safety,modul,module,4117,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:56,security,model,model,56,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:553,security,model,model,553,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:851,security,model,model,851,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1163,security,modif,modify,1163," the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_sha",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1904,security,model,model,1904,"8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/wor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2015,security,model,model,2015,"lopment and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2866,security,model,model,2866,"40 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 12",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2874,security,model,modeling,2874,"ariants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3205,security,model,model,3205,"ile ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3958,security,lock,lock,3958,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:42,testability,test,tested,42,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:151,testability,test,test,151,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1123,testability,plan,planned,1123,"essage. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1199,testability,depend,dependencies,1199,"you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2171,testability,Trace,Traceback,2171,"ream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4019,testability,Trace,Traceback,4019,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:120,usability,error,error,120,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:343,usability,Input,Input,343,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:590,usability,Error,Error,590,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:629,usability,command,command,629,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:966,usability,User,UserWarning,966,"question about deepvariant 1.6; Hello,. I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: Use",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1078,usability,minim,minimal,1078,"1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong? My cmd:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --ref ${fasta} \. --reads ${Input.bam} \. --output_vcf output/output.vcf.gz \. --output_gvcf output/output.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir output/intermediate_results_dir \. --regions chr20 \. --customized_model model/weights-51-0.995354.ckpt. ```. Error message:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1651,usability,input,input,1651,"deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1694,usability,input,input,1694,"tput/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1803,usability,input,input,1803,"ake_examples.tfreco. rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:1997,usability,User,UserWarning,1997," has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2044,usability,input,input,2044,"ew features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:2105,usability,input,input,2105,"e mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started. I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]. I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. Traceback (most recent call last):. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3980,usability,error,error,3980,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:3997,usability,Input,Input,3997,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/725:4010,usability,error,error,4010,"s_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants. model = modeling.inceptionv3(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3. backbone = add_l2_regularizers(. File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers. model.save_weights(tmp_weights_path). File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler. raise e.with_traceback(filtered_tb) from None. File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__. fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr). File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid. fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl). File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper. File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper. File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create. OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'). Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main. for line in proc.stdout:. KeyboardInterrupt. ```. Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/725
https://github.com/google/deepvariant/issues/726:249,availability,error,error,249,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:388,availability,SLI,SLIM,388,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:445,availability,error,errors,445,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:664,availability,error,errors,664,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:515,deployability,version,version,515,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:143,energy efficiency,model,model,143,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:298,energy efficiency,model,model,298,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:342,energy efficiency,model,model,342,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:393,energy efficiency,model,model,393,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:551,energy efficiency,model,model,551,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:515,integrability,version,version,515,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:260,interoperability,prox,proxy,260,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:463,modifiability,paramet,parameters,463,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:515,modifiability,version,version,515,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:249,performance,error,error,249,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:445,performance,error,errors,445,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:664,performance,error,errors,664,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:388,reliability,SLI,SLIM,388,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:249,safety,error,error,249,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:445,safety,error,errors,445,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:500,safety,except,except,500,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:664,safety,error,errors,664,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:143,security,model,model,143,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:298,security,model,model,298,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:342,security,model,model,342,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:393,security,model,model,393,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:551,security,model,model,551,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:624,security,sign,sign,624,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:635,security,sign,significantly,635,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:249,usability,error,error,249,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:445,usability,error,errors,445,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:664,usability,error,errors,664,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/726:710,usability,help,help,710,"Results are different between v1.5 and v1.6; Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%. Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version. It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this? Any help and discussion would be appreciated. Thank you. Zuyao.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/726
https://github.com/google/deepvariant/issues/727:305,availability,error,error,305,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:335,availability,error,error,335,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:789,availability,Error,Error,789,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:990,availability,state,state,990,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3856,availability,error,error,3856,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3930,availability,error,error,3930,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3979,availability,error,error,3979,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:33,deployability,build,building,33,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:145,deployability,build,build,145,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:179,deployability,build,build-prereq,179,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1494,deployability,instal,install-,1494,"ning from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1585,deployability,modul,module,1585,"nd, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1617,deployability,instal,install-,1617,"thon sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1758,deployability,instal,install-,1758,"it: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1939,deployability,instal,install-,1939,"val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2091,deployability,instal,install-,2091,"---------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2436,deployability,fail,fail,2436,"pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2688,deployability,modul,module,2688,""", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3099,deployability,build,build-env-,3099,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3261,deployability,build,build-env-,3261,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3414,deployability,build,build-env-,3414,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3600,deployability,modul,module,3600,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3775,deployability,fail,failed,3775,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3818,deployability,fail,failed,3818,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3957,deployability,fail,failed,3957,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:312,integrability,sub,subprocess-exited-with-error,312,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:990,integrability,state,state,990,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2193,integrability,sub,subprocess,2193," nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2253,integrability,sub,subprocess,2253,"values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2334,integrability,sub,subprocess,2334,"ept? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3880,integrability,sub,subprocess,3880,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:451,interoperability,prox,proxychains,451,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:474,interoperability,prox,proxychains-ng,474,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:698,interoperability,prox,proxychains,698,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:721,interoperability,prox,proxychains-ng,721,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:743,interoperability,prox,proxychains,743,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:766,interoperability,prox,proxychains-ng,766,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1269,interoperability,incompatib,incompatible,1269,"much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1585,modifiability,modul,module,1585,"nd, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2608,modifiability,pac,packages,2608,"/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. set",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2688,modifiability,modul,module,2688,""", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2743,modifiability,pac,packages,2743," ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise Runt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2921,modifiability,pac,packages,2921,"ile ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3145,modifiability,pac,packages,3145,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3307,modifiability,pac,packages,3307,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3460,modifiability,pac,packages,3460,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3600,modifiability,modul,module,3600,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:4002,modifiability,pac,package,4002,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:305,performance,error,error,305,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:335,performance,error,error,335,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:789,performance,Error,Error,789,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3856,performance,error,error,3856,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3930,performance,error,error,3930,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3979,performance,error,error,3979,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:235,reliability,Doe,Does,235,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2436,reliability,fail,fail,2436,"pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3775,reliability,fail,failed,3775,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3818,reliability,fail,failed,3818,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3957,reliability,fail,failed,3957,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:305,safety,error,error,305,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:335,safety,error,error,335,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:789,safety,Error,Error,789,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1188,safety,except,except,1188,".sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1248,safety,Except,Exception,1248,"lp me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1336,safety,except,except,1336,"r. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subpr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1585,safety,modul,module,1585,"nd, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2688,safety,modul,module,2688,""", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3600,safety,modul,module,3600,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3856,safety,error,error,3856,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3930,safety,error,error,3930,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3979,safety,error,error,3979,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1443,testability,Trace,Traceback,1443,". [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2534,testability,Trace,Traceback,2534,"7acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2853,testability,hook,hook,2853,"ocess_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3041,testability,hook,hook,3041,"ssor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:221,usability,user,user,221,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:251,usability,help,help,251,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:305,usability,error,error,305,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:335,usability,error,error,335,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:546,usability,User,UserWarning,546,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:583,usability,command,command,583,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:789,usability,Error,Error,789,"There is something wrong while I building DeepVariant from source; I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error. . × Preparing metadata (pyproject.toml) did not run successfully. │ exit code: 1. ╰─> [57 lines of output]. [proxychains] DLL init: proxychains-ng 4.16. Running from numpy source directory. setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1550,usability,tool,tools,1550,"ning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates. run_build = parse_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most rec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1673,usability,tool,tools,1673,"_setuppy_commands(). [proxychains] DLL init: proxychains-ng 4.16. [proxychains] DLL init: proxychains-ng 4.16. . Error compiling Cython file:. ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1814,usability,tool,tools,1814,". ------------------------------------------------------------. ... for i in range(1, RK_STATE_LEN):. self.rng_state.key[i] = val[i]. self.rng_state.pos = i. . self._bitgen.state = &self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:1995,usability,tool,tools,1995,"&self.rng_state. self._bitgen.next_uint64 = &mt19937_uint64. ^. ------------------------------------------------------------. . _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2147,usability,tool,tools,2147,"not assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'. Processing numpy/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2365,usability,Command,Command,2365,"py/random/_bounded_integers.pxd.in. Processing numpy/random/_mt19937.pyx. Traceback (most recent call last):. File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. su",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:2503,usability,statu,status,2503,"z1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>. main(). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main. find_process_files(root_dir). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files. process(root_dir, fromfile, tofile, function, hash_db). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process. processor_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3856,usability,error,error,3856,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3930,usability,error,error,3930,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/727:3979,usability,error,error,3979,"or_function(fromfile, tofile). File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx. subprocess.check_call(. File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1. Cythonizing sources. Traceback (most recent call last):. File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>. main(). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main. json_out['return_val'] = hook(**hook_input['kwargs']). File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel. return hook(metadata_directory, config_settings). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel. self.run_setup(). File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup. super(_BuildMetaLegacyBackend,. File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup. exec(compile(code, __file__, 'exec'), locals()). File ""setup.py"", line 499, in <module>. setup_package(). File ""setup.py"", line 479, in setup_package. generate_cython(). File ""setup.py"", line 274, in generate_cython. raise RuntimeError(""Running cythonize failed!""). RuntimeError: Running cythonize failed! [end of output]. . note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed. × Encountered error while generating package metadata. ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/727
https://github.com/google/deepvariant/issues/728:1057,availability,consist,consistent,1057,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:877,deployability,observ,observer,877,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:924,energy efficiency,model,model,924,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:1103,energy efficiency,model,model,1103,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:987,integrability,FILTER,FILTER,987,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:576,interoperability,standard,standards,576,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:619,interoperability,standard,standard,619,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:229,modifiability,paramet,parameters,229,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:293,modifiability,paramet,parameters,293,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:646,modifiability,paramet,parameters,646,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:141,safety,except,except,141,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:729,security,ident,identify,729,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:924,security,model,model,924,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:1103,security,model,model,1103,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:185,testability,coverag,coverages,185,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:877,testability,observ,observer,877,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/728:1057,usability,consist,consistent,1057,"questions about retraining DeepVariant; Hi,. I‘ve been working on training Deepvaraint using mulitiple BAM files for samples HG001 to HG007, except HG003 for evaluation, with different coverages. I’ve experimented with different parameters for the **make_examples** step, either using default parameters or adding **min_base_quality=5, vsc_min_Fraction_snps 0.02, vsc_min_count_snps 2**. However, I am still not able to get the expected low VAF variants for our evaluation sample. We use Horizon sample and here is the truthset of it https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard. . By setting the parameters **vsc_min_Fraction_snps 0.02** and **min_base_quality=5**, I am able to identify 9 variants out of 14 variants. However, the majority of these variants are considered as REF variants. In the attached screenshot, you can observer the result generated by the retrained model. There is only one change in variant **BRAF** that the **FILTER** value goes from **RefCall** to **PASS** and the rest remains consistent with those produced by the default model. . I am seeking for strategies to increase number of candidate variants generated, with the goal of capturing missing variants. Your insights or suggestions would be greatly appreciated. ![image](https://github.com/google/deepvariant/assets/135994439/f537faae-220c-476b-ab0b-d5dec2d45597).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/728
https://github.com/google/deepvariant/issues/729:27,energy efficiency,model,model,27,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:97,energy efficiency,model,model,97,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:8,modifiability,Pac,PacBio,8,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:122,modifiability,Pac,PacBio,122,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:27,security,model,model,27,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:97,security,model,model,97,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/729:75,testability,plan,plans,75,"ONT and PacBio hifi hybrid model; Hello,. I was wondering if there are any plans to add a hybrid model which uses ONT and PacBio hifi reads. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/729
https://github.com/google/deepvariant/issues/730:113,availability,error,error,113,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:44,deployability,build,build,44,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:76,deployability,build,build-prereq,76,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:193,deployability,modul,module,193,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:428,deployability,instal,install,428,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:443,deployability,Instal,InstallCommand,443,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:592,deployability,modul,module,592,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:734,deployability,modul,module,734,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:874,deployability,modul,module,874,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1010,deployability,modul,module,1010,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1117,deployability,configurat,configuration,1117,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1149,deployability,modul,module,1149,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1262,deployability,log,logging,1262,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1288,deployability,modul,module,1288,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1424,deployability,modul,module,1424,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1568,deployability,modul,module,1568,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1714,deployability,modul,module,1714,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1723,deployability,Modul,ModuleNotFoundError,1723,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1747,deployability,modul,module,1747,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1117,integrability,configur,configuration,1117,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:193,modifiability,modul,module,193,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:592,modifiability,modul,module,592,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:734,modifiability,modul,module,734,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:874,modifiability,modul,module,874,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1010,modifiability,modul,module,1010,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1117,modifiability,configur,configuration,1117,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1149,modifiability,modul,module,1149,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1288,modifiability,modul,module,1288,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1424,modifiability,modul,module,1424,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1568,modifiability,modul,module,1568,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1714,modifiability,modul,module,1714,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1723,modifiability,Modul,ModuleNotFoundError,1723,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1747,modifiability,modul,module,1747,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:113,performance,error,error,113,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:113,safety,error,error,113,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:193,safety,modul,module,193,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:592,safety,modul,module,592,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:734,safety,modul,module,734,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:874,safety,modul,module,874,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1010,safety,modul,module,1010,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1149,safety,modul,module,1149,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1262,safety,log,logging,1262,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1288,safety,modul,module,1288,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1424,safety,modul,module,1424,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1568,safety,modul,module,1568,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1714,safety,modul,module,1714,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1723,safety,Modul,ModuleNotFoundError,1723,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1747,safety,modul,module,1747,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1117,security,configur,configuration,1117,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1262,security,log,logging,1262,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:122,testability,Trace,Traceback,122,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:1262,testability,log,logging,1262,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:113,usability,error,error,113,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:419,usability,command,commands,419,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/730:557,usability,command,commands,557,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error. ; Traceback (most recent call last):. File ""get-pip.py"", line 32992, in <module>. main(). File ""get-pip.py"", line 135, in main. bootstrap(tmpdir=tmpdir). File ""get-pip.py"", line 111, in bootstrap. monkeypatch_for_cert(tmpdir). File ""get-pip.py"", line 92, in monkeypatch_for_cert. from pip._internal.commands.install import InstallCommand. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>. File ""<frozen zipimport>"", line 259, in load_module. File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>. ModuleNotFoundError: No module named 'distutils.cmd'.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/730
https://github.com/google/deepvariant/issues/731:28,interoperability,specif,specific,28,"[Question] Force Genotyping specific sites or call from existing gvcf files.; Greetings,. I have gvcf files already generated from deepvariant and I have a vcf file of bi-allelic snps Im interested in genotyping. Ive been trying to find if a method exists to genotype snp sites from the gvcf likelihoods. Otherwise, is there an argument to include specific snp sites to genotype in a deepvariant run? From example, HaplotypeCaller had an --alleles option to ""...force-call regardless of evidence"" and I was looking for a deepvariant equivalent. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/731:348,interoperability,specif,specific,348,"[Question] Force Genotyping specific sites or call from existing gvcf files.; Greetings,. I have gvcf files already generated from deepvariant and I have a vcf file of bi-allelic snps Im interested in genotyping. Ive been trying to find if a method exists to genotype snp sites from the gvcf likelihoods. Otherwise, is there an argument to include specific snp sites to genotype in a deepvariant run? From example, HaplotypeCaller had an --alleles option to ""...force-call regardless of evidence"" and I was looking for a deepvariant equivalent. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/731
https://github.com/google/deepvariant/issues/732:82,integrability,sub,submitted,82,Why does the reference/alternate not add up to the read depth column?; Sorry I've submitted this wrong! But basically I was wondering why sometimes the reference/alternate read depth column values do not always add up to the read depth column such as here: . ![image](https://github.com/google/deepvariant/assets/110385188/818edea3-82d3-4877-9ab6-963226f12706).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/732:4,reliability,doe,does,4,Why does the reference/alternate not add up to the read depth column?; Sorry I've submitted this wrong! But basically I was wondering why sometimes the reference/alternate read depth column values do not always add up to the read depth column such as here: . ![image](https://github.com/google/deepvariant/assets/110385188/818edea3-82d3-4877-9ab6-963226f12706).,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/732
https://github.com/google/deepvariant/issues/733:390,availability,error,error,390,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:572,availability,Operat,Operating,572,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1306,availability,Error,Error,1306," fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1537,availability,checkpoint,checkpoint,1537,"lease help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1769,availability,mainten,maintenance,1769,"ference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1853,availability,down,downstream,1853,"ick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(max",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3690,availability,error,error,3690,"s.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json. gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:634,deployability,version,version,634,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:652,deployability,Instal,Installation,652,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1785,deployability,releas,release,1785,", anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1882,deployability,depend,dependencies,1882,"ce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2226,deployability,modul,module,2226,"t.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1554,energy efficiency,model,models,1554,". **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:634,integrability,version,version,634,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1882,integrability,depend,dependencies,1882,"ce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1906,integrability,repositor,repositories,1906,"udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2759,integrability,Queue,Queue,2759,"inimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2835,integrability,Queue,Queue,2835," Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2849,integrability,Queue,Queue,2849," downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2930,integrability,queue,queues,2930,"low community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1906,interoperability,repositor,repositories,1906,"udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:634,modifiability,version,version,634,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1597,modifiability,pac,packages,1597,"erprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1882,modifiability,depend,dependencies,1882,"ce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2226,modifiability,modul,module,2226,"t.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:390,performance,error,error,390,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1306,performance,Error,Error,1306," fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1374,performance,time,time,1374," get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2759,performance,Queue,Queue,2759,"inimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2835,performance,Queue,Queue,2835," Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2849,performance,Queue,Queue,2849," downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2930,performance,queue,queues,2930,"low community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2982,performance,Lock,Lock,2982,". For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3056,performance,Lock,Lock,3056,"07. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3069,performance,Lock,Lock,3069,"warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.js",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3140,performance,synch,synchronize,3140,"qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3271,performance,synch,synchronize,3271,"accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.j",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3690,performance,error,error,3690,"s.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json. gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1537,reliability,checkpoint,checkpoint,1537,"lease help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1769,reliability,mainten,maintenance,1769,"ference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3466,reliability,Doe,Does,3466," File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:390,safety,error,error,390,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:945,safety,input,input,945,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1067,safety,input,input,1067,"tps://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1118,safety,input,input,1118,"FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1306,safety,Error,Error,1306," fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1882,safety,depend,dependencies,1882,"ce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2226,safety,modul,module,2226,"t.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3487,safety,test,test,3487,"files_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3523,safety,test,test,3523,"eepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3690,safety,error,error,3690,"s.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json. gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1554,security,model,models,1554,". **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1846,security,modif,modify,1846,"om the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2982,security,Lock,Lock,2982,". For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3056,security,Lock,Lock,3056,"07. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3069,security,Lock,Lock,3069,"warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.js",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:761,testability,instrument,instrument,761,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1090,testability,unit,unittest,1090,"deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1312,testability,trace,trace,1312,"for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1806,testability,plan,planned,1806,"at is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1882,testability,depend,dependencies,1882,"ce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2078,testability,Trace,Traceback,2078,".chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:2809,testability,context,context,2809," end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3031,testability,context,context,3031,"orflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfreco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3487,testability,test,test,3487,"files_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3523,testability,test,test,3523,"eepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3745,testability,context,context,3745,"rocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json. gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples.tfrecord-00011-of-00016.gz. gvcf.tfrecord-00007-of-0001",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:165,usability,clear,clear,165,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:390,usability,error,error,390,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:548,usability,help,help,548,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:897,usability,Command,Command,897,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:945,usability,input,input,945,"Unable to run call_variants in udocker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more infor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1067,usability,input,input,1067,"tps://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1118,usability,input,input,1118,"FAQ.md**: yes. **Describe the issue:** . (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1306,usability,Error,Error,1306," fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1359,usability,command,command,1359,"l_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**. - Operating system: Red Hat Enterprise Linux 8.6. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File """,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1649,usability,User,UserWarning,1649,"tallation method (Docker, built from source, etc.): Docker (run via udocker). - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_googl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:1761,usability,minim,minimal,1761,"rument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**. - Command:. ```. udocker run \. -v ${INPUT_DIR}:""/input"" \. -v ${OUTPUT_DIR}:""/output"" \. DeepVariant \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \. --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=16. ```. - Error trace: (if applicable). ```. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3430,usability,user,user,3430," in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/733:3690,usability,error,error,3690,"s.py"", line 430, in call_variants. output_queue = multiprocessing.Queue(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue. return Queue(maxsize, ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__. self._rlock = ctx.Lock(). File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock. return Lock(ctx=self.get_context()). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__. SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx). File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__. sl = self._semlock = _multiprocessing.SemLock(. FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s. user 0m6.224s. sys 0m3.683s. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```. gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz. gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json. gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz. gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json. gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz. gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json. gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/733
https://github.com/google/deepvariant/issues/734:171,reliability,Doe,Does,171,"Mark Duplicates - parse sam aux fields?; To follow up on #384 , I have been running deepvariant with and without the --keep-duplicates flag, and getting identical vcfs. . Does keep duplicates need parse-sam-aux-fields to function? . Or, is the reverse true - does deepvariant exclude marked duplicate reads by default, but requires parse-sam-aux-fields to do so?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/734:259,reliability,doe,does,259,"Mark Duplicates - parse sam aux fields?; To follow up on #384 , I have been running deepvariant with and without the --keep-duplicates flag, and getting identical vcfs. . Does keep duplicates need parse-sam-aux-fields to function? . Or, is the reverse true - does deepvariant exclude marked duplicate reads by default, but requires parse-sam-aux-fields to do so?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/734:153,security,ident,identical,153,"Mark Duplicates - parse sam aux fields?; To follow up on #384 , I have been running deepvariant with and without the --keep-duplicates flag, and getting identical vcfs. . Does keep duplicates need parse-sam-aux-fields to function? . Or, is the reverse true - does deepvariant exclude marked duplicate reads by default, but requires parse-sam-aux-fields to do so?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/734
https://github.com/google/deepvariant/issues/735:46,availability,replic,replicate,46,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:224,availability,Operat,Operating,224,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:336,deployability,version,version,336,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:354,deployability,Instal,Installation,354,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1486,deployability,configurat,configuration,1486,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:249,energy efficiency,cloud,cloud,249,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:304,energy efficiency,CPU,CPUs,304,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1480,energy efficiency,cloud,cloud,1480,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:336,integrability,version,version,336,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1486,integrability,configur,configuration,1486,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:116,interoperability,specif,specified,116,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:286,interoperability,standard,standard-,286,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1661,interoperability,specif,specified,1661,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:336,modifiability,version,version,336,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1486,modifiability,configur,configuration,1486,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:304,performance,CPU,CPUs,304,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:606,safety,test,testdata,606,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:725,safety,input,input,725,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1014,safety,input,input,1014,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1148,safety,input,input,1148,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1172,safety,input,input,1172,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1531,safety,compl,complete,1531,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1486,security,configur,configuration,1486,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1531,security,compl,complete,1531,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:458,testability,instrument,instrument,458,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:606,testability,test,testdata,606,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:655,usability,Command,Command,655,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:725,usability,input,input,725,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1014,usability,input,input,1014,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1148,usability,input,input,1148,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1172,usability,input,input,1172,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/735:1514,usability,close,close,1514,"Runtime for a WES sample; I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook . n1-standard-64 - 64v CPUs - 240GB RAM. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): docker deepvariant . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:. export BIN_VERSION=""1.5.0"". export INPUT_DIR=""/home/jupyter/input"". export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"". export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam"". export OUTPUT_DIR=""/home/jupyter/output"". export OUTPUT_VCF=""HG003.deepvariant.vcf.gz"". export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md. . I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. . Is there a reason for the differences in runtime? . . Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/735
https://github.com/google/deepvariant/issues/736:198,availability,Operat,Operating,198,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:291,availability,down,down,291,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:324,availability,avail,available,324,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:495,availability,Error,Error,495,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1448,availability,avail,available,1448,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1582,availability,error,error,1582,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:6,deployability,instal,installation,6,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:19,deployability,fail,fails,19,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:65,deployability,instal,install,65,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:110,deployability,fail,fails,110,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:171,deployability,depend,dependencies,171,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:249,deployability,version,version,249,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:346,deployability,Instal,Installation,346,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:459,deployability,instal,install,459,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:648,deployability,instal,installable,648,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1047,deployability,instal,installed,1047,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1421,deployability,instal,install,1421,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1440,deployability,version,version,1440,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1505,deployability,releas,releases,1505,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1530,deployability,version,version,1530,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:160,energy efficiency,estimat,estimator,160,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1247,energy efficiency,estimat,estimator,1247,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1494,energy efficiency,estimat,estimator,1494,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1633,energy efficiency,estimat,estimator,1633,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:171,integrability,depend,dependencies,171,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:249,integrability,version,version,249,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1440,integrability,version,version,1440,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1530,integrability,version,version,1530,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:616,interoperability,incompatib,incompatible,616,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:171,modifiability,depend,dependencies,171,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:249,modifiability,version,version,249,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:522,modifiability,pac,packages,522,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:603,modifiability,pac,packages,603,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1440,modifiability,version,version,1440,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1530,modifiability,version,version,1530,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:495,performance,Error,Error,495,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1582,performance,error,error,1582,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:19,reliability,fail,fails,19,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:110,reliability,fail,fails,110,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:324,reliability,availab,available,324,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:776,reliability,doe,does,776,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:896,reliability,doe,does,896,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1134,reliability,doe,does,1134,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1271,reliability,doe,does,1271,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1322,reliability,Doe,Does,1322,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1448,reliability,availab,available,1448,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:171,safety,depend,dependencies,171,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:324,safety,avail,available,324,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:495,safety,Error,Error,495,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1343,safety,test,test,1343,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1448,safety,avail,available,1448,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1582,safety,error,error,1582,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:324,security,availab,available,324,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1448,security,availab,available,1448,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:171,testability,depend,dependencies,171,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:501,testability,trace,trace,501,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1343,testability,test,test,1343,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1394,testability,context,context,1394,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:443,usability,Command,Command,443,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:495,usability,Error,Error,495,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/736:1582,usability,error,error,1582,"Conda installation fails; **Describe the issue:**. Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**. - Operating system: Amazon Linux 2023. - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda. - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**. - Command: `mamba install deepvariant -c bioconda`. - Error trace: . ```. Pinned packages:. - python 3.10.*. Could not solve for environment specs. The following packages are incompatible. └─ deepvariant is installable with the potential options. ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require. │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.10.0|1.0.0] would require. │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);. ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require. │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;. ├─ deepvariant [0.7.1|0.7.2] would require. │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);. └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require. └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel). ```. **Does the quick start test work on your system?**. N/A. **Any additional context:**. My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8 .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/736
https://github.com/google/deepvariant/issues/737:385,availability,echo,echo,385,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1373,availability,Error,Error,1373,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:33,deployability,build,building,33,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:77,deployability,build,build-prereq,77,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:189,deployability,instal,installing,189,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:279,deployability,instal,installed,279,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:533,deployability,build,build,533,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:556,deployability,build,build,556,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1281,deployability,version,version,1281,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1316,deployability,modul,module,1316,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1404,deployability,Modul,Modules,1404,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1487,deployability,Stack,Stack,1487,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1542,deployability,Modul,Modules,1542,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1616,deployability,modul,modules,1616,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1281,integrability,version,version,1281,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1437,integrability,messag,message,1437,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1387,interoperability,share,share,1387,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1437,interoperability,messag,message,1437,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1525,interoperability,share,share,1525,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1281,modifiability,version,version,1281,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1316,modifiability,modul,module,1316,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1341,modifiability,pac,package,1341,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1404,modifiability,Modul,Modules,1404,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1459,modifiability,pac,package,1459,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1542,modifiability,Modul,Modules,1542,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1616,modifiability,modul,modules,1616,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1373,performance,Error,Error,1373,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:826,safety,Detect,Detecting,826,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:860,safety,Detect,Detecting,860,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:901,safety,Detect,Detecting,901,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:934,safety,Detect,Detecting,934,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1081,safety,Detect,Detecting,1081,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1117,safety,Detect,Detecting,1117,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1160,safety,Detect,Detecting,1160,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1195,safety,Detect,Detecting,1195,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1316,safety,modul,module,1316,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1373,safety,Error,Error,1373,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1404,safety,Modul,Modules,1404,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1542,safety,Modul,Modules,1542,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1616,safety,modul,modules,1616,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:644,security,ident,identification,644,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:693,security,ident,identification,693,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:826,security,Detect,Detecting,826,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:860,security,Detect,Detecting,860,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:901,security,Detect,Detecting,901,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:934,security,Detect,Detecting,934,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1081,security,Detect,Detecting,1081,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1117,security,Detect,Detecting,1117,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1160,security,Detect,Detecting,1160,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1195,security,Detect,Detecting,1195,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/737:1373,usability,Error,Error,1373,"There is something wrong while I building DeepVariant from source, I run the build-prereq.sh.; My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do? ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/737
https://github.com/google/deepvariant/issues/739:1147,availability,ERROR,ERROR,1147,"sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating obj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2662,availability,state,state,2662," broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2784,availability,state,state,2784,"ent instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3040,availability,operat,operation,3040,"ing into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXEC",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3671,availability,echo,echo,3671,"can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3846,availability,echo,echo,3846,"ch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4834,availability,Error,Error,4834,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:10,deployability,build,build,10,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:137,deployability,build,build-prereq,137,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:169,deployability,version,version,169,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:186,deployability,build,build,186,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:238,deployability,build,build-prereq,238,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:280,deployability,instal,install,280,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:953,deployability,Instal,Installing,953,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1043,deployability,instal,installation,1043,"e. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1160,deployability,depend,dependency,1160,"ich version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1243,deployability,instal,installed,1243,"eq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1300,deployability,depend,dependency,1300,"ized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1585,deployability,instal,installed,1585,"al/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1735,deployability,manag,manager,1735,"satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and y",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3227,deployability,INSTAL,INSTALL,3227,"sing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3253,deployability,INSTAL,INSTALL,3253,"1), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3330,deployability,build,build,3330,"d 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3709,deployability,build,build,3709,"nges and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3750,deployability,build,build,3750," any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3994,deployability,build,build,3994," switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4017,deployability,build,build,4017,"name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgCo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4742,deployability,version,version,4742,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4777,deployability,modul,module,4777,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4865,deployability,Modul,Modules,4865,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4948,deployability,Stack,Stack,4948,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5003,deployability,Modul,Modules,5003,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5077,deployability,modul,modules,5077,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1189,energy efficiency,current,currently,1189,"There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1735,energy efficiency,manag,manager,1735,"satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and y",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3493,energy efficiency,cpu,cpuinfo,3493,"f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3518,energy efficiency,cpu,cpuinfo,3518,"heckout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:169,integrability,version,version,169,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1160,integrability,depend,dependency,1160,"ich version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1300,integrability,depend,dependency,1300,"ized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2662,integrability,state,state,2662," broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2784,integrability,state,state,2784,"ent instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4742,integrability,version,version,4742,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4898,integrability,messag,message,4898,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1311,interoperability,conflict,conflicts,1311,"uf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1462,interoperability,incompatib,incompatible,1462,"rameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1558,interoperability,incompatib,incompatible,1558,"otobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b6086",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1689,interoperability,conflict,conflicting,1689,"3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1938,interoperability,prox,proxychains,1938,"69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or late",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1996,interoperability,Prox,ProxyChains-,1996,"ing uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git swit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2020,interoperability,prox,proxychains,2020,". Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4848,interoperability,share,share,4848,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4898,interoperability,messag,message,4898,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4986,interoperability,share,share,4986,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:169,modifiability,version,version,169,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:296,modifiability,paramet,parameterized,296,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:417,modifiability,pac,packages,417,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:466,modifiability,paramet,parameterized,466,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:513,modifiability,pac,packages,513,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:612,modifiability,pac,packages,612,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:792,modifiability,pac,packages,792,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:910,modifiability,pac,packages,910,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:974,modifiability,pac,packages,974,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1160,modifiability,depend,dependency,1160,"ich version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1225,modifiability,pac,packages,1225,"ng the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1300,modifiability,depend,dependency,1300,"ized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1727,modifiability,pac,package,1727,"already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit the",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2299,modifiability,reu,reused,2299,"pendency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2323,modifiability,pac,pack-reused,2323,"2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3110,modifiability,variab,variable,3110,"443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identific",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4742,modifiability,version,version,4742,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4777,modifiability,modul,module,4777,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4802,modifiability,pac,package,4802,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4865,modifiability,Modul,Modules,4865,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4920,modifiability,pac,package,4920,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5003,modifiability,Modul,Modules,5003,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5077,modifiability,modul,modules,5077,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:666,performance,cach,cached,666,"I want to build deepvariant:1.5.0 from source code. My environment is Ubuntu20.04.Did I need to prebuilt the protobuf before I run the ./build-prereq.sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. Prox",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1147,performance,ERROR,ERROR,1147,"sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating obj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3493,performance,cpu,cpuinfo,3493,"f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for wor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3518,performance,cpu,cpuinfo,3518,"heckout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4834,performance,Error,Error,4834,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1180,reliability,doe,does,1180,"d I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1147,safety,ERROR,ERROR,1147,"sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating obj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1160,safety,depend,dependency,1160,"ich version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1300,safety,depend,dependency,1300,"ized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1673,safety,permiss,permissions,1673,"ing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1735,safety,manag,manager,1735,"satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and y",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4287,safety,Detect,Detecting,4287,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4321,safety,Detect,Detecting,4321,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4362,safety,Detect,Detecting,4362,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4395,safety,Detect,Detecting,4395,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4542,safety,Detect,Detecting,4542,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4578,safety,Detect,Detecting,4578,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4621,safety,Detect,Detecting,4621,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4656,safety,Detect,Detecting,4656,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4777,safety,modul,module,4777,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4834,safety,Error,Error,4834,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4865,safety,Modul,Modules,4865,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5003,safety,Modul,Modules,5003,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:5077,safety,modul,modules,5077,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4105,security,ident,identification,4105,"riable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4154,security,ident,identification,4154,"at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ``",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4287,security,Detect,Detecting,4287,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4321,security,Detect,Detecting,4321,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4362,security,Detect,Detecting,4362,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4395,security,Detect,Detecting,4395,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4542,security,Detect,Detecting,4542,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4578,security,Detect,Detecting,4578,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4621,security,Detect,Detecting,4621,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4656,security,Detect,Detecting,4656,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1160,testability,depend,dependency,1160,"ich version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1300,testability,depend,dependency,1300,"ized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1147,usability,ERROR,ERROR,1147,"sh? If need, which version should I build.; There are some problems while running the ./build-prereq.sh:. ```. + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating obj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1259,usability,behavi,behaviour,1259,"ython3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0. Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0). Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0). Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), don",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1647,usability,user,user,1647,"ng==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:1701,usability,behavi,behaviour,1701,"whl (56 kB). Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0). Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2). Installing collected packages: pyparsing. Attempting uninstall: pyparsing. Found existing installation: pyparsing 3.1.1. Uninstalling pyparsing-3.1.1:. Successfully uninstalled pyparsing-3.1.1. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. + DV_PLATFORM=ubuntu-20.04. + ln -sf /usr/bin/python3.8 /usr/local/bin/python3. + cd. + rm -rf clif. + proxychains git clone https://github.com/google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimenta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:2975,usability,command,command,2975,"google/clif.git. ProxyChains-3.1 (http://proxychains.sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:3030,usability,undo,undo,3030,"sf.net). Cloning into 'clif'... |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK. remote: Enumerating objects: 5846, done. remote: Counting objects: 100% (700/700), done. remote: Compressing objects: 100% (111/111), done. remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146. Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done. Resolving deltas: 100% (4683/4683), done. + cd clif. + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]. + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7. Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental. changes and commit them, and you can discard any commits you make in this. state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may. do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/739:4834,usability,Error,Error,4834,"t 9ec44bd Replace C++ `#import <...>` with `#include <...>`. + ./INSTALL.sh. +++ dirname ./INSTALL.sh. ++ cd . ++ pwd. + CLIFSRC_DIR=/root/clif. + BUILD_DIR=/root/clif/build. + declare -a CMAKE_G_FLAG. + declare -a MAKE_PARALLELISM. + which ninja. + CMAKE_G_FLAGS=(). + MAKE_OR_NINJA=make. + MAKE_PARALLELISM=(-j 2). + [[ -r /proc/cpuinfo ]]. ++ cat /proc/cpuinfo. ++ grep -c '^processor'. + N_CPUS=32. + [[ 32 -gt 0 ]]. + MAKE_PARALLELISM=(-j $N_CPUS). + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}). + echo 'Using make for the clif backend build.'. Using make for the clif backend build. + [[ '' =~ ^-?-h ]]. + [[ -n '' ]]. ++ which python3. + PYTHON=/usr/local/bin/python3. + echo -n 'Using Python interpreter: /usr/local/bin/python3'. Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]. + mkdir -p /root/clif/build. + cd /root/clif/build. + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif. -- The C compiler identification is GNU 9.4.0. -- The CXX compiler identification is GNU 9.4.0. -- Check for working C compiler: /usr/bin/cc. -- Check for working C compiler: /usr/bin/cc -- works. -- Detecting C compiler ABI info. -- Detecting C compiler ABI info - done. -- Detecting C compile features. -- Detecting C compile features - done. -- Check for working CXX compiler: /usr/bin/c++. -- Check for working CXX compiler: /usr/bin/c++ -- works. -- Detecting CXX compiler ABI info. -- Detecting CXX compiler ABI info - done. -- Detecting CXX compile features. -- Detecting CXX compile features - done. -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""). -- Checking for module 'protobuf'. -- No package 'protobuf' found. CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):. A required package was not found. Call Stack (most recent call first):. /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal). clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules). clif/CMakeLists.txt:22 (include). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/739
https://github.com/google/deepvariant/issues/740:439,availability,avail,available,439,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:0,deployability,Build,Building,0,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:157,deployability,build,building,157,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:208,deployability,instal,install,208,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:295,deployability,build,build,295,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:632,deployability,version,version,632,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:744,deployability,version,version,744,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:790,deployability,build,build-prereq,790,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:632,integrability,version,version,632,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:744,integrability,version,version,744,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:76,modifiability,pac,package,76,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:170,modifiability,pac,package,170,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:220,modifiability,pac,package,220,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:305,modifiability,pac,package,305,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:632,modifiability,version,version,632,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:744,modifiability,version,version,744,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:439,reliability,availab,available,439,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:439,safety,avail,available,439,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:439,security,availab,available,439,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/740:709,security,modif,modify,709,"Building DeepVariant from sources on RedHat; Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh. build-prereq.sh. build_release_binaries.sh. run-prereq.sh. settings.sh.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/740
https://github.com/google/deepvariant/issues/741:374,availability,error,error,374,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:404,deployability,Fail,Failed,404,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:499,deployability,log,log,499,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:464,modifiability,interm,intermediate,464,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:673,modifiability,deco,decode,673,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1019,modifiability,deco,decode,1019,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:374,performance,error,error,374,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:178,reliability,doe,does,178,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:404,reliability,Fail,Failed,404,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:117,safety,input,input,117,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:374,safety,error,error,374,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:499,safety,log,log,499,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:648,safety,input,input,648,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:994,safety,input,input,994,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1442,safety,compl,completeness,1442,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:499,security,log,log,499,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1442,security,compl,completeness,1442,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:63,testability,regress,regression,63,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:499,testability,log,log,499,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:14,usability,support,supported,14,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:117,usability,input,input,117,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:374,usability,error,error,374,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:648,usability,input,input,648,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:994,usability,input,input,994,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1570,usability,support,support,1570,"CRAM v3.1 not supported in DV v1.6; Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like. ```. make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference. ```. make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'. genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/742:0,availability,Error,Errors,0,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:82,availability,error,errors,82,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1545,deployability,modul,module,1545,"put/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2628,deployability,modul,module,2628,"pp.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3709,deployability,modul,module,3709,"/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz'). ```. Any thoughts? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:425,energy efficiency,cpu,cpus,425,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:831,energy efficiency,cpu,cpus,831,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1239,energy efficiency,cpu,cpus,1239,"nts --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:30,modifiability,Pac,PacBio,30,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:125,modifiability,Pac,PacBio,125,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1545,modifiability,modul,module,1545,"put/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2628,modifiability,modul,module,2628,"pp.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3709,modifiability,modul,module,3709,"/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz'). ```. Any thoughts? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:0,performance,Error,Errors,0,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:82,performance,error,errors,82,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:198,performance,time,time,198,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:425,performance,cpu,cpus,425,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:602,performance,time,time,602,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:831,performance,cpu,cpus,831,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1010,performance,time,time,1010,"sting DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1239,performance,cpu,cpus,1239,"nts --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:0,safety,Error,Errors,0,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:10,safety,test,testing,10,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:82,safety,error,errors,82,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1545,safety,modul,module,1545,"put/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2364,safety,input,input,2364,"/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2628,safety,modul,module,2628,"pp.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3447,safety,input,input,3447,"t1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3709,safety,modul,module,3709,"/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz'). ```. Any thoughts? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:4528,safety,input,input,4528,"/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz'). ```. Any thoughts? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:10,testability,test,testing,10,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:1385,testability,Trace,Traceback,1385,"e ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2468,testability,Trace,Traceback,2468,"google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3549,testability,Trace,Traceback,3549,"m_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:0,usability,Error,Errors,0,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:82,usability,error,errors,82,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:183,usability,command,command,183,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:587,usability,command,command,587,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:995,usability,command,command,995,"Errors on testing DeepTrio on PacBio samples; Hi DeepVariant,. I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:2364,usability,input,input,2364,"/HG004.g.vcf.gz"". Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:3447,usability,input,input,3447,"t1.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:4528,usability,input,input,4528,"/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_35db5i5u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_child.tfrecord.gz'). Traceback (most recent call last):. File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>. app.run(main). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main. sample_name = get_sample_name(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name. _, record = get_cvo_paths_and_first_record(). File ""/var/tmp/Bazel.runfiles_qvjnuxsy/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record. raise ValueError(. ValueError: ('Found multiple file patterns in input filename space: ', './output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz'). ```. Any thoughts? Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/743:0,availability,Error,Error,0,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:225,availability,Operat,Operating,225,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:475,availability,Error,Error,475,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:258,deployability,version,version,258,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:270,deployability,Instal,Installation,270,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:258,integrability,version,version,258,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:258,modifiability,version,version,258,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:0,performance,Error,Error,0,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:475,performance,Error,Error,475,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:507,reliability,Doe,Does,507,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:0,safety,Error,Error,0,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:9,safety,test,testing,9,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:475,safety,Error,Error,475,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:528,safety,test,test,528,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:564,safety,test,test,564,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:9,testability,test,testing,9,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:354,testability,instrument,instrument,354,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:481,testability,trace,trace,481,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:528,testability,test,test,528,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:564,testability,test,test,564,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:739,testability,context,context,739,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:0,usability,Error,Error,0,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:158,usability,clear,clear,158,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:463,usability,Command,Command,463,"Error on testing deepvariant for WES; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
