id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/root-project/root/pull/2:565,availability,Error,Error,565,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:603,availability,failur,failure,603,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:655,availability,Error,Error,655,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:844,availability,Error,Errors,844,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:603,deployability,fail,failure,603,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:565,performance,Error,Error,565,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:603,performance,failur,failure,603,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:655,performance,Error,Error,655,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:844,performance,Error,Errors,844,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:603,reliability,fail,failure,603,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:639,reliability,doe,does,639,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:12,safety,test,test,12,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:384,safety,test,test,384,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:565,safety,Error,Error,565,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:655,safety,Error,Error,655,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:844,safety,Error,Errors,844,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:761,security,Hash,HashTable,761,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:12,testability,test,test,12,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:384,testability,test,test,384,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:478,testability,assert,assertTEnum,478,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:565,usability,Error,Error,565,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:655,usability,Error,Error,655,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/2:844,usability,Error,Errors,844,"Develop and test enum; So here are the changes that I needed to make from the comments of the last pull request and some other changes:. 1) I've added the name for a enum constant because I realised when declaring kMyConstant = 42 that I didn't save the name. 2) I've added some checks in the TCling to see if the TEnum and TEnumConstants are actually created, because when I run the test:. // MyEnumComment. enum EMyEnum {. kMyEnumConstant = 42 // enumConstantComment. };. int assertTEnum(). {. ```. if (!(TEnum*)gROOT->GetListOfEnums()->FindObject(""EMyEnum"")) {. Error (""TEnum"", ""Constructor of TEnum failure."");. return -1;. ```. }. It does return and Error. I tried that on the root[0] promp as well and the address of GetListOfEnum is not NULL(because the HashTable is created), but the address of FindObject is 0x0. . Now I don't get the Errors of TEnum and TEnumConstant are not created..I wanted to check whether they are added to the fEnums and fGlobals, but Add() for TCollection is a void function. My best guess is that they are not added to the lists, maybe you can see why...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/2
https://github.com/root-project/root/pull/4:205,availability,Recov,RecoveryPath,205,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:373,availability,error,error,373,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:420,availability,error,error,420,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:205,deployability,Recov,RecoveryPath,205,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:379,integrability,messag,message,379,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:304,interoperability,specif,specifying,304,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:379,interoperability,messag,message,379,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:373,performance,error,error,373,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:420,performance,error,error,420,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:205,reliability,Recov,RecoveryPath,205,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:205,safety,Recov,RecoveryPath,205,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:373,safety,error,error,373,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:420,safety,error,error,420,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:205,security,Recov,RecoveryPath,205,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:373,usability,error,error,373,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/4:420,usability,error,error,420,"- Add new TPPClingCallbacks class & two new methods TCling__CompileMacro...; ... and TCling__SplitAclicMode. The FileNotFound() method of TPPClingCallbacks is called via. Callbacks->FileNotFound(Filename, RecoveryPath) in. Preprocessor::HandleIncludeDirective(), allowing to compile code via. ACLiC when specifying #include ""myfile.C+"", and hence suppressing. preprocessor error message like:. input_line_23:1:10: fatal error: 'myfile.C+' file not found.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/4
https://github.com/root-project/root/pull/6:86,deployability,API,API,86,"64bit index; 64bit int ""extensions"" to TTreeIndex, TTreeFormula and TLeaf::GetValue() API. now against the correct branch 5.34!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/6
https://github.com/root-project/root/pull/6:86,integrability,API,API,86,"64bit index; 64bit int ""extensions"" to TTreeIndex, TTreeFormula and TLeaf::GetValue() API. now against the correct branch 5.34!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/6
https://github.com/root-project/root/pull/6:86,interoperability,API,API,86,"64bit index; 64bit int ""extensions"" to TTreeIndex, TTreeFormula and TLeaf::GetValue() API. now against the correct branch 5.34!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/6
https://github.com/root-project/root/pull/6:24,modifiability,extens,extensions,24,"64bit index; 64bit int ""extensions"" to TTreeIndex, TTreeFormula and TLeaf::GetValue() API. now against the correct branch 5.34!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/6
https://github.com/root-project/root/pull/8:7,safety,safe,safe,7,Thread-safe determination of TObject::IsOnHeap;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8
https://github.com/root-project/root/pull/8:38,security,IsO,IsOnHeap,38,Thread-safe determination of TObject::IsOnHeap;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8
https://github.com/root-project/root/pull/10:99,performance,lock,lock,99,"Don't reset global in TClass::New; Helgrind was complaining about a global being written without a lock. In this case, the global is not actually changed so we do a check. to see if the old value was the same as the value we set and if. so, we don't bother to change it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/10
https://github.com/root-project/root/pull/10:48,safety,compl,complaining,48,"Don't reset global in TClass::New; Helgrind was complaining about a global being written without a lock. In this case, the global is not actually changed so we do a check. to see if the old value was the same as the value we set and if. so, we don't bother to change it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/10
https://github.com/root-project/root/pull/10:48,security,compl,complaining,48,"Don't reset global in TClass::New; Helgrind was complaining about a global being written without a lock. In this case, the global is not actually changed so we do a check. to see if the old value was the same as the value we set and if. so, we don't bother to change it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/10
https://github.com/root-project/root/pull/10:99,security,lock,lock,99,"Don't reset global in TClass::New; Helgrind was complaining about a global being written without a lock. In this case, the global is not actually changed so we do a check. to see if the old value was the same as the value we set and if. so, we don't bother to change it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/10
https://github.com/root-project/root/pull/11:72,safety,avoid,avoid,72,"Made TClass::fgCallingNew a thread_local file scope static; In order to avoid thread-safety issues, the static class member. TClass::fgCallingNew is no longer a class member and is instead. a file scoped static declared thread_local. It was necessary to. not have it is a class member since CINT could not parse the new. thread_local keyword.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/11
https://github.com/root-project/root/pull/11:85,safety,safe,safety,85,"Made TClass::fgCallingNew a thread_local file scope static; In order to avoid thread-safety issues, the static class member. TClass::fgCallingNew is no longer a class member and is instead. a file scoped static declared thread_local. It was necessary to. not have it is a class member since CINT could not parse the new. thread_local keyword.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/11
https://github.com/root-project/root/pull/12:65,interoperability,specif,specifically,65,Made reading/writing ROOT files thread safe; Thread-safe changes specifically affecting reading/writing different ROOT files from different threads. These changes require the use of C++11.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/12
https://github.com/root-project/root/pull/12:39,safety,safe,safe,39,Made reading/writing ROOT files thread safe; Thread-safe changes specifically affecting reading/writing different ROOT files from different threads. These changes require the use of C++11.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/12
https://github.com/root-project/root/pull/12:52,safety,safe,safe,52,Made reading/writing ROOT files thread safe; Thread-safe changes specifically affecting reading/writing different ROOT files from different threads. These changes require the use of C++11.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/12
https://github.com/root-project/root/pull/13:190,deployability,Depend,Depends,190,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:190,integrability,Depend,Depends,190,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:111,modifiability,inherit,inherits,111,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:190,modifiability,Depend,Depends,190,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:174,safety,safe,safety,174,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:190,safety,Depend,Depends,190,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/13:190,testability,Depend,Depends,190,Fix genreflex problem with fgIsA; Fixes a problem with using genreflex to create a dictionary for a class that inherits from TObject. The problem was triggered by the thread-safety changes. Depends on previous pull request (#12). .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/13
https://github.com/root-project/root/pull/14:54,deployability,contain,contains,54,"Unify cint root mutex; Philippe, here's the pull that contains the change were I make gCintMutex and gROOTMutex point to the same mutex object. Make sure to only take the last commit in this change since most of the other changes are CMS specific.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/14
https://github.com/root-project/root/pull/14:238,interoperability,specif,specific,238,"Unify cint root mutex; Philippe, here's the pull that contains the change were I make gCintMutex and gROOTMutex point to the same mutex object. Make sure to only take the last commit in this change since most of the other changes are CMS specific.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/14
https://github.com/root-project/root/pull/15:87,deployability,fail,failed,87,TDavixFile bugfixes corrections for ROOT 6; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/15
https://github.com/root-project/root/pull/15:112,integrability,messag,message,112,TDavixFile bugfixes corrections for ROOT 6; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/15
https://github.com/root-project/root/pull/15:112,interoperability,messag,message,112,TDavixFile bugfixes corrections for ROOT 6; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/15
https://github.com/root-project/root/pull/15:87,reliability,fail,failed,87,TDavixFile bugfixes corrections for ROOT 6; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/15
https://github.com/root-project/root/pull/16:87,deployability,fail,failed,87,TDavixFile bugfixes corrections for ROOT 5; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/16
https://github.com/root-project/root/pull/16:112,integrability,messag,message,112,TDavixFile bugfixes corrections for ROOT 5; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/16
https://github.com/root-project/root/pull/16:112,interoperability,messag,message,112,TDavixFile bugfixes corrections for ROOT 5; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/16
https://github.com/root-project/root/pull/16:87,reliability,fail,failed,87,TDavixFile bugfixes corrections for ROOT 5; - fix ROOTTest executions problem : remove failed word from warning message. - gridMode considers now CAPath correctly. - correct problem related to X509_CERT_DIR and gridmode.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/16
https://github.com/root-project/root/pull/17:13,deployability,contain,contains,13,PCM filename contains libName + dictName;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/17
https://github.com/root-project/root/pull/18:4,usability,Custom,Customizations,4,CMS Customizations on top of ROOT6 HEAD; This pull request is to make ROOT people aware about actual customizations CMS has on top of ROOT6 master branch. Not intended for merging. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/18
https://github.com/root-project/root/pull/18:101,usability,custom,customizations,101,CMS Customizations on top of ROOT6 HEAD; This pull request is to make ROOT people aware about actual customizations CMS has on top of ROOT6 master branch. Not intended for merging. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/18
https://github.com/root-project/root/pull/19:85,availability,error,errors,85,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:250,availability,operat,operator,250,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:4,deployability,updat,update,4,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:452,deployability,contain,containers,452,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:228,energy efficiency,optim,optimization,228,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:331,energy efficiency,Alloc,Allocator,331,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:85,performance,error,errors,85,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:228,performance,optimiz,optimization,228,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:285,performance,Memor,Memory,285,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:4,safety,updat,update,4,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:85,safety,error,errors,85,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:4,security,updat,update,4,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:482,security,Sign,Signed-off-by,482,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:85,usability,error,errors,85,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:139,usability,support,support,139,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:285,usability,Memor,Memory,285,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/19:355,usability,minim,minimum,355,Vc: update to Vc 0.7.4; Summary of the changes in Vc 0.7.4:. - fixed several compile errors / warnings with newer or old C++. compilers. - support clean compilation with more -W flags. - fixed compilation when compiling without optimization. - added operator-- to Vector<T>. - Copying Memory now uses SIMD move instructions. - Vc::Allocator<T> now uses a minimum alignment of the SIMD types of. the chosen Vc implementation. Thus making it useable for containers of. builtin types. Signed-off-by: Matthias Kretz kretz@kde.org.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/19
https://github.com/root-project/root/pull/20:70,modifiability,variab,variables,70,Made smatrix code thread safe by removing statics; Changed all static variables which were only being used as local variables to be local variables. This avoids threading problems.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/20
https://github.com/root-project/root/pull/20:116,modifiability,variab,variables,116,Made smatrix code thread safe by removing statics; Changed all static variables which were only being used as local variables to be local variables. This avoids threading problems.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/20
https://github.com/root-project/root/pull/20:138,modifiability,variab,variables,138,Made smatrix code thread safe by removing statics; Changed all static variables which were only being used as local variables to be local variables. This avoids threading problems.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/20
https://github.com/root-project/root/pull/20:25,safety,safe,safe,25,Made smatrix code thread safe by removing statics; Changed all static variables which were only being used as local variables to be local variables. This avoids threading problems.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/20
https://github.com/root-project/root/pull/20:154,safety,avoid,avoids,154,Made smatrix code thread safe by removing statics; Changed all static variables which were only being used as local variables to be local variables. This avoids threading problems.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/20
https://github.com/root-project/root/pull/23:19,deployability,patch,patches,19,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/23:58,deployability,patch,patches,58,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/23:19,safety,patch,patches,19,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/23:58,safety,patch,patches,58,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/23:19,security,patch,patches,19,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/23:58,security,patch,patches,58,RZip.h for 5-34-00-patches; These are changes for 5-34-00-patches branch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/23
https://github.com/root-project/root/pull/24:38,performance,concurren,concurrent,38,Protect global list of functions from concurrent access; The list returned from TROOT::GetListOfFunctions needs to be protected. from concurrent access to allow use of TFormulas on different. threads.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/24
https://github.com/root-project/root/pull/24:134,performance,concurren,concurrent,134,Protect global list of functions from concurrent access; The list returned from TROOT::GetListOfFunctions needs to be protected. from concurrent access to allow use of TFormulas on different. threads.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/24
https://github.com/root-project/root/pull/24:49,security,access,access,49,Protect global list of functions from concurrent access; The list returned from TROOT::GetListOfFunctions needs to be protected. from concurrent access to allow use of TFormulas on different. threads.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/24
https://github.com/root-project/root/pull/24:145,security,access,access,145,Protect global list of functions from concurrent access; The list returned from TROOT::GetListOfFunctions needs to be protected. from concurrent access to allow use of TFormulas on different. threads.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/24
https://github.com/root-project/root/pull/25:303,performance,multi-thread,multi-threaded,303,"Make TMVA thread-safe with respect to use of Reader; It is now possible to create independent TMVA::Readers and use. them simultaneously on different threads. Training of MVAs is still only safe single-threaded. In addition,. it is not safe to use multiple instances of MethodCFMlpANN either. single or multi-threaded because of a global 'this' pointer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/25
https://github.com/root-project/root/pull/25:17,safety,safe,safe,17,"Make TMVA thread-safe with respect to use of Reader; It is now possible to create independent TMVA::Readers and use. them simultaneously on different threads. Training of MVAs is still only safe single-threaded. In addition,. it is not safe to use multiple instances of MethodCFMlpANN either. single or multi-threaded because of a global 'this' pointer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/25
https://github.com/root-project/root/pull/25:190,safety,safe,safe,190,"Make TMVA thread-safe with respect to use of Reader; It is now possible to create independent TMVA::Readers and use. them simultaneously on different threads. Training of MVAs is still only safe single-threaded. In addition,. it is not safe to use multiple instances of MethodCFMlpANN either. single or multi-threaded because of a global 'this' pointer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/25
https://github.com/root-project/root/pull/25:236,safety,safe,safe,236,"Make TMVA thread-safe with respect to use of Reader; It is now possible to create independent TMVA::Readers and use. them simultaneously on different threads. Training of MVAs is still only safe single-threaded. In addition,. it is not safe to use multiple instances of MethodCFMlpANN either. single or multi-threaded because of a global 'this' pointer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/25
https://github.com/root-project/root/pull/25:122,testability,simul,simultaneously,122,"Make TMVA thread-safe with respect to use of Reader; It is now possible to create independent TMVA::Readers and use. them simultaneously on different threads. Training of MVAs is still only safe single-threaded. In addition,. it is not safe to use multiple instances of MethodCFMlpANN either. single or multi-threaded because of a global 'this' pointer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/25
https://github.com/root-project/root/pull/26:69,deployability,resourc,resource,69,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:69,energy efficiency,resourc,resource,69,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:12,performance,lock,lock,12,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:69,performance,resourc,resource,69,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:69,safety,resourc,resource,69,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:12,security,lock,lock,12,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/26:69,testability,resourc,resource,69,Recursively lock Reflex::Class::PathToBase; Need to protect a global resource in Reflex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/26
https://github.com/root-project/root/pull/27:126,performance,I/O,I/O,126,Another thread safety fix for TFormula; Additional changes needed to safely use different TFormula on different threads while I/O is occurring on another thread.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/27
https://github.com/root-project/root/pull/27:15,safety,safe,safety,15,Another thread safety fix for TFormula; Additional changes needed to safely use different TFormula on different threads while I/O is occurring on another thread.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/27
https://github.com/root-project/root/pull/27:69,safety,safe,safely,69,Another thread safety fix for TFormula; Additional changes needed to safely use different TFormula on different threads while I/O is occurring on another thread.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/27
https://github.com/root-project/root/pull/28:45,performance,Cach,Caching,45,Make TClass::fIsAMethod setting thread safe; Caching and initialization of TClass::fIsAMethod have been changed. to make them thread safe and for calls to the TMethodCall can happen. concurrently.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/28
https://github.com/root-project/root/pull/28:183,performance,concurren,concurrently,183,Make TClass::fIsAMethod setting thread safe; Caching and initialization of TClass::fIsAMethod have been changed. to make them thread safe and for calls to the TMethodCall can happen. concurrently.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/28
https://github.com/root-project/root/pull/28:39,safety,safe,safe,39,Make TClass::fIsAMethod setting thread safe; Caching and initialization of TClass::fIsAMethod have been changed. to make them thread safe and for calls to the TMethodCall can happen. concurrently.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/28
https://github.com/root-project/root/pull/28:133,safety,safe,safe,133,Make TClass::fIsAMethod setting thread safe; Caching and initialization of TClass::fIsAMethod have been changed. to make them thread safe and for calls to the TMethodCall can happen. concurrently.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/28
https://github.com/root-project/root/pull/29:504,availability,failur,failure,504,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:341,deployability,fail,fail,341,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:504,deployability,fail,failure,504,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:572,deployability,build,buildlogs,572,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:504,performance,failur,failure,504,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:328,reliability,diagno,diagnose,328,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:341,reliability,fail,fail,341,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:504,reliability,fail,failure,504,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:218,security,token,token,218,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:666,security,Sign,Signed-off-by,666,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:328,testability,diagno,diagnose,328,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/29:424,usability,ui,uiuc,424,Fix Reflex dictionary compilation with LLVM/Clang 3.5; See the following 5 years old commit in Clang:. ```. https://github.com/llvm-mirror/clang/commit/. a78c5c34fbd20fde02261c3f3e21933cd58fcc04. ```. Clang forces '(' token after reference to dtor. Reflex uses an extra. parentheses around dtor reference. It force Clang 3.5 to diagnose and fail. A thread in cfe-dev (Clang) mainling-list was started:. ```. http://lists.cs.uiuc.edu/pipermail/cfe-dev/. 2014-October/039371.html. ```. The following fixes failure in CMSSW with Clang 3.5: https://cmssdt.cern.ch/SDT/cgi-bin/buildlogs/slc6_amd64_gcc481/CMSSW_7_3_CLANG_X_2014-10-01-0200/AnalysisDataFormats/TopObjects. Signed-off-by: David Abdurachmanov davidlt@cern.ch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/29
https://github.com/root-project/root/pull/30:102,availability,sli,slightly,102,Remove parentheses from destructor call to fix problem with clang 3.5; The clang 3.5 compiler needs a slightly different syntax when calling. the destructor. It does not allow a set of parentheses between the. name of the destructor and the parentheses which cause the call.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/30
https://github.com/root-project/root/pull/30:102,reliability,sli,slightly,102,Remove parentheses from destructor call to fix problem with clang 3.5; The clang 3.5 compiler needs a slightly different syntax when calling. the destructor. It does not allow a set of parentheses between the. name of the destructor and the parentheses which cause the call.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/30
https://github.com/root-project/root/pull/30:161,reliability,doe,does,161,Remove parentheses from destructor call to fix problem with clang 3.5; The clang 3.5 compiler needs a slightly different syntax when calling. the destructor. It does not allow a set of parentheses between the. name of the destructor and the parentheses which cause the call.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/30
https://github.com/root-project/root/pull/31:0,performance,Lock,Lock,0,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/31:105,safety,safe,safely,105,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/31:0,security,Lock,Lock,0,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/31:5,security,access,access,5,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/31:58,security,access,accesses,58,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/31:158,security,access,access,158,"Lock access to global functions list; TFormula indirectly accesses gROOT->fGlobalFunctions. Therefore to safely call different TFormula on different threads, access to the global function list must be serialized.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/31
https://github.com/root-project/root/pull/32:371,availability,error,error,371,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:479,availability,down,down,479,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:113,deployability,version,version,113,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:414,deployability,fail,fails,414,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:113,integrability,version,version,113,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:113,modifiability,version,version,113,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:304,performance,lock,locking,304,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:371,performance,error,error,371,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:414,reliability,fail,fails,414,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:18,safety,safe,safety,18,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:371,safety,error,error,371,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:304,security,lock,locking,304,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:172,usability,interact,interaction,172,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/32:371,usability,error,error,371,"Additional thread safety fixes; These changes fix a periodic crash which was occuring while running the threaded version of CMS' reconstruction code. The problem was a bad interaction between rebinning of a TProfile while being filled and the cloning of a TFormula. The problem stemmed from insufficient locking of cint data structures. As part of the change, additional error reporting for the case where cloning fails was added. These reports were extremely useful in tracking down the problem and may prove useful in the future if a similar problem surfaces again.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/32
https://github.com/root-project/root/pull/33:136,availability,state,state,136,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:115,deployability,updat,updates,115,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:263,deployability,Build,Build,263,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:136,integrability,state,state,136,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:15,performance,lock,lock,15,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:176,performance,lock,lock,176,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:322,performance,lock,lock,322,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:115,safety,updat,updates,115,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:15,security,lock,lock,15,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:115,security,updat,updates,115,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:176,security,lock,lock,176,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/33:322,security,lock,lock,322,Must take cint lock before calling TCint::CallFunc_Factory; Helgrind found that TCint::CallFunc_Factory indirectly updates. global cint state. Therefore one must take the cint lock before. calling the function. This was the only place (other than. TSelectorCint::Build) which calls this function without first. taking the lock.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/33
https://github.com/root-project/root/pull/34:143,integrability,buffer,buffer,143,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:24,performance,cach,cache,24,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:65,performance,cach,cache,65,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:114,performance,lock,locking,114,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:37,safety,safe,safe,37,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:129,safety,safe,safe,129,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/34:114,security,lock,locking,114,"Make G__FastAllocString cache thread safe; When using C++11, the cache used by G__FastAllocString utilizes. a non-locking thread safe circular buffer.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/34
https://github.com/root-project/root/pull/35:0,performance,Synch,Synchronize,0,Synchronize construction of TApplication; It was possible for multiple threads to attempt construction of a. TApplication or for one thread to think the TApplication was already. constructed while another thread was in the process of doing the. construction.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/35
https://github.com/root-project/root/pull/36:143,availability,operat,operations,143,Implement Clone for histograms; This change fixes many threading issues (both data races and syncrhonization points) with respect to histogram operations which make use of Clone. One such operation is the 'Fill' where the axis are allowed to rebin. This change was made under consultation with Philippe Canal.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/36
https://github.com/root-project/root/pull/36:188,availability,operat,operation,188,Implement Clone for histograms; This change fixes many threading issues (both data races and syncrhonization points) with respect to histogram operations which make use of Clone. One such operation is the 'Fill' where the axis are allowed to rebin. This change was made under consultation with Philippe Canal.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/36
https://github.com/root-project/root/pull/37:235,availability,error,error,235,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:387,availability,Error,Error,387,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:235,performance,error,error,235,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:387,performance,Error,Error,387,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:428,performance,I/O,I/O,428,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:235,safety,error,error,235,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:387,safety,Error,Error,387,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:411,security,modif,modifications,411,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:656,security,Modif,Modification,656,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:706,security,access,access,706,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:737,security,Sign,Significant,737,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:235,usability,error,error,235,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:387,usability,Error,Error,387,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/37:583,usability,document,documentation,583,"Mofidications in TBufferXML/SQL/JSON and TXMLFile classes; 1. Equip TCanvas::Streamer to provide data members information for TBufferJSON/TBufferXML/TBufferSQL2 classes. 2. Fix problem with streamer infos reading from TXMLFile. 3. Fix error in TBufferXML/TBufferJSON/TBufferSQL2 when equipted streamers are used. Reported also here: http://root.cern.ch/phpBB3/viewtopic.php?f=3&t=18802. Error was introduced by modifications in I/O between 5-34/19 and 5-34/20. 4. Implement TBufferJSON::CheckObject(), enable correct storage of colors palete in JSON with TCanvas. 5. Provide missing documentation for some methods in TBufferXML/TBufferSQL2/TBufferJSON. 6. Modification in THttpServer class - one could now access any objects memeber. 7. Significant redesign of JSROOT - now everything can be redrawn and resize.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/37
https://github.com/root-project/root/pull/38:8,performance,concurren,concurrent,8,Protect concurrent access to gROOT->GetListOfFiles(); Use the gRootMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/38
https://github.com/root-project/root/pull/38:103,performance,concurren,concurrent,103,Protect concurrent access to gROOT->GetListOfFiles(); Use the gRootMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/38
https://github.com/root-project/root/pull/38:19,security,access,access,19,Protect concurrent access to gROOT->GetListOfFiles(); Use the gRootMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/38
https://github.com/root-project/root/pull/38:84,security,access,access,84,Protect concurrent access to gROOT->GetListOfFiles(); Use the gRootMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/38
https://github.com/root-project/root/pull/38:114,security,access,accesses,114,Protect concurrent access to gROOT->GetListOfFiles(); Use the gRootMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/38
https://github.com/root-project/root/pull/39:0,deployability,Patch,Patches,0,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/39:289,deployability,patch,patch,289,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/39:0,safety,Patch,Patches,0,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/39:289,safety,patch,patch,289,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/39:0,security,Patch,Patches,0,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/39:289,security,patch,patch,289,"Patches; Hi,. I added 2 convenient methods for TGraph:. - AppendPoint adds a new point at the ""end"" of the graph. - RemoveAllPoints calls RemovePoint(0) until there are no points left. This probably could also be done (faster) by deleting the fX and fY arrays, so feel free to change this patch. Benni.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/39
https://github.com/root-project/root/pull/40:130,availability,state,state-of-the-art,130,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1454,availability,consist,consistently,1454,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1758,availability,avail,available,1758,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1916,availability,avail,available,1916,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2081,availability,avail,available,2081,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2133,availability,avail,available,2133,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1260,deployability,build,building,1260,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,deployability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:325,energy efficiency,optim,optimization,325,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:130,integrability,state,state-of-the-art,130,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,integrability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:294,interoperability,bind,bindings,294,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,interoperability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:294,modifiability,bind,bindings,294,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,modifiability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2192,modifiability,maintain,maintaining,2192,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:263,performance,perform,performance,263,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:325,performance,optimiz,optimization,325,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:933,performance,perform,performances,933,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1377,performance,Perform,Performances,1377,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2043,performance,time,time,2043,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1758,reliability,availab,available,1758,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1916,reliability,availab,available,1916,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2081,reliability,availab,available,2081,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2133,reliability,availab,available,2133,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,reliability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1171,safety,test,tests,1171,"zer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibl",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1758,safety,avail,available,1758,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1916,safety,avail,available,1916,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2081,safety,avail,available,2081,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2133,safety,avail,available,2133,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2155,safety,test,testing,2155,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2192,safety,maintain,maintaining,2192,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:622,security,team,teams,622,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:965,security,access,access,965,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1758,security,availab,available,1758,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1836,security,auth,authuser,1836,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1916,security,availab,available,1916,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1994,security,auth,authuser,1994,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2081,security,availab,available,2081,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2133,security,availab,available,2133,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,security,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1171,testability,test,tests,1171,"zer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibl",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2155,testability,test,testing,2155,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2176,testability,integr,integrating,2176,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:0,usability,Support,Support,0,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:19,usability,minim,minimizer,19,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:62,usability,support,support,62,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:168,usability,minim,minimizer,168,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:263,usability,perform,performance,263,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:347,usability,minim,minimizer,347,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:875,usability,support,support,875,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:933,usability,perform,performances,933,"Support for CMA-ES minimizer based on libcmaes; This is ROOT6 support for [CMA-ES](https://www.lri.fr/~hansen/cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authus",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1106,usability,support,support,1106,"cmaesintro.html), a state-of-the-art black box stochastic minimizer. The implementation uses [libcmaes](https://github.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auge",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1217,usability,Document,Documentation,1217,"thub.com/beniz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1242,usability,document,documentation,1242,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1287,usability,minim,minimizer,1287,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1377,usability,Perform,Performances,1377,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:1454,usability,consist,consistently,1454,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/40:2147,usability,help,help,2147,"iz/libcmaes), a novel high performance C++11 (with Python bindings) library for blackbox optimization. The new minimizer yields better results than Minuit2 for most problems, though for an higher computation cost on average. See below for links to relevant benchmarks backing up these claims. This implementation is on behalf of [Inria Saclay Research group TAO](http://www.inria.fr/en/teams/tao), [Laboratoire de l'Accelerateur Lineaire, group AppStat](http://appstat.lal.in2p3.fr/) and [University Paris-Sud LRI](https://www.lri.fr/index_en.php?lang=EN). ===Features===. - Seamless replacement for Minuit, Minuit2 and Fumili. - Seamless support for RooFit. - Relying on libcmaes allows for best performances known for CMA-ES + access to several flavors of the original algorithm, yielding best results with a trade off for computational cost, as needed. - Compilation support for both Autotools and CMake. - Included tutorial files, tests, and a special benchmark vs Minuit2. ===Documentation===. - Main documentation for building and using the new minimizer is here: https://github.com/beniz/libcmaes/wiki/using-CMA-ES-in-CERN's-ROOT. ===Performances===. - On-par with Minuit2 on low dimensional problems (< 10-D), consistently beats Minuit2 in higher dimension, leading to better fits. These claims are backed by two benchmarks and two experiments on real world data (we are still waiting from some results from usage at CERN). - Benchmark CMA-ES vs Minuit on [BBOB](http://coco.gforge.inria.fr/doku.php?id=bbob-2013) available here: https://drive.google.com/open?id=0B3J1vWYhta9ibktXc2JLRUExUTA&authuser=0. - In-ROOT benchmark vs Minuit2 on low-dimensional problems, results available here: https://drive.google.com/open?id=0B3J1vWYhta9iTmR0T0hnN21lSGM&authuser=0. - Beats out Minuit2 up to 98% of the time on some experiments (data + code available on demand from Auger group, ask me). I am available for help in testing and possibly integrating and maintaining this addition, just let me know.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/40
https://github.com/root-project/root/pull/41:27,energy efficiency,green,greenwich,27,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:155,energy efficiency,Green,Greenwich,155,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:488,integrability,repositor,repository,488,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:488,interoperability,repositor,repository,488,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:56,performance,time,time,56,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:194,performance,time,time,194,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:467,performance,time,times,467,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/41:414,usability,help,helpful,414,"Added methods to calculate greenwich and local sidereal time.; This commit improves the functionality of TTimeStamp class by adding 4 methods to calculate Greenwich/local mean/apparent sidereal time. Calculation was referenced from. Aoki et. al. Astron. Astrophys. 105, 359-362 (1982), http://adsabs.harvard.edu/abs/1982A%26A...105..359A. and. http://aa.usno.navy.mil/faq/docs/GAST.php. In the future, it would be helpful to add a class that pulls the UTC-UT1 offset times from the below repository for enhanced accuracy:. ftp://ftp.iers.org/products/eop/bulletinb/format_2009/.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/41
https://github.com/root-project/root/pull/42:187,interoperability,XML,XML,187,move TString and std::string streamers into TBuffer class; These changes let to reimplement string streamers in derived classes to provide better simpler representation of string in JSON/XML/SQL .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/42
https://github.com/root-project/root/pull/42:146,testability,simpl,simpler,146,move TString and std::string streamers into TBuffer class; These changes let to reimplement string streamers in derived classes to provide better simpler representation of string in JSON/XML/SQL .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/42
https://github.com/root-project/root/pull/42:146,usability,simpl,simpler,146,move TString and std::string streamers into TBuffer class; These changes let to reimplement string streamers in derived classes to provide better simpler representation of string in JSON/XML/SQL .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/42
https://github.com/root-project/root/pull/44:17,deployability,patch,patches,17,TBuffer-releated patches for 5- 34-00 branch;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/44
https://github.com/root-project/root/pull/44:17,safety,patch,patches,17,TBuffer-releated patches for 5- 34-00 branch;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/44
https://github.com/root-project/root/pull/44:17,security,patch,patches,17,TBuffer-releated patches for 5- 34-00 branch;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/44
https://github.com/root-project/root/pull/45:8,performance,concurren,concurrent,8,Protect concurrent access to gROOT->GetListOfFiles(); Use the gROOTMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/45
https://github.com/root-project/root/pull/45:103,performance,concurren,concurrent,103,Protect concurrent access to gROOT->GetListOfFiles(); Use the gROOTMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/45
https://github.com/root-project/root/pull/45:19,security,access,access,19,Protect concurrent access to gROOT->GetListOfFiles(); Use the gROOTMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/45
https://github.com/root-project/root/pull/45:84,security,access,access,84,Protect concurrent access to gROOT->GetListOfFiles(); Use the gROOTMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/45
https://github.com/root-project/root/pull/45:114,security,access,accesses,114,Protect concurrent access to gROOT->GetListOfFiles(); Use the gROOTMutex to protect access to possible concurrent accesses. to gROOT->GetListOfFiles().,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/45
https://github.com/root-project/root/pull/46:7,safety,safe,safe,7,Thread safe TListOfFunctions; Thread safety issues with TListOfFunctions was found using the CMS threaded framework. These changes were done in consultation with Philippe Canal.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/46
https://github.com/root-project/root/pull/46:37,safety,safe,safety,37,Thread safe TListOfFunctions; Thread safety issues with TListOfFunctions was found using the CMS threaded framework. These changes were done in consultation with Philippe Canal.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/46
https://github.com/root-project/root/pull/47:4,performance,lock,locks,4,Add locks to calls involving TCling interpreter classes; Need to take the interpreter lock for all cases where the cling. internals might be reached. These fix threading problems with TFormula.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/47
https://github.com/root-project/root/pull/47:86,performance,lock,lock,86,Add locks to calls involving TCling interpreter classes; Need to take the interpreter lock for all cases where the cling. internals might be reached. These fix threading problems with TFormula.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/47
https://github.com/root-project/root/pull/47:4,security,lock,locks,4,Add locks to calls involving TCling interpreter classes; Need to take the interpreter lock for all cases where the cling. internals might be reached. These fix threading problems with TFormula.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/47
https://github.com/root-project/root/pull/47:86,security,lock,lock,86,Add locks to calls involving TCling interpreter classes; Need to take the interpreter lock for all cases where the cling. internals might be reached. These fix threading problems with TFormula.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/47
https://github.com/root-project/root/pull/48:168,interoperability,standard,standard,168,"Assign directly to static variable to avoid race condition; Initialization of a function static variable is guaranteed to be. done in a thread safe manner by the C++11 standard. Previously, the. static was initialized to 0 and then reset which lead to a data race.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/48
https://github.com/root-project/root/pull/48:26,modifiability,variab,variable,26,"Assign directly to static variable to avoid race condition; Initialization of a function static variable is guaranteed to be. done in a thread safe manner by the C++11 standard. Previously, the. static was initialized to 0 and then reset which lead to a data race.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/48
https://github.com/root-project/root/pull/48:96,modifiability,variab,variable,96,"Assign directly to static variable to avoid race condition; Initialization of a function static variable is guaranteed to be. done in a thread safe manner by the C++11 standard. Previously, the. static was initialized to 0 and then reset which lead to a data race.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/48
https://github.com/root-project/root/pull/48:38,safety,avoid,avoid,38,"Assign directly to static variable to avoid race condition; Initialization of a function static variable is guaranteed to be. done in a thread safe manner by the C++11 standard. Previously, the. static was initialized to 0 and then reset which lead to a data race.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/48
https://github.com/root-project/root/pull/48:143,safety,safe,safe,143,"Assign directly to static variable to avoid race condition; Initialization of a function static variable is guaranteed to be. done in a thread safe manner by the C++11 standard. Previously, the. static was initialized to 0 and then reset which lead to a data race.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/48
https://github.com/root-project/root/pull/49:71,performance,cach,caching,71,"Made TIsAProxy thread safe; The TIsAProxy does late initialization and caching, both of which. need to be done in a thread-safe manner.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/49
https://github.com/root-project/root/pull/49:42,reliability,doe,does,42,"Made TIsAProxy thread safe; The TIsAProxy does late initialization and caching, both of which. need to be done in a thread-safe manner.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/49
https://github.com/root-project/root/pull/49:22,safety,safe,safe,22,"Made TIsAProxy thread safe; The TIsAProxy does late initialization and caching, both of which. need to be done in a thread-safe manner.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/49
https://github.com/root-project/root/pull/49:123,safety,safe,safe,123,"Made TIsAProxy thread safe; The TIsAProxy does late initialization and caching, both of which. need to be done in a thread-safe manner.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/49
https://github.com/root-project/root/pull/50:12,interoperability,specif,specific,12,"Reuse range-specific AbsorbObjects() code; Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/50
https://github.com/root-project/root/pull/50:0,modifiability,Reu,Reuse,0,"Reuse range-specific AbsorbObjects() code; Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/50
https://github.com/root-project/root/pull/51:182,performance,time,time,182,Make TClass::fStreamerImpl thread safe; The exact streamer implementation to use for a TClass is determined. late and therefore two threads could be doing the same work at the. same time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/51
https://github.com/root-project/root/pull/51:34,safety,safe,safe,34,Make TClass::fStreamerImpl thread safe; The exact streamer implementation to use for a TClass is determined. late and therefore two threads could be doing the same work at the. same time.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/51
https://github.com/root-project/root/pull/52:63,deployability,manag,managed,63,"Reuse range-specific AbsorbObjects() code; [Second try since I managed to retroactively mess up the first pull request. Sorry.]. Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/52
https://github.com/root-project/root/pull/52:63,energy efficiency,manag,managed,63,"Reuse range-specific AbsorbObjects() code; [Second try since I managed to retroactively mess up the first pull request. Sorry.]. Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/52
https://github.com/root-project/root/pull/52:12,interoperability,specif,specific,12,"Reuse range-specific AbsorbObjects() code; [Second try since I managed to retroactively mess up the first pull request. Sorry.]. Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/52
https://github.com/root-project/root/pull/52:0,modifiability,Reu,Reuse,0,"Reuse range-specific AbsorbObjects() code; [Second try since I managed to retroactively mess up the first pull request. Sorry.]. Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/52
https://github.com/root-project/root/pull/52:63,safety,manag,managed,63,"Reuse range-specific AbsorbObjects() code; [Second try since I managed to retroactively mess up the first pull request. Sorry.]. Fixes https://sft.its.cern.ch/jira/browse/ROOT-6995. Also adds a check to ensure the upper index is inside array bounds, which is something I ran into while trying to work around the other function.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/52
https://github.com/root-project/root/pull/53:67,availability,avail,available,67,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:0,deployability,Updat,Update,0,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:67,reliability,availab,available,67,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:0,safety,Updat,Update,0,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:67,safety,avail,available,67,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:0,security,Updat,Update,0,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/53:67,security,availab,available,67,"Update TMVA scripts for ROOT6; Fixes the method-independent macros available from TMVAGui.C to work with cling. Correlation scatter plots are still broken because of "")"" in strings passed to other macros (reported to D. Piparo).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/53
https://github.com/root-project/root/pull/54:145,availability,error,error,145,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:254,availability,error,error,254,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:145,performance,error,error,145,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:254,performance,error,error,254,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:145,safety,error,error,145,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:254,safety,error,error,254,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:88,usability,User,Users,88,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:145,usability,error,error,145,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/54:254,usability,error,error,254,TNetXNGFile: explicitly include XrdVersion.hh; Fixes compilation with xrootd v4:. ```. /Users/veprbl/root/net/netxng/src/TNetXNGFile.cxx:670:26: error: no member named 'GetDataServer' in 'XrdCl::File'. URL dataServer(fFile->GetDataServer());. ~~~~~ ^. 1 error generated. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/54
https://github.com/root-project/root/pull/55:7,safety,safe,safe,7,"Thread-safe TClass enums and TCling return values; Made obtaining the list of enums from a TClass thread safe. As part of that, made all the statics used as return values by TCling to be thread_local.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/55
https://github.com/root-project/root/pull/55:105,safety,safe,safe,105,"Thread-safe TClass enums and TCling return values; Made obtaining the list of enums from a TClass thread safe. As part of that, made all the statics used as return values by TCling to be thread_local.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/55
https://github.com/root-project/root/pull/56:50,modifiability,exten,extends,50,Thread-safe interaction with all enum lists; This extends the previous work on enum thread-safety to encompass. all the ways the enums can be scoped.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/56
https://github.com/root-project/root/pull/56:7,safety,safe,safe,7,Thread-safe interaction with all enum lists; This extends the previous work on enum thread-safety to encompass. all the ways the enums can be scoped.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/56
https://github.com/root-project/root/pull/56:91,safety,safe,safety,91,Thread-safe interaction with all enum lists; This extends the previous work on enum thread-safety to encompass. all the ways the enums can be scoped.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/56
https://github.com/root-project/root/pull/56:12,usability,interact,interaction,12,Thread-safe interaction with all enum lists; This extends the previous work on enum thread-safety to encompass. all the ways the enums can be scoped.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/56
https://github.com/root-project/root/pull/57:252,integrability,translat,translates,252,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/57
https://github.com/root-project/root/pull/57:252,interoperability,translat,translates,252,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/57
https://github.com/root-project/root/pull/57:53,safety,safe,safe,53,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/57
https://github.com/root-project/root/pull/57:241,safety,safe,safe,241,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/57
https://github.com/root-project/root/pull/57:319,safety,safe,safe,319,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/57
https://github.com/root-project/root/pull/58:252,integrability,translat,translates,252,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/58
https://github.com/root-project/root/pull/58:252,interoperability,translat,translates,252,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/58
https://github.com/root-project/root/pull/58:53,safety,safe,safe,53,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/58
https://github.com/root-project/root/pull/58:241,safety,safe,safe,241,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/58
https://github.com/root-project/root/pull/58:319,safety,safe,safe,319,"Made calling TGenCollectionStreamer::Generate thread safe; Although each thread will get its own copy of TGenCollectionStreamer,. that copy is made by calling Generate on a global instance of the class. Therefore Generate needs to be thread safe. This translates to requiring. TGenCollectionProxy::Initialize be thread safe. This required that. TGenCollectionProxy::fValue be atomic and be the last value to be set. in TGenCollectionProxy::InitializeEx.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/58
https://github.com/root-project/root/pull/59:1733,availability,slo,slower,1733," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2173,availability,slo,slower,2173," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:307,energy efficiency,current,currently,307,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1378,energy efficiency,CPU,CPU,1378," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1458,energy efficiency,CPU,CPU,1458," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1628,energy efficiency,CPU,CPU,1628," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1789,energy efficiency,CPU,CPU,1789," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1832,energy efficiency,CPU,CPU,1832," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1885,energy efficiency,CPU,CPU,1885," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1928,energy efficiency,CPU,CPU,1928," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2072,energy efficiency,CPU,CPU,2072," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2116,energy efficiency,CPU,CPU,2116," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:644,integrability,Event,Event,644,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:784,integrability,Event,Event,784,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:817,integrability,Event,Event,817,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:888,integrability,event,events,888,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1010,integrability,event,event,1010,"he LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% fas",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:737,interoperability,specif,specified,737,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:347,modifiability,deco,decompression,347,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:687,modifiability,exten,extend,687,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1125,modifiability,exten,extended,1125," The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for ",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1763,modifiability,deco,decompression,1763," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1802,modifiability,deco,decompression,1802," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1859,modifiability,deco,decompression,1859," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1898,modifiability,deco,decompression,1898," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1963,modifiability,deco,decompression,1963," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2045,modifiability,deco,decompression,2045," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2085,modifiability,deco,decompression,2085," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2152,modifiability,deco,decompression,2152," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:777,performance,time,time,777,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:810,performance,time,time,810,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1228,performance,time,time-only,1228," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1245,performance,Perform,Performance,1245," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1378,performance,CPU,CPU,1378," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1382,performance,time,time,1382," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1458,performance,CPU,CPU,1458," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1462,performance,time,time,1462," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1550,performance,time,time,1550," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1628,performance,CPU,CPU,1628," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1632,performance,time,time,1632," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1720,performance,time,time,1720," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1789,performance,CPU,CPU,1789," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1793,performance,time,time,1793," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1832,performance,CPU,CPU,1832," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1836,performance,time,time,1836," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1885,performance,CPU,CPU,1885," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1889,performance,time,time,1889," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1928,performance,CPU,CPU,1928," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1932,performance,time,time,1932," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2072,performance,CPU,CPU,2072," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2076,performance,time,time,2076," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2116,performance,CPU,CPU,2116," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2120,performance,time,time,2120," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1733,reliability,slo,slower,1733," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:2173,reliability,slo,slower,2173," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:286,safety,test,test,286,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:668,safety,test,test,668,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1266,safety,test,testing,1266," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1202,security,access,access,1202," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:286,testability,test,test,286,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:668,testability,test,test,668,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1266,testability,test,testing,1266," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:0,usability,Support,Support,0,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:54,usability,support,support,54,"Support for the LZ4 algorithm; This pull request adds support for the LZ4 compression algorithm (https://code.google.com/p/lz4/). The code is BSD-licensed. The pull-request includes a source tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulti",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/59:1245,usability,Perform,Performance,1245," tarball (allowing a ""builtin_lz4"" option in CMake analogous to ""builtin_lzma""). I am unable to test this on Windows currently. The LZ4 algorithm focuses on decompression speed while sacrificing compression ratio. Compression level < 4 uses the ""LZ4 compression"" algorithm while compression level >= 4 uses ""LZ4HC"" (HC = high compression) variant. LZ4HC is comparable to zlib in speeds, but has approximately 20% larger file size. To evaluate, I use the Event executable in the test/ directory. I extend this to allow the compression algorithm be specified via CLI. Example invocation:. time ./Event 4000 6 99 1 1000 4; time ./Event 4000 4 99 0 1000 4. Here, the CLI arguments are:. - 1: Number of events (4000). - 2: Compression ratio (6). - 3: Split level (99). - 4: 1 for write, 0 for read. - 5: Number of tracks per event (1000). - 6: Compression algorithm (1 = zlib, 2 = lzma, 4 = lz4. 3 is the deprecated zlib-like algorithm). I extended MainEvent.cxx to include TTreePerfStats information, which gives us access to the compression-time-only rates. Performance results (testing on a 2.6GHz Intel Westmere E56xx-based VM) summary:. LZ4HC compression:. - File size: 231MB. - 14.7MB/s CPU time for writing. ZLIB level-6 compression:. - File size: 189MB. - 10.5MB/s CPU time for writing. Summary: LZ4HC compression resulted in a file 20% larger. Compression time was 44% faster. LZMA level-6 compression:. - File size: 163MB. - .62MB/s CPU time for writing. Summary: LZMA compression resulted in a file 13% smaller. Compression time was 16x slower than ZLIB level-6. LZ4 decompression:. - 233MB/s CPU time for decompression only. - 189MB/s CPU time for reading. ZLIB decompression:. - 118MB/s CPU time for decompression only. - 104MB/s CPU time for reading. Summary: LZ4 decompression was 97% faster, resulting in 81% faster reading for this case. LZMA decompression:. - 15.4MB/s CPU time for decompression only. - 14.7MB/s CPU time for reading. Summary: LZMA decompression was 7x slower than zlib.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/59
https://github.com/root-project/root/pull/60:62,deployability,compos,composite,62,"Port of ratioplot.C to python; Additional pyroot tutorial for composite plots. ratioplot.py generates two random gaussian histograms, plots them within the same graph, and plots their ratios (with uncertainties) below that graph. Line to freeze canvas after generation is included, but commented out.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/60
https://github.com/root-project/root/pull/60:62,modifiability,compos,composite,62,"Port of ratioplot.C to python; Additional pyroot tutorial for composite plots. ratioplot.py generates two random gaussian histograms, plots them within the same graph, and plots their ratios (with uncertainties) below that graph. Line to freeze canvas after generation is included, but commented out.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/60
https://github.com/root-project/root/pull/61:161,availability,avail,available,161,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:116,reliability,doe,does,116,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:161,reliability,availab,available,161,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:151,safety,test,tests,151,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:161,safety,avail,available,161,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:161,security,availab,available,161,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:146,testability,unit,unit,146,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:151,testability,test,tests,151,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/61:97,usability,clear,clear,97,"Change auto-generated code for std::bitset to use reset method.; Previously, it tried to use the clear method which does not exist. Corresponding unit tests are available on this branch:. https://github.com/bbockelm/roottest/tree/makeproject_bitset. @pcanal .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/61
https://github.com/root-project/root/pull/62:38,deployability,manag,manager,38,Quiet unnecessary warning from plugin manager.; Only print a warning if there's actually a difference in the number of arguments.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/62
https://github.com/root-project/root/pull/62:38,energy efficiency,manag,manager,38,Quiet unnecessary warning from plugin manager.; Only print a warning if there's actually a difference in the number of arguments.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/62
https://github.com/root-project/root/pull/62:31,interoperability,plug,plugin,31,Quiet unnecessary warning from plugin manager.; Only print a warning if there's actually a difference in the number of arguments.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/62
https://github.com/root-project/root/pull/62:38,safety,manag,manager,38,Quiet unnecessary warning from plugin manager.; Only print a warning if there's actually a difference in the number of arguments.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/62
https://github.com/root-project/root/pull/63:113,availability,operat,operation,113,"Replace the Query option by Filter and Aliphysics ones; With those modifications, one can trigger a stage+filter operation on an Alice Analysis Facility. PS: this is my first ever pull request (being Root or otherwise) so please bear with me if that's not the correct procedure to contribute back to Root...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/63
https://github.com/root-project/root/pull/63:100,deployability,stage,stage,100,"Replace the Query option by Filter and Aliphysics ones; With those modifications, one can trigger a stage+filter operation on an Alice Analysis Facility. PS: this is my first ever pull request (being Root or otherwise) so please bear with me if that's not the correct procedure to contribute back to Root...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/63
https://github.com/root-project/root/pull/63:28,integrability,Filter,Filter,28,"Replace the Query option by Filter and Aliphysics ones; With those modifications, one can trigger a stage+filter operation on an Alice Analysis Facility. PS: this is my first ever pull request (being Root or otherwise) so please bear with me if that's not the correct procedure to contribute back to Root...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/63
https://github.com/root-project/root/pull/63:106,integrability,filter,filter,106,"Replace the Query option by Filter and Aliphysics ones; With those modifications, one can trigger a stage+filter operation on an Alice Analysis Facility. PS: this is my first ever pull request (being Root or otherwise) so please bear with me if that's not the correct procedure to contribute back to Root...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/63
https://github.com/root-project/root/pull/63:67,security,modif,modifications,67,"Replace the Query option by Filter and Aliphysics ones; With those modifications, one can trigger a stage+filter operation on an Alice Analysis Facility. PS: this is my first ever pull request (being Root or otherwise) so please bear with me if that's not the correct procedure to contribute back to Root...",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/63
https://github.com/root-project/root/pull/64:143,availability,error,error,143,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:6,deployability,build,build,6,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:46,deployability,build,building,46,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:125,deployability,modul,module,125,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:321,energy efficiency,optim,optimizations,321,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:125,modifiability,modul,module,125,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:143,performance,error,error,143,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:321,performance,optimiz,optimizations,321,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:125,safety,modul,module,125,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:143,safety,error,error,143,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:30,testability,assert,assertion,30,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:82,testability,assert,assertion,82,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:192,testability,assert,assertion,192,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:288,testability,assert,assertion,288,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/64:143,usability,error,error,143,gcc49 build on osx fix linker assertion; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/64
https://github.com/root-project/root/pull/65:156,availability,error,error,156,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:6,deployability,build,build,6,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:59,deployability,build,building,59,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:138,deployability,modul,module,138,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:334,energy efficiency,optim,optimizations,334,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:138,modifiability,modul,module,138,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:156,performance,error,error,156,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:334,performance,optimiz,optimizations,334,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:138,safety,modul,module,138,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:156,safety,error,error,156,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:30,testability,assert,assertion,30,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:95,testability,assert,assertion,95,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:205,testability,assert,assertion,205,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:301,testability,assert,assertion,301,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/65:156,usability,error,error,156,gcc49 build on osx fix linker assertion #64 resubmit; When building with gcc49 on osx a linker assertion happens when linking interpreter module. Trial and error reveal that setting -O0 removes the linker assertion. Dan Riley found that adding the flag -fno-omit-frame-pointer also removed the linker assertion without removing other optimizations.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/65
https://github.com/root-project/root/pull/66:17,deployability,Modul,Module,17,interpreter/llvm/Module.mk -- add extra $ in front of LLVM_FLAGS for osx gcc build;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/66
https://github.com/root-project/root/pull/66:77,deployability,build,build,77,interpreter/llvm/Module.mk -- add extra $ in front of LLVM_FLAGS for osx gcc build;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/66
https://github.com/root-project/root/pull/66:17,modifiability,Modul,Module,17,interpreter/llvm/Module.mk -- add extra $ in front of LLVM_FLAGS for osx gcc build;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/66
https://github.com/root-project/root/pull/66:17,safety,Modul,Module,17,interpreter/llvm/Module.mk -- add extra $ in front of LLVM_FLAGS for osx gcc build;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/66
https://github.com/root-project/root/pull/67:12,performance,lock,lock,12,[WIP] Use a lock to protect access to collection from TROOT::GetListOfCleanups; The protection of the collection returned from TROOT::GetListOfCleanups(). was insufficient since thread related crashes could still occur. This. commit protects all uses except for those from the GUI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/67
https://github.com/root-project/root/pull/67:251,safety,except,except,251,[WIP] Use a lock to protect access to collection from TROOT::GetListOfCleanups; The protection of the collection returned from TROOT::GetListOfCleanups(). was insufficient since thread related crashes could still occur. This. commit protects all uses except for those from the GUI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/67
https://github.com/root-project/root/pull/67:12,security,lock,lock,12,[WIP] Use a lock to protect access to collection from TROOT::GetListOfCleanups; The protection of the collection returned from TROOT::GetListOfCleanups(). was insufficient since thread related crashes could still occur. This. commit protects all uses except for those from the GUI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/67
https://github.com/root-project/root/pull/67:28,security,access,access,28,[WIP] Use a lock to protect access to collection from TROOT::GetListOfCleanups; The protection of the collection returned from TROOT::GetListOfCleanups(). was insufficient since thread related crashes could still occur. This. commit protects all uses except for those from the GUI.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/67
https://github.com/root-project/root/pull/68:69,deployability,build,build,69,interpreter/llvm/CMakelists.txt: add -fno-omit-frame-pointer for gcc build on osx; This is a workaround for the linker assert when building with gcc on osx. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/68
https://github.com/root-project/root/pull/68:131,deployability,build,building,131,interpreter/llvm/CMakelists.txt: add -fno-omit-frame-pointer for gcc build on osx; This is a workaround for the linker assert when building with gcc on osx. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/68
https://github.com/root-project/root/pull/68:119,testability,assert,assert,119,interpreter/llvm/CMakelists.txt: add -fno-omit-frame-pointer for gcc build on osx; This is a workaround for the linker assert when building with gcc on osx. .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/68
https://github.com/root-project/root/pull/70:7,integrability,buffer,buffers,7,Shrink buffers when requested.; Previous fixes to TBuffer::Expand broke its ability to shrink when data is not being copied (essential for keeping TBranches small).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/70
https://github.com/root-project/root/pull/71:11,modifiability,variab,variable,11,Fix unused variable warning.; This appears both on master and in the v5-34-XX branch. Can it be cleaned up?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/71
https://github.com/root-project/root/pull/72:5,interoperability,specif,specific,5,"msvc specific link command; these are specific link options to be used only on a msvc host, won't work for a WIN32 mingw target for example. what does it do ? it seems it relinks the PyROOT library, with different options maybe ? isn't it something that could be achieved with cmake in a more generic way, by editing properties or flags ? also it seems its specific to arch ix86.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/72
https://github.com/root-project/root/pull/72:38,interoperability,specif,specific,38,"msvc specific link command; these are specific link options to be used only on a msvc host, won't work for a WIN32 mingw target for example. what does it do ? it seems it relinks the PyROOT library, with different options maybe ? isn't it something that could be achieved with cmake in a more generic way, by editing properties or flags ? also it seems its specific to arch ix86.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/72
https://github.com/root-project/root/pull/72:357,interoperability,specif,specific,357,"msvc specific link command; these are specific link options to be used only on a msvc host, won't work for a WIN32 mingw target for example. what does it do ? it seems it relinks the PyROOT library, with different options maybe ? isn't it something that could be achieved with cmake in a more generic way, by editing properties or flags ? also it seems its specific to arch ix86.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/72
https://github.com/root-project/root/pull/72:146,reliability,doe,does,146,"msvc specific link command; these are specific link options to be used only on a msvc host, won't work for a WIN32 mingw target for example. what does it do ? it seems it relinks the PyROOT library, with different options maybe ? isn't it something that could be achieved with cmake in a more generic way, by editing properties or flags ? also it seems its specific to arch ix86.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/72
https://github.com/root-project/root/pull/72:19,usability,command,command,19,"msvc specific link command; these are specific link options to be used only on a msvc host, won't work for a WIN32 mingw target for example. what does it do ? it seems it relinks the PyROOT library, with different options maybe ? isn't it something that could be achieved with cmake in a more generic way, by editing properties or flags ? also it seems its specific to arch ix86.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/72
https://github.com/root-project/root/pull/73:0,deployability,instal,install,0,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:15,deployability,modul,module,15,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:52,deployability,patch,patch,52,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:58,deployability,instal,installs,58,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:74,deployability,modul,modules,74,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:276,deployability,instal,installing,276,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:101,interoperability,standard,standard,101,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:15,modifiability,modul,module,15,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:74,modifiability,modul,modules,74,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:363,modifiability,Pac,Packaging,363,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:432,modifiability,pac,packaging-manuals,432,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:15,safety,modul,module,15,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:52,safety,patch,patch,52,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:74,safety,modul,modules,74,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:238,safety,avoid,avoids,238,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:52,security,patch,patch,52,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:457,security,polic,policy,457,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/73:339,usability,user,user,339,"install python module to site-dir (ROOT-3316); This patch installs python modules to python site-dir standard location (see some doc here:https://docs.python.org/2/library/site.html), see https://sft.its.cern.ch/jira/browse/ROOT-3316. It avoids to have to set PYTHONPATH when installing to a system folder /usr or /usr/local, and even the user site-dir ~/.local. Packaging may have to be reworked though (https://www.debian.org/doc/packaging-manuals/python-policy/ch-python.html#s-paths).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/73
https://github.com/root-project/root/pull/74:165,availability,operat,operators,165,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:277,deployability,updat,updated,277,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:277,safety,updat,updated,277,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:277,security,updat,updated,277,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:263,usability,Document,Documentation,263,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:335,usability,User,Users,335,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/74:341,usability,Guid,Guide,341,"Master root r; Code from ROOT R project,. -> New features added. now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. Best Regards.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/74
https://github.com/root-project/root/pull/75:17,deployability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,deployability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,integrability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,integrability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,interoperability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,interoperability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,modifiability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,modifiability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,reliability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,reliability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:152,safety,prevent,prevented,152,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,security,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:152,security,preven,prevented,152,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:362,security,ident,identifiers,362,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,security,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:17,testability,integr,integration,17,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/75:615,testability,integr,integration,615,"New printValue() integration; **1st commit:** Do not randomly add to lookup results during <code>EvaluateT()</code>. Axel fixed the notorious bug which prevented having templated <code>printValue()</code> functions in the ""RuntimePrintValue.h"" header which is declared programmatically on the first <code>printValue()</code>invocation. Runtime resolving of some identifiers was used when it shouldn't be. Moving the checking of this condition to the beginning of the function fixed the issue. **2nd commit:** Removed old <code>printValue</code> from TDatime and TString. **3rd commit:** New <code>printValue</code> integration. **4th commit:** Re-added <code>printValue</code> functionality to TString and TDatime. **5th, final commit:** Minor fix.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/75
https://github.com/root-project/root/pull/76:187,testability,simpl,simpler,187,Print value fixes and removal of the old code; Print value fixes introduced based on comments after the first pull request. Removed the old code and made various changes to make the code simpler.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/76
https://github.com/root-project/root/pull/76:187,usability,simpl,simpler,187,Print value fixes and removal of the old code; Print value fixes introduced based on comments after the first pull request. Removed the old code and made various changes to make the code simpler.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/76
https://github.com/root-project/root/pull/77:217,interoperability,format,format,217,"Print value fixes v2, fixes roottest; **Removed ""VALID"" after address output, fixes roottest.**. Various fixes based on comments after second round. _More changes to come (char removal, CATCHALL, potential real types format refinements)._.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/77
https://github.com/root-project/root/pull/77:49,safety,VALID,VALID,49,"Print value fixes v2, fixes roottest; **Removed ""VALID"" after address output, fixes roottest.**. Various fixes based on comments after second round. _More changes to come (char removal, CATCHALL, potential real types format refinements)._.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/77
https://github.com/root-project/root/pull/78:437,availability,error,error,437,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/78:437,performance,error,error,437,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/78:437,safety,error,error,437,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/78:409,testability,assert,assert,409,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/78:352,usability,support,support,352,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/78:437,usability,error,error,437,"Catch-all printValue implementation changed from refs to pointers; Catch-all printValue implementation changed to enable correct invocation if only parent type overload exists (ex. if there is no overload for TF1*, compiler invokes the overload to its best parent overload match, in the worst case void*). Argument changed from reference to pointer to support this. isEnumType Coverity bug changed from if to assert (coding, not runtime error). Changed the way printValue is invoked in order to correctly cast Value to the needed value (e.g. LL -> short). Extracted value stays in scope while we execute printValue, because we use the address.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/78
https://github.com/root-project/root/pull/80:0,usability,Interact,Interactive,0,Interactive printing mode .interactive (5 lines by 5 lines).;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/80
https://github.com/root-project/root/pull/80:27,usability,interact,interactive,27,Interactive printing mode .interactive (5 lines by 5 lines).;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/80
https://github.com/root-project/root/pull/81:55,deployability,updat,updated,55,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:37,interoperability,format,format,37,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:196,modifiability,deco,decompression,196,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:585,modifiability,deco,decompressed,585,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:669,modifiability,deco,decompression,669,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:862,modifiability,deco,decompression,862,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:124,performance,perform,performance,124,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:417,performance,perform,performance,417,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:731,performance,time,time,731,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:55,safety,updat,updated,55,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:741,safety,test,test,741,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:809,safety,test,tests,809,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:55,security,updat,updated,55,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:741,testability,test,test,741,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:809,testability,test,tests,809,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:4,usability,support,support,4,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:124,usability,perform,performance,124,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/81:417,usability,perform,performance,417,"Add support for LZ4 as a compression format; This is a updated pull request from #59 The same experiments have been run and performance results are shown here:. | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 11.74 MB/s | 131.06 MB/s | 181 MB |. | lzma | 0.86 MB/s | 17.36 MB/s | 157 MB |. | lz4 | 5.22 MB/s | 143.81 MB/s | 221 MB |. The following performance is from the root file @pcanal's ticket (https://root.cern.ch/files/CMS_7250E9A5-682D-DF11-8701-002618943934.root). The file is 1.9 GB large, and I tried to decompressed it and it seems its original size is 6.4 GB. The following compression/decompression speeds are calculated by dividing 6.4 GB by the time each test run. @bbockelm , we could discuss implementation details of my tests tomorrow. . | Algorithm | compression(write) | decompression(read) | Compressed File Size |. | --- | --- | --- | --- |. | zlib | 15.83 MB/s | 63.23 MB/s | 1.6 GB |. | lzma | 1.28 MB/s | 22.62 MB/s | 1.2 GB |. | lz4 | 8.32 MB/s | 66.53 MB/s | 1.8 GB |.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/81
https://github.com/root-project/root/pull/82:37,availability,mask,masked,37,Comments and refactorings (hidden -> masked) in the TextInput.; Passes cling test and roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/82
https://github.com/root-project/root/pull/82:13,modifiability,refact,refactorings,13,Comments and refactorings (hidden -> masked) in the TextInput.; Passes cling test and roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/82
https://github.com/root-project/root/pull/82:13,performance,refactor,refactorings,13,Comments and refactorings (hidden -> masked) in the TextInput.; Passes cling test and roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/82
https://github.com/root-project/root/pull/82:77,safety,test,test,77,Comments and refactorings (hidden -> masked) in the TextInput.; Passes cling test and roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/82
https://github.com/root-project/root/pull/82:77,testability,test,test,77,Comments and refactorings (hidden -> masked) in the TextInput.; Passes cling test and roottest.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/82
https://github.com/root-project/root/pull/83:228,availability,operat,operators,228,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:492,availability,operat,operators,492,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:340,deployability,updat,updated,340,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,deployability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,integrability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,interoperability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,modifiability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,reliability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:81,safety,review,review,81,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:340,safety,updat,updated,340,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:340,security,updat,updated,340,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,security,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:81,testability,review,review,81,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:631,testability,integr,integrate,631,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:100,usability,resum,resume,100,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:326,usability,Document,Documentation,326,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:398,usability,User,Users,398,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:404,usability,Guid,Guide,404,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:427,usability,document,documentation,427,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:529,usability,document,documentation,529,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:607,usability,user,users,607,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/83:613,usability,guid,guide,613,"ROOT R:; Hi Axel and Lorenzo,. I take the code and I put it in one commit to let review it easy. In resume the changes are . -> Now you can use R functions in C++ very easy using the class TRFunctionImport, that have overloaded operators to use objects like functions that receives template arguments and return TRObjects. -> Documentation updated in http://oproject.org/tiki-index.php?page=ROOT+R+Users+Guide#Import. -> added documentation in doxygen. -> new propieties for TRDataFrame with operators. You can see the output of documentation in. http://files.oproject.org/root/rootdoc/html/group___r.html. users guide in markdown integrate to doxygen . http://files.oproject.org/root/rootdoc/html/md__home_omazapa_root_bindings_r_doc_users-guide__r_o_o_t_r__users__guide.html. I have the code now in http://github.com/oprojects/root. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/83
https://github.com/root-project/root/pull/84:112,availability,error,error,112,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:96,deployability,patch,patch,96,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:141,modifiability,interm,intermediate,141,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:112,performance,error,error,112,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:221,performance,memor,memory,221,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:96,safety,patch,patch,96,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:112,safety,error,error,112,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:235,safety,input,input,235,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:96,security,patch,patch,96,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:112,usability,error,error,112,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:177,usability,efficien,efficient,177,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:221,usability,memor,memory,221,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/84:235,usability,input,input,235,"Fix issue ROOT-7588.; cin is not seekable. Temporary stringstream is created to store cin. This patch fixes the error. But I am afraid using intermediate stringstream is not an efficient way, especially it costs a lot of memory if the input file is large. Any suggestion? @bbockelm .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/84
https://github.com/root-project/root/pull/85:202,deployability,toggl,toggling,202,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:176,energy efficiency,current,current,176,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:33,usability,command,command,33,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:75,usability,behavi,behavior,75,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:108,usability,command,command,108,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:118,usability,behavi,behavior,118,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/85:261,usability,intuit,intuitive,261,"Added recursive print depth meta command.; Implemented corresponding print behavior. Maybe revise this meta command's behavior in the case of a missing argument (print out the current value, instead of toggling between two defaults), if it turns out to be more intuitive.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/85
https://github.com/root-project/root/pull/87:9,deployability,patch,patches,9,python 3 patches; Can I propose these small fixes that would make ROOT6 compile-able with Python 3? . Thanks!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/87
https://github.com/root-project/root/pull/87:9,safety,patch,patches,9,python 3 patches; Can I propose these small fixes that would make ROOT6 compile-able with Python 3? . Thanks!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/87
https://github.com/root-project/root/pull/87:9,security,patch,patches,9,python 3 patches; Can I propose these small fixes that would make ROOT6 compile-able with Python 3? . Thanks!,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/87
https://github.com/root-project/root/pull/88:433,availability,error,error,433,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:87,deployability,patch,patch,87,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:155,deployability,instal,installed,155,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:211,deployability,instal,installed,211,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:550,deployability,patch,patch,550,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:11,integrability,Pub,Public,11,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:147,integrability,pub,public,147,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:433,performance,error,error,433,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:87,safety,patch,patch,87,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:433,safety,error,error,433,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:550,safety,patch,patch,550,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:87,security,patch,patch,87,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:550,security,patch,patch,550,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/88:433,usability,error,error,433,"ROOT-7541: Public header #includes private header; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-7541. The public (installed) header TXSocket.h #includes the private (not installed) header XrdProofConn.h. This means it can not be used in compilations:. g++ -I/usr/include/root -x c++ - <<< '#include ""TXSocket.h""'. In file included from <stdin>:1:0:. /usr/include/root/TXSocket.h:47:26: fatal error: XrdProofConn.h: No such file or directory. compilation terminated. A possible fix is provided in the attached patch.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/88
https://github.com/root-project/root/pull/89:135,availability,error,errors,135,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:11,deployability,Updat,Update,11,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:61,deployability,patch,patch,61,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:168,deployability,modul,module,168,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:168,modifiability,modul,module,168,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:135,performance,error,errors,135,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:11,safety,Updat,Update,11,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:61,safety,patch,patch,61,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:135,safety,error,errors,135,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:168,safety,modul,module,168,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:11,security,Updat,Update,11,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:61,security,patch,patch,61,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/89:135,usability,error,errors,135,ROOT-5494: Update THDFS; This is a pull request based on the patch in https://sft.its.cern.ch/jira/browse/ROOT-5494. Fixes compilation errors and warnings in the THDFS module.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/89
https://github.com/root-project/root/pull/90:24,availability,fault,fault,24,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:95,deployability,patch,patches,95,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1082,deployability,patch,patch,1082,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1123,deployability,patch,patch,1123,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:24,energy efficiency,fault,fault,24,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:554,energy efficiency,current,currently,554,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1049,energy efficiency,current,current,1049,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:808,interoperability,format,format,808,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:972,interoperability,architectur,architectures,972,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:24,performance,fault,fault,24,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:24,reliability,fault,fault,24,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:694,reliability,doe,does,694,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:24,safety,fault,fault,24,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:95,safety,patch,patches,95,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1082,safety,patch,patch,1082,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1123,safety,patch,patch,1123,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:95,security,patch,patches,95,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1082,security,patch,patch,1082,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/90:1123,security,patch,patch,1123,"Root 7457: Segmentation fault when embedding Type 1 fonts; This is a pull request based on the patches in https://sft.its.cern.ch/jira/browse/ROOT-7457. There are two problem with the code. Looking at a hexdump of a Type 1 font:. ```. 00000000 80 01 fb 15 00 00 25 21 50 53 2d 41 64 6f 62 65 |......%!PS-Adobe|. 00000010 46 6f 6e 74 2d 31 2e 30 3a 20 53 74 61 6e 64 61 |Font-1.0: Standa|. [ ... ]. 00008390 30 30 30 30 30 30 30 30 30 30 30 30 30 30 0d 63 |00000000000000.c|. 000083a0 6c 65 61 72 74 6f 6d 61 72 6b 0a 80 03 |leartomark...|. ```. The code currently reads beyond the 80 03 at the end of the file by trying to determing the length of the following block - but an end of file block does not have a length, the file ends right after the end of file block tag 08 03. The length is in little endian format - as can be seen in the beginning of the file. The ascii block tag 80 01 is followed by fb 15 00 00 which is little endian for 000015fb. So it is big endian architectures that needs to do a byte swap, not little endian ones as in the current code. The first attached patch addresses these issues. The second patch implements returning the fontname for Type 1 embedding.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/90
https://github.com/root-project/root/pull/91:317,deployability,patch,patch,317,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:260,integrability,configur,configure,260,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:260,modifiability,configur,configure,260,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:149,performance,time,time,149,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:242,performance,time,time,242,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:252,safety,test,test,252,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:317,safety,patch,patch,317,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:260,security,configur,configure,260,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:317,security,patch,patch,317,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:252,testability,test,test,252,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/91:56,usability,support,support,56,"ROOT-7542: Remove dead xrootd bonjour code; The bonjour support in xrootd (XrdOuc/XrdOucBonjour.hh) was dropped in xrootd 3.2.0, which is now a long time ago. The code in root that requires this header is therefore now dead code since a long time. The test in configure to enable it newer worked anyway. The attached patch removes the dead code.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/91
https://github.com/root-project/root/pull/92:89,usability,document,documenttation,89,"master oproject; Hi,. This pull request have the fixes for PyMVA in sl6 and some doxygen documenttation for RMVA. Best Regards. O.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/92
https://github.com/root-project/root/pull/93:84,performance,cach,cached,84,TClass::GetCheckSum fix for returned arugment; If the GetCheckSum value was already cached then the argument passed. to the function was not set. This now properly sets the value.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/93
https://github.com/root-project/root/pull/94:29,deployability,patch,patches,29,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:89,deployability,build,build,89,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:494,deployability,API,API,494,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:494,integrability,API,API,494,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:408,interoperability,standard,standard,408,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:494,interoperability,API,API,494,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:29,safety,patch,patches,29,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:29,security,patch,patches,29,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:341,security,access,access,341,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:502,security,command-lin,command-line,502,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:298,usability,user,user,298,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:303,usability,user,username,303,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/94:502,usability,command,command-line,502,"THDFSFile fixes; This set of patches makes THDFSFile work again. . It also enables CMake build and allows linking against libhdfs3 (experimental native HDFS client implementation). Kind of major change: HDFS URLs are now absolute instead of relative as it was before. I.e. one have to use ""hdfs:///user/username/dir1/file2.root"" notation to access file in the home directory. . This makes HDFS URLs somewhat standard in the sense that they could be used interchangeably between ROOT and Hadoop API and command-line utilities. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/94
https://github.com/root-project/root/pull/95:70,deployability,releas,release-indices,70,"Fix typo ouput => output in various places.; User-visible places: Old release-indices, informative output of hadd.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/95
https://github.com/root-project/root/pull/95:45,usability,User,User-visible,45,"Fix typo ouput => output in various places.; User-visible places: Old release-indices, informative output of hadd.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/95
https://github.com/root-project/root/pull/96:261,availability,Servic,Services,261,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:62,deployability,patch,patch,62,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:261,deployability,Servic,Services,261,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:308,deployability,patch,patch,308,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:435,deployability,patch,patch,435,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:261,integrability,Servic,Services,261,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:261,modifiability,Servic,Services,261,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:331,performance,multi-thread,multi-thread,331,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:27,safety,avoid,avoid,27,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:39,safety,unsaf,unsafe,39,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:62,safety,patch,patch,62,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:111,safety,avoid,avoids,111,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:124,safety,unsaf,unsafe,124,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:308,safety,patch,patch,308,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:344,safety,test,test,344,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:388,safety,compl,complicated,388,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:400,safety,test,test,400,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:435,safety,patch,patch,435,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:0,security,Modif,Modified,0,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:9,security,sign,signal,9,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:62,security,patch,patch,62,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:82,security,sign,signal,82,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:144,security,sign,signal,144,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:308,security,patch,patch,308,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:388,security,compl,complicated,388,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:435,security,patch,patch,435,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:324,testability,simpl,simple,324,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:344,testability,test,test,344,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:400,testability,test,test,400,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/96:324,usability,simpl,simple,324,"Modified signal handler to avoid async-unsafe functions; This patch reimplemented signal handling in CMSSW. It avoids async-unsafe functions in signal handler. For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/96
https://github.com/root-project/root/pull/97:299,availability,Servic,Services,299,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:83,deployability,patch,patch,83,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:299,deployability,Servic,Services,299,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:346,deployability,patch,patch,346,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:473,deployability,patch,patch,473,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:299,integrability,Servic,Services,299,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:299,modifiability,Servic,Services,299,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:369,performance,multi-thread,multi-thread,369,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:30,safety,avoid,avoid,30,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:42,safety,unsaf,unsafe,42,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:83,safety,patch,patch,83,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:137,safety,avoid,avoids,137,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:150,safety,unsaf,unsafe,150,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:346,safety,patch,patch,346,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:382,safety,test,test,382,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:426,safety,compl,complicated,426,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:438,safety,test,test,438,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:473,safety,patch,patch,473,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:12,security,sign,signal,12,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:83,security,patch,patch,83,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:108,security,sign,signal,108,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:170,security,sign,signal,170,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:346,security,patch,patch,346,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:426,security,compl,complicated,426,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:473,security,patch,patch,473,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:362,testability,simpl,simple,362,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:382,testability,test,test,382,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:438,testability,test,test,438,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/97:362,usability,simpl,simple,362,"Reimplement signal handler to avoid async-unsafe functions; @bbockelm @pcanal This patch copies the code of signal handling in CMSSW. It avoids async-unsafe functions in signal handler functions. . For reference, see the link https://github.com/bbockelm/cmssw/blob/stacktrace_handler_revisit/FWCore/Services/src/InitRootHandlers.cc. I tried this patch with some simple multi-thread test cases and it worked fine. Is there any complicated test cases I can run? I think this patch is not very ready to merge, but it achieved basic functions. Any criticisms are welcome.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/97
https://github.com/root-project/root/pull/98:271,performance,time,time,271,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/98:129,safety,compl,complete,129,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/98:279,safety,avoid,avoiding,279,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/98:129,security,compl,complete,129,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/98:54,usability,document,documentation,54,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/98:211,usability,document,documentation,211,TAttMarker: Added missing marker style names in class documentation; It is a minor fix but I though it would be nice to have the complete list since. I always forget the style names for polymarkers and use this documentation as. a reference. May also save someone else's time by avoiding a lookup in the. header file.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/98
https://github.com/root-project/root/pull/99:55,integrability,event,event,55,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:139,integrability,pub,public,139,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:634,integrability,interfac,interface,634,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:634,interoperability,interfac,interface,634,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:634,modifiability,interfac,interface,634,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:185,reliability,doe,does,185,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:463,reliability,doe,does,463,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:4,security,access,accessor,4,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:90,security,access,accessed,90,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:179,security,hack,hack,179,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/99:360,usability,minim,minimal,360,"Add accessor functions and functionality needed by CMS event display.; Parts of this were accessed through redefining private/protected as public. which, besides of being a nasty hack, does not work with gcc-5. TColor had a static bool member fgInitDone that is now a local static in InitializeColors(). I just added a bool argument force=kFALSE as this was a minimal change. If desired, I can do the following:. . Split InitializeColors() into initial part that does the check is-init-done and the actual initialization code that is private. . Introduce new static function InitializeColorsForce() that skips the check. This way the interface to InitializeColors() will not change.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/99
https://github.com/root-project/root/pull/100:150,deployability,depend,depends,150,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:150,integrability,depend,depends,150,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:150,modifiability,depend,depends,150,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:4,performance,perform,performance,4,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:150,safety,depend,depends,150,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:226,safety,test,test,226,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:57,testability,regress,regression,57,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:150,testability,depend,depends,150,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:226,testability,test,test,226,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:4,usability,perform,performance,4,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/100:285,usability,progress,progress,285,"Bdt performance; Increase the speed of BDT training. For regression analysis with Grad boosting, the speed gain is almost 2x. For multiclass the gain depends on the number of multiclasses. For classification: haven't done the test. Non BDT algorithms will also be faster (assuming the progress bar is enabled).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/100
https://github.com/root-project/root/pull/101:109,testability,simul,simultaneously,109,Made function pointer held by TMinuitMinimize thread local; In order to allow different histograms to be fit simultaneously. on different threads the function pointer passed to Minuit by. TMinuitMinimizer must be unique for thread thread.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/101
https://github.com/root-project/root/pull/102:81,deployability,patch,patches,81,CMS cleanup backported to 6-04; @pcanal @davidlt -- this is the backport to 6.04-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/102
https://github.com/root-project/root/pull/102:81,safety,patch,patches,81,CMS cleanup backported to 6-04; @pcanal @davidlt -- this is the backport to 6.04-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/102
https://github.com/root-project/root/pull/102:81,security,patch,patches,81,CMS cleanup backported to 6-04; @pcanal @davidlt -- this is the backport to 6.04-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/102
https://github.com/root-project/root/pull/103:81,deployability,patch,patches,81,CMS cleanup backported to 6-02; @pcanal @davidlt -- this is the backport to 6.02-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/103
https://github.com/root-project/root/pull/103:81,safety,patch,patches,81,CMS cleanup backported to 6-02; @pcanal @davidlt -- this is the backport to 6.02-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/103
https://github.com/root-project/root/pull/103:81,security,patch,patches,81,CMS cleanup backported to 6-02; @pcanal @davidlt -- this is the backport to 6.02-patches.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/103
https://github.com/root-project/root/pull/104:94,deployability,contain,contains,94,"new neural net implementation for TMVA; Implementation of a new neural network for TMVA which contains recent developments in the field of neural networks (e.g. weight initialization [Gerlot], SGD, Hogwild style multithreading, drop-out, momentum). It allows for multithreaded training. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/104
https://github.com/root-project/root/pull/104:71,performance,network,network,71,"new neural net implementation for TMVA; Implementation of a new neural network for TMVA which contains recent developments in the field of neural networks (e.g. weight initialization [Gerlot], SGD, Hogwild style multithreading, drop-out, momentum). It allows for multithreaded training. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/104
https://github.com/root-project/root/pull/104:146,performance,network,networks,146,"new neural net implementation for TMVA; Implementation of a new neural network for TMVA which contains recent developments in the field of neural networks (e.g. weight initialization [Gerlot], SGD, Hogwild style multithreading, drop-out, momentum). It allows for multithreaded training. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/104
https://github.com/root-project/root/pull/104:71,security,network,network,71,"new neural net implementation for TMVA; Implementation of a new neural network for TMVA which contains recent developments in the field of neural networks (e.g. weight initialization [Gerlot], SGD, Hogwild style multithreading, drop-out, momentum). It allows for multithreaded training. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/104
https://github.com/root-project/root/pull/104:146,security,network,networks,146,"new neural net implementation for TMVA; Implementation of a new neural network for TMVA which contains recent developments in the field of neural networks (e.g. weight initialization [Gerlot], SGD, Hogwild style multithreading, drop-out, momentum). It allows for multithreaded training. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/104
https://github.com/root-project/root/pull/106:0,energy efficiency,Adapt,Adapt,0,Adapt to gfal2 2.10 - uses a different #define;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/106
https://github.com/root-project/root/pull/106:0,integrability,Adapt,Adapt,0,Adapt to gfal2 2.10 - uses a different #define;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/106
https://github.com/root-project/root/pull/106:0,interoperability,Adapt,Adapt,0,Adapt to gfal2 2.10 - uses a different #define;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/106
https://github.com/root-project/root/pull/106:0,modifiability,Adapt,Adapt,0,Adapt to gfal2 2.10 - uses a different #define;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/106
https://github.com/root-project/root/pull/108:10,availability,error,error,10,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:175,integrability,Protocol,Protocols,175,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:175,interoperability,Protocol,Protocols,175,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:10,performance,error,error,10,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:10,safety,error,error,10,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:10,usability,error,error,10,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:29,usability,Statu,Status,29,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/108:96,usability,Statu,Status,96,"TWebFile: error handling for Status Code 301 without Location; According to the RFC 2616, a 301 Status Code only ""SHOULD"" return. the new URI -- not ""MUST"". http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.2. Jira #7809: https://sft.its.cern.ch/jira/browse/ROOT-7809.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/108
https://github.com/root-project/root/pull/109:65,availability,error,error,65,add constructor for struct Environ; required to suppress warning/error when compiling with -Werror=effc++.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/109
https://github.com/root-project/root/pull/109:65,performance,error,error,65,add constructor for struct Environ; required to suppress warning/error when compiling with -Werror=effc++.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/109
https://github.com/root-project/root/pull/109:65,safety,error,error,65,add constructor for struct Environ; required to suppress warning/error when compiling with -Werror=effc++.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/109
https://github.com/root-project/root/pull/109:65,usability,error,error,65,add constructor for struct Environ; required to suppress warning/error when compiling with -Werror=effc++.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/109
https://github.com/root-project/root/pull/110:123,availability,Servic,Service,123,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:123,deployability,Servic,Service,123,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:123,integrability,Servic,Service,123,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:343,integrability,pub,public,343,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:123,modifiability,Servic,Service,123,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:29,security,session,session,29,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:108,security,Secur,Security,108,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:117,security,Token,Token,117,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:213,security,access,access,213,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:224,security,secur,secure,224,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:262,security,session,session,262,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:270,security,token,token,270,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:17,usability,support,support,17,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:156,usability,document,documentation,156,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:309,usability,support,support,309,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:417,usability,prefer,prefer,417,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/110:528,usability,cursor,cursory,528,"TS3WebFile - add support for session keys; Hi,. Amazon has this concept of short-lived credentials via the ""Security Token Service"" (https://aws.amazon.com/documentation/iam/). In addition to generating the usual access and secure keys, the STS also generates a session token that needs to be used. This adds support for this. . I made fToken public instead of a do-nothing getter, but that's a trivial change if you prefer it the other way. (Also I should ask Georgios and friends to add this to davix - I didn't see it from a cursory glance at the code.).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/110
https://github.com/root-project/root/pull/111:27,safety,test,test,27,Enable the NullDeref Cling test for MethodCalls.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/111
https://github.com/root-project/root/pull/111:27,testability,test,test,27,Enable the NullDeref Cling test for MethodCalls.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/111
https://github.com/root-project/root/pull/112:29,integrability,inject,injection,29,NullDeref check changed from injection of if(stmt) to a runtime call.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/112
https://github.com/root-project/root/pull/112:29,security,inject,injection,29,NullDeref check changed from injection of if(stmt) to a runtime call.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/112
https://github.com/root-project/root/pull/113:22,interoperability,specif,specify,22,Allow ROOT_INCLUDE to specify standard library include paths; This way we do not need g++ at runtime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/113
https://github.com/root-project/root/pull/113:30,interoperability,standard,standard,30,Allow ROOT_INCLUDE to specify standard library include paths; This way we do not need g++ at runtime.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/113
https://github.com/root-project/root/pull/115:512,availability,slo,slower,512,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1084,availability,slo,slower,1084,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:4,deployability,stack,stack,4,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:87,deployability,updat,updated,87,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:165,deployability,updat,updated,165,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:190,deployability,stack,stack,190,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:223,deployability,stack,stack,223,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:446,deployability,stack,stack,446,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:857,deployability,log,logs,857,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1069,deployability,version,version,1069,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:972,energy efficiency,measur,measurements,972,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1024,energy efficiency,measur,measurements,1024,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1069,integrability,version,version,1069,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1069,modifiability,version,version,1069,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:78,performance,cach,cache,78,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:125,performance,cach,cache,125,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:361,performance,cach,cache,361,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:599,performance,time,time,599,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:967,performance,time,time,967,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1019,performance,time,time,1019,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:512,reliability,slo,slower,512,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:1084,reliability,slo,slower,1084,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:87,safety,updat,updated,87,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:165,safety,updat,updated,165,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:653,safety,test,tested,653,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:783,safety,test,test,783,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:857,safety,log,logs,857,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:87,security,updat,updated,87,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:165,security,updat,updated,165,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:857,security,log,logs,857,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:653,testability,test,tested,653,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:783,testability,test,test,783,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/115:857,testability,log,logs,857,"Fix stack corruption in RooVectorDataStore; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56528/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56529/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/115
https://github.com/root-project/root/pull/116:524,availability,slo,slower,524,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1096,availability,slo,slower,1096,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:15,deployability,Stack,Stack,15,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:99,deployability,updat,updated,99,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:177,deployability,updat,updated,177,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:202,deployability,stack,stack,202,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:235,deployability,stack,stack,235,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:458,deployability,stack,stack,458,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:869,deployability,log,logs,869,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1081,deployability,version,version,1081,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:984,energy efficiency,measur,measurements,984,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1036,energy efficiency,measur,measurements,1036,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1081,integrability,version,version,1081,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1081,modifiability,version,version,1081,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:90,performance,cach,cache,90,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:137,performance,cach,cache,137,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:373,performance,cach,cache,373,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:611,performance,time,time,611,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:979,performance,time,time,979,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1031,performance,time,time,1031,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:524,reliability,slo,slower,524,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:1096,reliability,slo,slower,1096,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:99,safety,updat,updated,99,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:177,safety,updat,updated,177,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:665,safety,test,tested,665,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:795,safety,test,test,795,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:869,safety,log,logs,869,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:99,security,updat,updated,99,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:177,security,updat,updated,177,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:869,security,log,logs,869,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:665,testability,test,tested,665,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:795,testability,test,test,795,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/116:869,testability,log,logs,869,"Fix ROOT-7121: Stack corruption in RooVectorDataStore ; This is a fix for ROOT-7121. If a cache is updated in RooVectorDataStore and the cache has more than 1000 elements to be updated, an array on the stack will overrun and smash the stack. roofit will therefore crash. Solution: RooVectorDataStore uses a std::vector instead of an array[1000] to hold the pointers to the cache elements. Comments on the speed of the fix:. Using a std::vector placed on the stack (mimicking the original implementation), the fits would get slower. Therefore I added the vector as a member of RooVectorDataStore. This saves the time of constantly having to reallocate the vector. I tested with my (private) workspace: The crash is fixed. Unfortunately, I cannot provide this workspace. To give a more meaningful test for you guys, I ran all the roofit/roostats tutorials and diffed the logs to check if roofit gives the same results. The diffs are attached. Apart from out-of-order execution and time measurements, there is no difference. From the time measurements you can also see that the fixed version is not slower. [tutorials_roofit.diff.txt](https://github.com/root-mirror/root/files/56546/tutorials_roofit.diff.txt). [tutorials_roostats.diff.txt](https://github.com/root-mirror/root/files/56547/tutorials_roostats.diff.txt).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/116
https://github.com/root-project/root/pull/117:8,security,access,access,8,Protect access to TROOT::GetListOfGlobalFunctions in TFormula; Protected all calls to TROOT::GetListOfGlobalFunctions from TFormula. with the proper mutex.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/117
https://github.com/root-project/root/pull/118:34,safety,safe,safe,34,Make TListOfFunctions::Get thread-safe; Need to make the Find and possible later call to Add be in one. critical section.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/118
https://github.com/root-project/root/pull/119:122,deployability,updat,updated,122,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:133,modifiability,Variab,Variable,133,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:122,safety,updat,updated,122,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:169,safety,test,tested,169,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:122,security,updat,updated,122,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:169,testability,test,tested,169,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/119:222,usability,support,support,222,"Master tmva dataloader vi; Hello,. This is the Implementation of DataLoader for TMVA according to new design. - Tutorials updated. - Variable Importance implemented and tested. - New ROC algorithm implemented. - PyMVA now support python 3. Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/119
https://github.com/root-project/root/pull/120:45,deployability,build,build,45,"added explicit check for OpenGL GLU in cmake build; If only the OpenGL headers are installed, but not GLU, the cmake check for OpenGL/GLU succeeds, but compile will fail at some point, because the GLU headers are needed. This pull request fixes this inconsistency by explicitly checking also for GLU in the cmake script.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/120
https://github.com/root-project/root/pull/120:83,deployability,instal,installed,83,"added explicit check for OpenGL GLU in cmake build; If only the OpenGL headers are installed, but not GLU, the cmake check for OpenGL/GLU succeeds, but compile will fail at some point, because the GLU headers are needed. This pull request fixes this inconsistency by explicitly checking also for GLU in the cmake script.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/120
https://github.com/root-project/root/pull/120:165,deployability,fail,fail,165,"added explicit check for OpenGL GLU in cmake build; If only the OpenGL headers are installed, but not GLU, the cmake check for OpenGL/GLU succeeds, but compile will fail at some point, because the GLU headers are needed. This pull request fixes this inconsistency by explicitly checking also for GLU in the cmake script.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/120
https://github.com/root-project/root/pull/120:165,reliability,fail,fail,165,"added explicit check for OpenGL GLU in cmake build; If only the OpenGL headers are installed, but not GLU, the cmake check for OpenGL/GLU succeeds, but compile will fail at some point, because the GLU headers are needed. This pull request fixes this inconsistency by explicitly checking also for GLU in the cmake script.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/120
https://github.com/root-project/root/pull/121:373,energy efficiency,cpu,cpu,373,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:527,energy efficiency,optim,optimising,527,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:171,performance,network,network,171,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:346,performance,network,network,346,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:373,performance,cpu,cpu,373,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:143,safety,test,test,143,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:492,safety,avoid,avoided,492,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:171,security,network,network,171,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:346,security,network,network,346,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/121:143,testability,test,test,143,"speedup for .class.C for MLP; I ported the changes from [pseyfert/tmva-mlp](https://github.com/pseyfert/tmva-mlp) to the code generation. As a test I just ran the class.C network resulting from tutorials/tmva/TMVAClassification.C (with ""MLP"") and evaluated it similar to tutorials/tmva/TMVAClassificationApplication.C. According to callgrind the network evaluation is ~17% cpu cycles faster. NB: i did not port all changes from pseyfert/tmva-mlp - I did not import the SSE/AVX intrinsics and avoided what seemed too difficult (optimising the putIndices and getIndices out) .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/121
https://github.com/root-project/root/pull/123:102,deployability,build,builds,102,optimise includes in math; cleaning up a few includes in math/mathmore and math/physics. NB:. 1. root builds for me with these changes but i didn't check for platform independence (when it comes to system headers). 2. removing includes from headers may break 3rd party code including those headers i manipulated.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/123
https://github.com/root-project/root/pull/123:0,energy efficiency,optim,optimise,0,optimise includes in math; cleaning up a few includes in math/mathmore and math/physics. NB:. 1. root builds for me with these changes but i didn't check for platform independence (when it comes to system headers). 2. removing includes from headers may break 3rd party code including those headers i manipulated.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/123
https://github.com/root-project/root/pull/123:158,interoperability,platform,platform,158,optimise includes in math; cleaning up a few includes in math/mathmore and math/physics. NB:. 1. root builds for me with these changes but i didn't check for platform independence (when it comes to system headers). 2. removing includes from headers may break 3rd party code including those headers i manipulated.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/123
https://github.com/root-project/root/pull/124:48,deployability,patch,patch,48,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:401,deployability,patch,patch,401,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:26,energy efficiency,adapt,adapted,26,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:308,energy efficiency,Adapt,Adapted,308,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:26,integrability,adapt,adapted,26,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:308,integrability,Adapt,Adapted,308,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:26,interoperability,adapt,adapted,26,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:308,interoperability,Adapt,Adapted,308,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:26,modifiability,adapt,adapted,26,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:308,modifiability,Adapt,Adapted,308,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:48,safety,patch,patch,48,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:401,safety,patch,patch,401,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:48,security,patch,patch,48,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:401,security,patch,patch,401,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:4,usability,support,support,4,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/124:393,usability,support,support,393,Add support for abi-tags (adapted from Fedora's patch); https://sft.its.cern.ch/jira/browse/ROOT-7818. https://sft.its.cern.ch/jira/browse/ROOT-7721. https://sft.its.cern.ch/jira/browse/ROOT-7654. https://sft.its.cern.ch/jira/browse/ROOT-7319. https://sft.its.cern.ch/jira/browse/ROOT-7285. Possibly more... Adapted from:. http://pkgs.fedoraproject.org/cgit/llvm.git/tree/0001-add-gcc-abi_tag-support.patch.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/124
https://github.com/root-project/root/pull/125:227,energy efficiency,optim,optimisation,227,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:324,energy efficiency,optim,optimisation,324,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:373,energy efficiency,optim,optimised,373,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:417,energy efficiency,optim,optimised,417,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:472,energy efficiency,current,currently,472,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:686,energy efficiency,optim,optimised,686,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:344,interoperability,specif,specify,344,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:217,modifiability,Paramet,Parameter,217,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:251,modifiability,paramet,parameters,251,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:314,modifiability,paramet,parameter,314,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:356,modifiability,paramet,parameters,356,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:512,modifiability,paramet,parameter,512,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:696,modifiability,paramet,parameters,696,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:445,security,loss,loss,445,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:538,security,sign,signal,538,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:625,security,sign,significantly,625,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/125:29,usability,Support,Support,29,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/125
https://github.com/root-project/root/pull/126:33,energy efficiency,CPU,CPUID,33,"Vc: ifdef x86-specific code; The CPUID code is x86-specific, so compile it only for x86 targets. Additionally, the Impl query must return false unconditionally for any SIMD. instruction set for non-x86. Signed-off-by: Matthias Kretz kretz@kde.org.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/126
https://github.com/root-project/root/pull/126:14,interoperability,specif,specific,14,"Vc: ifdef x86-specific code; The CPUID code is x86-specific, so compile it only for x86 targets. Additionally, the Impl query must return false unconditionally for any SIMD. instruction set for non-x86. Signed-off-by: Matthias Kretz kretz@kde.org.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/126
https://github.com/root-project/root/pull/126:51,interoperability,specif,specific,51,"Vc: ifdef x86-specific code; The CPUID code is x86-specific, so compile it only for x86 targets. Additionally, the Impl query must return false unconditionally for any SIMD. instruction set for non-x86. Signed-off-by: Matthias Kretz kretz@kde.org.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/126
https://github.com/root-project/root/pull/126:33,performance,CPU,CPUID,33,"Vc: ifdef x86-specific code; The CPUID code is x86-specific, so compile it only for x86 targets. Additionally, the Impl query must return false unconditionally for any SIMD. instruction set for non-x86. Signed-off-by: Matthias Kretz kretz@kde.org.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/126
https://github.com/root-project/root/pull/126:203,security,Sign,Signed-off-by,203,"Vc: ifdef x86-specific code; The CPUID code is x86-specific, so compile it only for x86 targets. Additionally, the Impl query must return false unconditionally for any SIMD. instruction set for non-x86. Signed-off-by: Matthias Kretz kretz@kde.org.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/126
https://github.com/root-project/root/pull/128:202,availability,failur,failure,202,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:202,deployability,fail,failure,202,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:217,deployability,patch,patch,217,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:202,performance,failur,failure,202,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:202,reliability,fail,failure,202,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:217,safety,patch,patch,217,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:286,safety,prevent,prevent,286,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:217,security,patch,patch,217,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:286,security,preven,prevent,286,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:26,usability,user,user,26,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/128:57,usability,user,user,57,"Unset Fortran compiler if user disables fortran.; If the user disables fortran but a fortran compiler is actually. present, then hist/CMakeLists.txt will still try to compile. hbook (which results in a failure). This patch explicitly sets the fortran compiler to not found. in order to prevent this from occurring.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/128
https://github.com/root-project/root/pull/129:26,deployability,Updat,Updated,26,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:15,safety,test,testsuite,15,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:26,safety,Updat,Updated,26,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:43,safety,test,test,43,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:58,safety,test,tests,58,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:26,security,Updat,Updated,26,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:15,testability,test,testsuite,15,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:43,testability,test,test,43,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:48,testability,unit,unit,48,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/129:58,testability,test,tests,58,"TMVA: fixes in testsuite; Updated code the test unit, all tests for TMVA are OK! Cheers,. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/129
https://github.com/root-project/root/pull/130:35,integrability,configur,configuring,35,Make it possible to run tests when configuring with -Dgnuinstall:BOOL=ON; This is a possible solution for ROOT-7538. I hope this is at least similar to what you had in mind when you wrote the comment in that ticket. Hopefully it can at least be used as a start.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/130
https://github.com/root-project/root/pull/130:35,modifiability,configur,configuring,35,Make it possible to run tests when configuring with -Dgnuinstall:BOOL=ON; This is a possible solution for ROOT-7538. I hope this is at least similar to what you had in mind when you wrote the comment in that ticket. Hopefully it can at least be used as a start.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/130
https://github.com/root-project/root/pull/130:24,safety,test,tests,24,Make it possible to run tests when configuring with -Dgnuinstall:BOOL=ON; This is a possible solution for ROOT-7538. I hope this is at least similar to what you had in mind when you wrote the comment in that ticket. Hopefully it can at least be used as a start.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/130
https://github.com/root-project/root/pull/130:35,security,configur,configuring,35,Make it possible to run tests when configuring with -Dgnuinstall:BOOL=ON; This is a possible solution for ROOT-7538. I hope this is at least similar to what you had in mind when you wrote the comment in that ticket. Hopefully it can at least be used as a start.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/130
https://github.com/root-project/root/pull/130:24,testability,test,tests,24,Make it possible to run tests when configuring with -Dgnuinstall:BOOL=ON; This is a possible solution for ROOT-7538. I hope this is at least similar to what you had in mind when you wrote the comment in that ticket. Hopefully it can at least be used as a start.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/130
https://github.com/root-project/root/pull/131:19,deployability,Updat,Updated,19,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/131:19,safety,Updat,Updated,19,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/131:19,security,Updat,Updated,19,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/131:36,usability,document,documentation,36,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/131:50,usability,user,user,50,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/131:55,usability,guid,guide,55,Master root r doc; Updated markdown documentation(user guide) for ROOT-R. Cheers. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/131
https://github.com/root-project/root/pull/132:0,deployability,Updat,Update,0,"Update `root` header to current year.; Small annoyance fix when root interpreter starts: we're in 2016, not 2014!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/132
https://github.com/root-project/root/pull/132:24,energy efficiency,current,current,24,"Update `root` header to current year.; Small annoyance fix when root interpreter starts: we're in 2016, not 2014!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/132
https://github.com/root-project/root/pull/132:0,safety,Updat,Update,0,"Update `root` header to current year.; Small annoyance fix when root interpreter starts: we're in 2016, not 2014!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/132
https://github.com/root-project/root/pull/132:0,security,Updat,Update,0,"Update `root` header to current year.; Small annoyance fix when root interpreter starts: we're in 2016, not 2014!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/132
https://github.com/root-project/root/pull/133:619,deployability,Stack,StackTrace,619,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:707,deployability,Stack,StackTrace,707,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:1025,deployability,patch,patch,1025,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:772,energy efficiency,current,current,772,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:0,modifiability,Refact,Refactor,0,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:0,performance,Refactor,Refactor,0,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:65,safety,safe,safe,65,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:238,safety,compl,comply,238,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:540,safety,unsaf,unsafe,540,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:588,safety,safe,safe,588,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:1010,safety,test,test,1010,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:1025,safety,patch,patch,1025,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:9,security,sign,signal,9,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:37,security,sign,signal,37,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:103,security,sign,signal,103,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:200,security,sign,signal,200,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:238,security,compl,comply,238,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:560,security,sign,signal,560,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:675,security,sign,signals,675,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:823,security,sign,signal,823,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:1025,security,patch,patch,1025,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/133:1010,testability,test,test,1010,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm . 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all such function calls and replace them with gSigHandling->(functions) if necessary. 2. I replace old unsafe functions in signal handlers with thread-safe ones. 3. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 4. @pcanal I have some problem with running roottest. I asked a question here:. https://github.com/root-mirror/root/pull/84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/133
https://github.com/root-project/root/pull/134:675,deployability,Stack,StackTrace,675,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:763,deployability,Stack,StackTrace,763,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:1041,deployability,patch,patch,1041,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:828,energy efficiency,current,current,828,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:96,integrability,sub,submitting,96,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:0,modifiability,Refact,Refactor,0,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:0,performance,Refactor,Refactor,0,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:65,safety,safe,safe,65,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:293,safety,compl,comply,293,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:596,safety,unsaf,unsafe,596,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:644,safety,safe,safe,644,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:1026,safety,test,test,1026,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:1041,safety,patch,patch,1041,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:9,security,sign,signal,9,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:37,security,sign,signal,37,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:130,security,sign,signal,130,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:158,security,sign,signal,158,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:255,security,sign,signal,255,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:293,security,compl,comply,293,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:616,security,sign,signal,616,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:731,security,sign,signals,731,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:879,security,sign,signal,879,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:1041,security,patch,patch,1041,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/134:1026,testability,test,test,1026,"Refactor signal handling and replace signal handlers with thread-safe functions; @bbockelm I am submitting a new pull request for signal handling:. 1. I move signal handling from TSystem to a new class called TSigHandling and use gSigHandling as a global signal handling object in ROOT. 2. To comply with some function calls like ""gSystem->ResetSignals()"". I move old ResetSignals() to the new class so TUnixSystem::ResetSignals() just call gSigHandling->ResetSignals(). I could also go over all gSystem->(functions) and replace them with gSigHandling->(functions) if necessary. 3. I replace old unsafe functions in signal handlers with thread-safe ones. 4. I only implement StackTrace functions for SIGBUS, SIGSEGV, SIGILL. Other signals are still using default StackTrace functions. kSigAlarm and kSigChild are ignored for my current implementation. Do we need to change other signal handlers? 5. @pcanal I have some problem with running roottest. I asked a question here:. #84. Could you take a look at it and I will write test case this patch also.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/134
https://github.com/root-project/root/pull/135:110,safety,review,review,110,Do not allow short reads.; Proposed fix for https://sft.its.cern.ch/jira/browse/ROOT-3341. @smithdh - can you review?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/135
https://github.com/root-project/root/pull/135:110,testability,review,review,110,Do not allow short reads.; Proposed fix for https://sft.its.cern.ch/jira/browse/ROOT-3341. @smithdh - can you review?,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/135
https://github.com/root-project/root/pull/136:394,deployability,updat,update,394,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:572,integrability,buffer,buffer,572,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:1387,integrability,buffer,buffer,1387,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:45,interoperability,format,format,45,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:552,reliability,doe,does,552,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:394,safety,updat,update,394,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:394,security,updat,update,394,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/136:1295,security,modif,modifying,1295,"ROOT-5073: Explore changing the on-file byte format to little endian; Hi, @pcanal @bbockelm . This branch implements little endian in TBuffer. The code is not ready to be merged and I hope it would be more convenient to discuss it on github. There is still a design issue. This is the link: https://sft.its.cern.ch/jira/browse/ROOT-5073. Let's take an example of writing to a TFile, we need to update header (TFile::WriteHeader), streamer info (TFile::WriteStreamerInfo) and free segments (TFile::WriteFree). . 1. TFile::WriteHeader creates a TKey but does not stream its buffer. When you read or write header, it is always stored as big endian. 2. TFile::WriteFree works in the same way with TFile::WriteHeader. 3. TFile::WriteStreamerInfo is quite different from above two cases. It creates a TKey but uses streaming function to change streamer info object TList to little endian. The problem is that all of three information are read by TFile::ReadBuffer or TFile::ReadKeyBuffer without converting the endianesss. header and free segments can be processed without any problems. But streamer info is read in reversed endianess. . To address this issue, one way is adding a fBit in TFile class and change all meta data (header, free segments and streamer info) to little endian. Another way is modifying the read function for streamer info and convert its endianess before read it from buffer. Zhe.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/136
https://github.com/root-project/root/pull/137:111,integrability,sub,submitted,111,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/137:90,security,modif,modifications,90,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/137:281,security,modif,modify,281,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/137:369,security,modif,modify,369,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/137:452,security,modif,modify,452,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/137:148,usability,Support,Support,148,"Adding new options for roofit: SetMaxIter(), SetMaxCalls() and SetEpsilon(); Here are the modifications I have submitted in the ""Stat and Math ROOT Support"" forum in order to add new options in RooAbsPdf::FitTo() options. The three options are:. -SetMaxIter(Int_t max) in order to modify the maximum number of iterations for the fit. -SetMaxCall(Int_t max) in order to modify the maximum number of calls for the fit. -SetEpsilon(Int_t eps) in order to modify the epsilon value of the fit. Best regards,. Quentin Fable, PhD student at GANIL.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/137
https://github.com/root-project/root/pull/138:227,energy efficiency,optim,optimisation,227,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:324,energy efficiency,optim,optimisation,324,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:373,energy efficiency,optim,optimised,373,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:417,energy efficiency,optim,optimised,417,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:472,energy efficiency,current,currently,472,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:686,energy efficiency,optim,optimised,686,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:344,interoperability,specif,specify,344,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:217,modifiability,Paramet,Parameter,217,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:251,modifiability,paramet,parameters,251,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:314,modifiability,paramet,parameter,314,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:356,modifiability,paramet,parameters,356,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:512,modifiability,paramet,parameter,512,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:696,modifiability,paramet,parameters,696,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:445,security,loss,loss,445,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:538,security,sign,signal,538,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:625,security,sign,significantly,625,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/138:29,usability,Support,Support,29,"Additional functionality for Support Vector Machines in TMVA; Additional functionality for SVMs includes:. ~ Multi-gaussian, product and sum kernel functions added, as well as polynomial kernel function re-enabled. ~ Parameter optimisation for kernel parameters and cost added following the implementation for BDT parameter optimisation. ~ Can specify the parameters to be optimised and the range over which they are optimised. ~ Calculation of loss functions, though not currently used. ~ Weighting of the cost parameter to the relative signal and background dataset sizes, as to not bias the SVM training if one dataset is significantly larger than the other. ~ Also return of map of optimised parameters from the tmva factory to allow for use in external program.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/138
https://github.com/root-project/root/pull/139:128,availability,error,error,128,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:136,deployability,stack,stack,136,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:195,deployability,modul,module,195,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:202,deployability,resourc,resource,202,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:240,deployability,stack,stack,240,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:289,deployability,stack,stack,289,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:202,energy efficiency,resourc,resource,202,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:110,integrability,messag,message,110,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:110,interoperability,messag,message,110,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:195,modifiability,modul,module,195,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:128,performance,error,error,128,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:202,performance,resourc,resource,202,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:128,safety,error,error,128,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:195,safety,modul,module,195,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:202,safety,resourc,resource,202,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:202,testability,resourc,resource,202,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:23,usability,support,support,23,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:67,usability,support,supported,67,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:128,usability,error,error,128,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/139:155,usability,close,close,155,"JupyROOT: fixed ROOT-R support for ROOTBooks; Hello! ROOT-R is now supported with JupyROOT, the problem was a message from . R ""error c stack usage is too close to the limit"". It was fixed using module resource from python to set unlimited stack size in multithread execution,. anyway the stack in OS will be the limit. ""see ulimit -s"" for Gnu/Linux. Best.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/139
https://github.com/root-project/root/pull/141:18,safety,permiss,permissons,18,Remove executable permissons;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/141
https://github.com/root-project/root/pull/142:18,safety,permiss,permissons,18,Remove executable permissons;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/142
https://github.com/root-project/root/pull/143:128,deployability,updat,update,128,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:230,deployability,updat,updated,230,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:366,deployability,continu,continue,366,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:386,deployability,updat,updated,386,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:66,integrability,interfac,interfaces,66,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:66,interoperability,interfac,interfaces,66,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:154,interoperability,format,format,154,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:66,modifiability,interfac,interfaces,66,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:128,safety,updat,update,128,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:230,safety,updat,updated,230,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:386,safety,updat,updated,386,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:128,security,updat,update,128,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:230,security,updat,updated,230,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:386,security,updat,updated,386,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:138,usability,support,support,138,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/143:252,usability,visual,visualization,252,"Master tmvagui; Hi Guys,. According to the new design of TMVA the interfaces TMVAGui, TMVAMultiClassGui and TMVARegGui needs an update to support the new format that is stored the results. In this pull requests I have the code to updated TMVAGui to do visualization of two class classification. lets see http://oproject.org/tiki-index.php?page=TMVA#TMVAGuis. I will continue working to updated TMVAMultiClassGui and TMVARegGui. NOTE: all requirements accorded in the meeting was implemented. Best Regards. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/143
https://github.com/root-project/root/pull/144:48,deployability,patch,patch,48,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:48,safety,patch,patch,48,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:366,safety,avoid,avoided,366,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:48,security,patch,patch,48,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:129,usability,user,user,129,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:281,usability,user,users,281,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/144:293,usability,prefer,prefer,293,"disable GREP_OPTIONS, this fixes ROOT-8056; The patch which I suggested on https://sft.its.cern.ch/jira/browse/ROOT-8056. If the user uses GREP_OPTIONS which add characters to the output of grep (colors, but also filenames or line numbers), root cannot be started anymore. Ideally users would prefer an alias for `grep` over `GREP_OPTIONS` but the crash can also be avoided on the root side.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/144
https://github.com/root-project/root/pull/145:21,usability,learn,learning,21,Master deepnn ; Deep learning code from Peter. with some fixes. Best. Omar.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/145
https://github.com/root-project/root/pull/146:742,availability,down,download,742,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:231,deployability,log,logic,231,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:315,deployability,automat,automatically,315,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:9,integrability,buffer,buffering,9,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:88,integrability,buffer,buffering,88,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:75,modifiability,interm,intermediate,75,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:178,performance,cach,cache,178,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:211,performance,disk,disk,211,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:412,performance,network,network,412,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:420,performance,latenc,latency,420,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:632,performance,cach,cache,632,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:648,performance,cach,cache-unfriendly,648,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:231,safety,log,logic,231,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:231,security,log,logic,231,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:412,security,network,network,412,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:231,testability,log,logic,231,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:315,testability,automat,automatically,315,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:420,usability,latenc,latency,420,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/146:665,usability,workflow,workflow,665,"Add file buffering capability to TFileCacheRead; This pull request adds an intermediate buffering mode between ""normal ROOT IO"" and the prefetching system. When enabled, it will cache a remote file to the local disk (uses the same logic as prefetching to determine what is ""remote"") for as long as it is opened and automatically cleans up afterward. This is useful in cases where you want to hide the effects of network latency (for various use cases which work poorly with `TTreeCache`, such as when an unpredictable set of branches are used or non-sequential scans) but do not want to set aside a directory to use as a persistent cache or have a cache-unfriendly workflow. The approach has been ported from CMSSW (there, it is called `lazy-download`) where it has been in use for several years.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/146
https://github.com/root-project/root/pull/148:136,modifiability,Refact,Refactored,136,Implementation of the new TPool and ThreadPool classes; Added the new ThreadPool class sharing TProcPool's MapReduce methods signature. Refactored the code so ThreadPool and TProcPool inherit from the new parent TPool.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/148
https://github.com/root-project/root/pull/148:184,modifiability,inherit,inherit,184,Implementation of the new TPool and ThreadPool classes; Added the new ThreadPool class sharing TProcPool's MapReduce methods signature. Refactored the code so ThreadPool and TProcPool inherit from the new parent TPool.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/148
https://github.com/root-project/root/pull/148:136,performance,Refactor,Refactored,136,Implementation of the new TPool and ThreadPool classes; Added the new ThreadPool class sharing TProcPool's MapReduce methods signature. Refactored the code so ThreadPool and TProcPool inherit from the new parent TPool.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/148
https://github.com/root-project/root/pull/148:125,security,sign,signature,125,Implementation of the new TPool and ThreadPool classes; Added the new ThreadPool class sharing TProcPool's MapReduce methods signature. Refactored the code so ThreadPool and TProcPool inherit from the new parent TPool.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/148
https://github.com/root-project/root/pull/149:142,deployability,releas,releases,142,"adding in ability for rooteventselector to skim trees based on a selection string; Hi all,. I love the new command line utils included in the releases in root/main/python/. However, I found the rooteventselector command to be lacking in some functionality, so I added the ability for it to skim based on a selection string (a la a TCut). I hope this is useful for others! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/149
https://github.com/root-project/root/pull/149:107,usability,command,command,107,"adding in ability for rooteventselector to skim trees based on a selection string; Hi all,. I love the new command line utils included in the releases in root/main/python/. However, I found the rooteventselector command to be lacking in some functionality, so I added the ability for it to skim based on a selection string (a la a TCut). I hope this is useful for others! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/149
https://github.com/root-project/root/pull/149:212,usability,command,command,212,"adding in ability for rooteventselector to skim trees based on a selection string; Hi all,. I love the new command line utils included in the releases in root/main/python/. However, I found the rooteventselector command to be lacking in some functionality, so I added the ability for it to skim based on a selection string (a la a TCut). I hope this is useful for others! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/149
https://github.com/root-project/root/pull/150:315,deployability,updat,updated,315,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:402,integrability,event,events,402,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:169,safety,input,input,169,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:315,safety,updat,updated,315,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:96,security,command-lin,command-line,96,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:315,security,updat,updated,315,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:7,usability,command,command,7,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:96,usability,command,command-line,96,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:109,usability,tool,tool,109,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:169,usability,input,input,169,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/150:363,usability,user,user,363,"Adding command line rootslimtree for removing branches from a tree. ; Hi all,. I've added a new command-line tool called rootslimtree which will remove branches from an input tree and write out a new file. This is actually implemented in cmdLineUtils.rootEventselector() and the rooteventselector function has been updated to also have this ability, so in case a user wants to remove both branches and events, this can be done in a single step. . Related to #149. Thanks! -Larry.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/150
https://github.com/root-project/root/pull/151:131,deployability,Updat,Update,131,"Small clean-up in TSpectrum*; 1. Remove unused `SetResolution` function and `fResolution` member from the `TSpectrum*` classes. 2. Update and add references to external documentation, such as the `*.ps.gz` papers and the ""manual"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/151
https://github.com/root-project/root/pull/151:131,safety,Updat,Update,131,"Small clean-up in TSpectrum*; 1. Remove unused `SetResolution` function and `fResolution` member from the `TSpectrum*` classes. 2. Update and add references to external documentation, such as the `*.ps.gz` papers and the ""manual"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/151
https://github.com/root-project/root/pull/151:131,security,Updat,Update,131,"Small clean-up in TSpectrum*; 1. Remove unused `SetResolution` function and `fResolution` member from the `TSpectrum*` classes. 2. Update and add references to external documentation, such as the `*.ps.gz` papers and the ""manual"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/151
https://github.com/root-project/root/pull/151:169,usability,document,documentation,169,"Small clean-up in TSpectrum*; 1. Remove unused `SetResolution` function and `fResolution` member from the `TSpectrum*` classes. 2. Update and add references to external documentation, such as the `*.ps.gz` papers and the ""manual"".",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/151
https://github.com/root-project/root/pull/152:26,security,access,access,26,"ROOT-5076: Support random access compression; This is a new feature to support random access compression. For details, please take a look at this link:. https://sft.its.cern.ch/jira/browse/ROOT-5076.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/152
https://github.com/root-project/root/pull/152:86,security,access,access,86,"ROOT-5076: Support random access compression; This is a new feature to support random access compression. For details, please take a look at this link:. https://sft.its.cern.ch/jira/browse/ROOT-5076.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/152
https://github.com/root-project/root/pull/152:11,usability,Support,Support,11,"ROOT-5076: Support random access compression; This is a new feature to support random access compression. For details, please take a look at this link:. https://sft.its.cern.ch/jira/browse/ROOT-5076.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/152
https://github.com/root-project/root/pull/152:71,usability,support,support,71,"ROOT-5076: Support random access compression; This is a new feature to support random access compression. For details, please take a look at this link:. https://sft.its.cern.ch/jira/browse/ROOT-5076.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/152
https://github.com/root-project/root/pull/153:0,deployability,Configurat,Configuration,0,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:66,deployability,build,build,66,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:122,deployability,build,builds,122,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:359,deployability,build,build,359,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:422,deployability,build,build,422,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:491,deployability,build,build,491,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:545,deployability,build,build,545,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,deployability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:733,deployability,build,build,733,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:836,deployability,build,build,836,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:1079,deployability,contain,contains,1079,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:590,energy efficiency,predict,predictive,590,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:0,integrability,Configur,Configuration,0,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:52,integrability,configur,configure,52,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:90,integrability,configur,configure,90,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:213,integrability,configur,configure,213,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:286,integrability,configur,configure,286,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:664,integrability,configur,configure,664,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,integrability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:794,integrability,configur,configure,794,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:1047,integrability,configur,configure,1047,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:559,interoperability,standard,standard,559,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,interoperability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:0,modifiability,Configur,Configuration,0,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:52,modifiability,configur,configure,52,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:90,modifiability,configur,configure,90,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:213,modifiability,configur,configure,213,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:286,modifiability,configur,configure,286,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:664,modifiability,configur,configure,664,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,modifiability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:794,modifiability,configur,configure,794,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:1047,modifiability,configur,configure,1047,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,reliability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:252,safety,compl,complete,252,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:509,safety,compl,complete,509,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:590,safety,predict,predictive,590,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:715,safety,test,test,715,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:768,safety,test,test,768,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:0,security,Configur,Configuration,0,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:52,security,configur,configure,52,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:90,security,configur,configure,90,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:213,security,configur,configure,213,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:252,security,compl,complete,252,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:286,security,configur,configure,286,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:509,security,compl,complete,509,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:664,security,configur,configure,664,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,security,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:794,security,configur,configure,794,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:1047,security,configur,configure,1047,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:614,testability,understand,understands,614,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:700,testability,integr,integrates,700,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:715,testability,test,test,715,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/153:768,testability,test,test,768,"Configuration fixes; Root has two different ways to configure the build - the traditional configure script and cmake. The builds generated by the two systems are similar, but far from equivalent. Historically the configure script has been more feature complete and some things that the configure script is able to do are either missing or broken in the cmake build. However, new features are often only added to the cmake build. This has resulted in that today neither of the two is able to build root with a complete set of features. The cmake build is more standard and behaves in a more predictive way, e.g. it understands CFLAGS, LDFLAGS and friends which the configure script never did. It also integrates the test suite in the build and allows for running ""make test"", a feature that the configure script is missing. So the cmake build is in many ways better, if it wasn't for those missing and broken things mentioned earlier. This pull request is an attempt to address those missing and broken issues (though it fixes a few things for the configure script as well). Also contains a fix for ROOT-7326.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/153
https://github.com/root-project/root/pull/154:0,usability,Document,Documentation,0,Documentation fixes; Here are some fixes to the doxygen documentation generation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/154
https://github.com/root-project/root/pull/154:56,usability,document,documentation,56,Documentation fixes; Here are some fixes to the doxygen documentation generation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/154
https://github.com/root-project/root/pull/155:0,safety,Test,Test,0,Test fixes; Here are some fixes to the tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/155
https://github.com/root-project/root/pull/155:39,safety,test,tests,39,Test fixes; Here are some fixes to the tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/155
https://github.com/root-project/root/pull/155:0,testability,Test,Test,0,Test fixes; Here are some fixes to the tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/155
https://github.com/root-project/root/pull/155:39,testability,test,tests,39,Test fixes; Here are some fixes to the tests.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/155
https://github.com/root-project/root/pull/156:183,deployability,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:196,deployability,contain,contains,196,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:220,integrability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:349,integrability,bridg,bridging,349,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:41,interoperability,convers,conversion,41,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:209,interoperability,convers,conversion,209,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:220,interoperability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:285,interoperability,plug,plugin,285,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:349,interoperability,bridg,bridging,349,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:470,interoperability,convers,conversion,470,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:665,interoperability,standard,standard,665,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:114,modifiability,pac,package,114,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:183,modifiability,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:220,modifiability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:520,performance,memor,memory,520,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:183,safety,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:634,safety,test,tests,634,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:647,security,access,accessed,647,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:634,testability,test,tests,634,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:21,usability,support,support,21,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:70,usability,support,support,70,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:401,usability,navigat,navigation,401,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:437,usability,visual,visualisation,437,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/156:520,usability,memor,memory,520,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/156
https://github.com/root-project/root/pull/157:183,deployability,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:196,deployability,contain,contains,196,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:220,integrability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:349,integrability,bridg,bridging,349,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:41,interoperability,convers,conversion,41,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:209,interoperability,convers,conversion,209,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:220,interoperability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:285,interoperability,plug,plugin,285,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:349,interoperability,bridg,bridging,349,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:470,interoperability,convers,conversion,470,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:665,interoperability,standard,standard,665,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:114,modifiability,pac,package,114,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:183,modifiability,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:220,modifiability,interfac,interface,220,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:520,performance,memor,memory,520,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:183,safety,modul,module,183,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:634,safety,test,tests,634,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:647,security,access,accessed,647,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:634,testability,test,tests,634,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:21,usability,support,support,21,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:70,usability,support,support,70,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:401,usability,navigat,navigation,401,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:437,usability,visual,visualisation,437,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/157:520,usability,memor,memory,520,"New feature: VecGeom support in ROOT and conversion of shapes.; Added support for the VecGeom library as external package (-Dvecgeom). Creating new library libConverterVG within geom module. This contains the conversion interface for ROOT shapes into vecgeom ones, activated using the plugin mechanism from ROOT. Included also the class TGeoVGShape bridging TGeoShape methods to either VecGeom solid (navigation) or existing ROOT shape (visualisation and the rest). The conversion can be done once a ROOT geometry is in memory using:. TVirtualGeoConverter::Instance()->ConvertGeometry(). Once this is done, all TGeo functionality and tests can be accessed as for a standard TGeo geometry. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/157
https://github.com/root-project/root/pull/158:246,availability,state,stated,246,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/158:246,integrability,state,stated,246,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/158:31,modifiability,paramet,parameter,31,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/158:114,modifiability,paramet,parameters,114,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/158:262,modifiability,paramet,parameters,262,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/158:171,usability,user,user,171,"RooStats::SPlot tutorial shape parameter constant-setting; RooStats::SPlot already takes care about setting shape parameters constant. It is not necessary to do this as a user. This should be demonstrated in the tutorial, especially it should be stated that the parameters are set constant already.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/158
https://github.com/root-project/root/pull/159:59,availability,sli,slightly,59,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:603,availability,failur,failure,603,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:732,availability,down,downstream-patching,732,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:194,deployability,contain,container,194,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:265,deployability,instal,installation,265,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:395,deployability,version,version,395,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:603,deployability,fail,failure,603,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:743,deployability,patch,patching,743,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:395,integrability,version,version,395,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:798,integrability,configur,configure,798,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:249,interoperability,standard,standard,249,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:480,interoperability,compatib,compatibility,480,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:395,modifiability,version,version,395,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:798,modifiability,configur,configure,798,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:603,performance,failur,failure,603,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:59,reliability,sli,slightly,59,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:603,reliability,fail,failure,603,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:720,safety,prevent,prevent,720,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:743,safety,patch,patching,743,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:720,security,preven,prevent,720,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:743,security,patch,patching,743,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:798,security,configur,configure,798,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:150,usability,prefer,preferred,150,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/159:370,usability,user,user-selected,370,"FindPostgreSQL.cmake: Add PATH_SUFFIX for searching.; This slightly changes lookup order: Namely, the system-folder ""/usr/include/postgresql"". is now preferred over ""/usr/include/"" even if both container libpq-fe.h. This finds the correct path on a standard Gentoo installation, where ""/usr/include/postgresql"". is a symlink to the folder with all include-files for the user-selected postgresql-version,. while ""/usr/include/libpq-fe.h"" is a single symlink provided for backwards-compatibility. Since ROOT uses also e.g. ""pg_config.h"", selecting ""/usr/include/"" over ""/usr/include/postgresql"". leads to failure on Gentoo. I hope this will not break any other existing setups but I don't see how it should. . That should prevent any downstream-patching which Gentoo is doing right now for old-style configure once it has moved to cmake. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/159
https://github.com/root-project/root/pull/160:274,deployability,fail,fail,274,"TText: Fix missing initialization in copy constructor and TText::Copy.; TText::Copy was broken in several ways:. - wchar-title was not copied to the given object, but from it. - nullptr checks were missing. The combination of both issues lead to all cases of TText::Copy to fail. (no matter if the wchar-name was set or not). The corresponding JIRA ticket is:. https://sft.its.cern.ch/jira/browse/ROOT-8116.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/160
https://github.com/root-project/root/pull/160:274,reliability,fail,fail,274,"TText: Fix missing initialization in copy constructor and TText::Copy.; TText::Copy was broken in several ways:. - wchar-title was not copied to the given object, but from it. - nullptr checks were missing. The combination of both issues lead to all cases of TText::Copy to fail. (no matter if the wchar-name was set or not). The corresponding JIRA ticket is:. https://sft.its.cern.ch/jira/browse/ROOT-8116.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/160
https://github.com/root-project/root/pull/161:0,usability,Document,Documentation,0,Documentation fixes; Here are some fixes to the doxygen documentation generation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/161
https://github.com/root-project/root/pull/161:56,usability,document,documentation,56,Documentation fixes; Here are some fixes to the doxygen documentation generation.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/161
https://github.com/root-project/root/pull/163:31,security,sign,signatures,31,TDavixFile: Add support for v4 signatures and STS tokens;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/163
https://github.com/root-project/root/pull/163:50,security,token,tokens,50,TDavixFile: Add support for v4 signatures and STS tokens;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/163
https://github.com/root-project/root/pull/163:16,usability,support,support,16,TDavixFile: Add support for v4 signatures and STS tokens;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/163
https://github.com/root-project/root/pull/164:243,availability,Error,Error,243,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:272,availability,error,error,272,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:378,deployability,modul,module,378,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:279,integrability,messag,messages,279,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:279,interoperability,messag,messages,279,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:378,modifiability,modul,module,378,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:243,performance,Error,Error,243,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:272,performance,error,error,272,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:243,safety,Error,Error,243,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:272,safety,error,error,272,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:378,safety,modul,module,378,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:385,security,ident,identification,385,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:243,usability,Error,Error,243,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/164:272,usability,error,error,272,Delete two empty files and make cosmetic changes; Deleted two empty files that were just including the header and made. some cosmetic changes to root/multiproc and TPool derived classes:. - #include reordering. - standarize the use of TError::Error as the way of throwing error. messages instead of mixing it with std::cerr here and there. - added missing copyright notices and module identification lines.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/164
https://github.com/root-project/root/pull/165:269,availability,state,statement,269,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:18,energy efficiency,optim,optimizations,18,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:207,energy efficiency,optim,optimize,207,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:389,energy efficiency,profil,profiling,389,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:269,integrability,state,statement,269,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:18,performance,optimiz,optimizations,18,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:149,performance,overhead,overhead,149,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:207,performance,optimiz,optimize,207,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:389,performance,profil,profiling,389,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:143,safety,avoid,avoid,143,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:244,safety,avoid,avoiding,244,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:11,security,access,access,11,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:118,security,access,accessors,118,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:174,security,access,accessing,174,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:216,security,access,access,216,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:442,security,access,accesses,442,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:435,testability,simpl,simple,435,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/165:435,usability,simpl,simple,435,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3). This merge request is result of profiling work in the AliRoot framework where simple accesses to the mentioned objects are considerable (on the 2% level). .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/165
https://github.com/root-project/root/pull/166:269,availability,state,statement,269,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:18,energy efficiency,optim,optimizations,18,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:207,energy efficiency,optim,optimize,207,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:269,integrability,state,statement,269,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:18,performance,optimiz,optimizations,18,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:149,performance,overhead,overhead,149,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:207,performance,optimiz,optimize,207,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:143,safety,avoid,avoid,143,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:244,safety,avoid,avoiding,244,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:11,security,access,access,11,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:118,security,access,accessors,118,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:174,security,access,accessing,174,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/166:216,security,access,access,216,"Inline and access optimizations for TLorentzVector and TVector3; - move important functions (constructors,destructors,accessors) to header. to avoid overhead in creating and accessing these small objects. - optimize access to TLorentzVector by avoiding a double switch statement. (switch on direction in TLorentzVector followed by same switch in TVector3).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/166
https://github.com/root-project/root/pull/169:1292,availability,error,error,1292,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1630,availability,error,error,1630,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:108,deployability,version,versions,108,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:202,deployability,contain,contain,202,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:290,deployability,version,versions,290,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:740,deployability,stage,stages,740,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:965,deployability,version,version,965,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1474,deployability,version,version,1474,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1768,deployability,version,version,1768,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:108,integrability,version,versions,108,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:193,integrability,messag,messages,193,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:290,integrability,version,versions,290,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:965,integrability,version,version,965,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1453,integrability,Configur,Configurable,1453,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1474,integrability,version,version,1474,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1768,integrability,version,version,1768,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:193,interoperability,messag,messages,193,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:108,modifiability,version,versions,108,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:290,modifiability,version,versions,290,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:965,modifiability,version,version,965,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1453,modifiability,Configur,Configurable,1453,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1474,modifiability,version,version,1474,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1545,modifiability,inherit,inherits,1545,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1768,modifiability,version,version,1768,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:382,performance,overhead,overhead,382,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1292,performance,error,error,1292,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1630,performance,error,error,1630,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1174,reliability,alert,alerted,1174,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:593,safety,test,testing,593,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1073,safety,test,testing,1073,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1174,safety,aler,alerted,1174,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1292,safety,error,error,1292,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1630,safety,error,error,1630,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:651,security,ident,identified,651,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1453,security,Configur,Configurable,1453,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:22,testability,instrument,instrumentation,22,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:593,testability,test,testing,593,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1073,testability,test,testing,1073,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:454,usability,user,users,454,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:580,usability,help,helpful,580,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1292,usability,error,error,1292,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/169:1630,usability,error,error,1630,"Remove superfluous IO instrumentation for many classes; This somewhat huge commit mainly demotes many class-versions to 0. . If wanted, I can for sure squash some things (right now, the commit messages contain the underlying reasoning). . In ROOT, a lot of classes were equipped with class-versions > 0 even though they are not meant for IO / streaming. . This produces unnecessary overhead (creation of Streamer() functions) and might be misleading for users (especially if they believe streaming of these classes would be ok and then lose parts of their data). That's even more helpful when testing framework's dataobject-code. . These classes were identified by https://github.com/olifre/rootStaticAnalyzer (a new project still in early stages) and I have created this PR to fix almost all these issues. . The last commit in the series also explicitly marks two members (of TSeqCollection and THashList) as transient, even though these classes are already class-version 0. This is purely to make it more explicit that these are not streamed - and allow for programmatic testing (since then the `kTransient` bit of the `TRealData` will be set correctly). . Several issues alerted by `rootStaticAnalyzer` still remain which are probably real bugs in ROOT 6. . Examples: . ```. TMVA/PDF.h:0: error: Data object class 'TMVA::PDF' will not stream the following indirect members: members 'fConfigName, fConfigDescription, fReferenceFile' from class 'TMVA::Configurable' (class-version 0)! ```. It seems like `TMVA::PDF` is meant for streaming, but inherits from a base which is not. . Similar to that:. ```. include/TTreeResult.h:0: error: Data object class 'TTreeResult' will not stream the following indirect members: members 'fRowCount' from class 'TSQLResult' (class-version 0)! ```.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/169
https://github.com/root-project/root/pull/170:511,availability,Error,Error,511,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:297,deployability,version,versions,297,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:536,deployability,Build,Build,536,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:912,deployability,fail,fails,912,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:189,integrability,buffer,buffer,189,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:297,integrability,version,versions,297,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:297,modifiability,version,versions,297,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:182,performance,memor,memory,182,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:511,performance,Error,Error,511,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:912,reliability,fail,fails,912,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:511,safety,Error,Error,511,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:827,safety,test,testing,827,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:835,safety,compl,completely,835,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:835,security,compl,completely,835,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:827,testability,test,testing,827,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:182,usability,memor,memory,182,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:511,usability,Error,Error,511,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/170:692,usability,clear,clear,692,"Fix bad streaming; This goes hand-in-hand with #169 . . These were found in a different way, though: . By creating them with their default constructor and trying to stream them to a memory buffer (the ""StreamingTest"" of https://github.com/olifre/rootStaticAnalyzer ). . This PR demotes some class-versions for classes which break when streamed (and which are not supposed to be streamed) and makes one more member transient which should be transient (in TTreeFormula). . There's one more remaining issue:. ```. Error in <TStreamerInfo::Build>: TRandom1, discarding: const unsigned int* fTheSeeds, no [dimension]. ```. I'm not sure what the ""dimension"" should be for this member - it's not so clear from the ranlux code to me. . More issues are probably still there since rootStaticAnalyzer right now excludes some classes from testing completely if their construction / destruction using the default constructor fails. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/170
https://github.com/root-project/root/pull/171:202,availability,error,error,202,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:105,deployability,automat,automatically,105,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:202,performance,error,error,202,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:202,safety,error,error,202,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:329,safety,test,test,329,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:395,safety,test,tests,395,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:401,safety,test,testStreamingUninitialized,401,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:588,safety,compl,complex,588,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:633,safety,test,test,633,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:438,security,ident,identifies,438,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:588,security,compl,complex,588,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:105,testability,automat,automatically,105,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:329,testability,test,test,329,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:395,testability,test,tests,395,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:401,testability,test,testStreamingUninitialized,401,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:633,testability,test,test,633,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/171:202,usability,error,error,202,"TProtoClass: Initialize also the fCheckSum member.; This was missing in the default constructor. . Found automatically by https://github.com/olifre/rootStaticAnalyzer : . ```. include/TProtoClass.h:78: error: Streamed member 'unsigned int fCheckSum' of dataobject 'TProtoClass' not initialized by constructor! ```. using the new test https://github.com/olifre/rootStaticAnalyzer/blob/master/src/tests/testStreamingUninitialized.cpp which identifies to-be-streamed members which are not initialized by the default constructors. . There are likely more, but the other cases seem a bit more complex - I'm still working on improving the test. .",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/171
https://github.com/root-project/root/pull/172:11,availability,error,error,11,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:59,availability,error,error,59,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:11,performance,error,error,11,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:59,performance,error,error,59,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:11,safety,error,error,11,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:59,safety,error,error,59,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:11,usability,error,error,11,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/172:59,usability,error,error,59,copy&paste error in cmake comment; seems like a copy&paste error (picked from #81).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/172
https://github.com/root-project/root/pull/173:4,availability,error,errors,4,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:215,availability,error,error,215,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:96,deployability,API,API,96,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:123,deployability,releas,release,123,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:96,integrability,API,API,96,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:96,interoperability,API,API,96,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:4,performance,error,errors,4,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:215,performance,error,error,215,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:4,safety,error,errors,4,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:201,safety,except,except,201,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:215,safety,error,error,215,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:4,usability,error,errors,4,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/173:215,usability,error,error,215,Fix errors with GCC 6.1.1 and Python 3.5; ROOT won't compile because of changes in the Python 3 API together with. GCC 6.1 release. The code changes fixes the issues on my machine running. Arch Linux (except for an error in Python 3.5 numpy/__multiarray_api.h).,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/173
https://github.com/root-project/root/pull/174:6,deployability,version,version,6,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:83,deployability,version,version,83,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:134,deployability,version,version,134,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:6,integrability,version,version,6,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:83,integrability,version,version,83,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:134,integrability,version,version,134,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:6,modifiability,version,version,6,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:83,modifiability,version,version,83,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:134,modifiability,version,version,134,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:20,safety,prevent,preventing,20,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/174:20,security,preven,preventing,20,fix a version check preventing non-Apple clang compilation with vc=ON; Apple clang version 4.0 is based on LLVM 3.1. That's where the version check for clang comes from.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/174
https://github.com/root-project/root/pull/177:302,deployability,version,version,302,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:682,deployability,version,versions,682,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:302,integrability,version,version,302,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:682,integrability,version,versions,682,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:345,interoperability,platform,platforms,345,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:415,interoperability,compatib,compatibilities,415,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:449,interoperability,incompatib,incompatible,449,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:639,interoperability,format,format,639,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:302,modifiability,version,version,302,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:313,modifiability,extens,extensive,313,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:682,modifiability,version,versions,682,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:323,safety,test,testing,323,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:400,safety,review,review,400,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:561,safety,review,reviewing,561,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:257,security,hack,hack,257,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:323,testability,test,testing,323,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:400,testability,review,review,400,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/177:561,testability,review,reviewing,561,"[wip] more and new compression algorithms; include lzo, lz4, zopfli, and brotli as compression algorithms in root. Essentially what is in [pseyfert/root-compression](https://github.com/pseyfert/root-compression) but built into root instead of as LD_PRELOAD hack. open issues:. - run benchmarks in this version. - extensive testing (e.g. various platforms). - decide which parts are worth pursuing. - review license compatibilities (LZO is GPL, i.e. incompatible with root; LZ4 is BSD; ZOPFLI is APACHE; BROTLI is MIT). - for lz4, there is also #81. It is worth reviewing the differences. notable features:. - zopfli compresses in the zlib format and can thus be read even with root versions w/o zopfli.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/177
https://github.com/root-project/root/pull/179:58,integrability,transform,transformation,58,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:58,interoperability,transform,transformation,58,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:258,interoperability,specif,specific,258,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:49,modifiability,variab,variable,49,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:174,modifiability,variab,variables,174,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:230,modifiability,variab,variables,230,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:435,safety,review,review,435,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/179:435,testability,review,review,435,Added Variance Threshold method in DataLoader; A variable transformation method Variance Threshold (VT) has been added in DataLoader class which computes the variance of all variables and return a new DataLoader with the selected variables which lie above a specific threshold. I have also created a Jupyter [notebook](https://github.com/abhinavmoudgil95/tmva-notebooks/blob/master/VarianceThreshold.ipynb) to demonstrate it. . Kindly review. . cc: @omazapa @lmoneta .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/179
https://github.com/root-project/root/pull/180:239,deployability,Log,Log,239,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:101,integrability,pub,public,101,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:225,integrability,Configur,Configurable,225,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:262,integrability,pub,public,262,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:337,integrability,pub,public,337,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:225,modifiability,Configur,Configurable,225,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:239,safety,Log,Log,239,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:225,security,Configur,Configurable,225,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:239,security,Log,Log,239,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/180:239,testability,Log,Log,239,"Master tmva gsoc2016; Hi Enric,. The next 3 items was applied. - DefaultDataSetInfo: Omar will add a public method to DataLoader for getting a const reference to the DataSetInfo. This is needed by both Attila and Georgios. - Configurable::Log: Omar will make it public. Attila needs this. - MethodBase::CreateVariableTransforms: make it public. Attila needs this. The last one is an static method in the class MethodBase, then may you can call it in python. from ROOT import TMVA. TMVA.MethodBase.CreateVariableTransforms. Best . Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/180
https://github.com/root-project/root/pull/181:0,availability,Restor,Restore,0,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:26,deployability,version,versions,26,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:96,deployability,version,versions,96,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:121,deployability,Version,Versions,121,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:465,deployability,fail,fails,465,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:523,deployability,version,versions,523,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:682,deployability,version,versions,682,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:26,integrability,version,versions,26,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:96,integrability,version,versions,96,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:121,integrability,Version,Versions,121,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:523,integrability,version,versions,523,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:682,integrability,version,versions,682,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:26,modifiability,version,versions,26,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:96,modifiability,version,versions,96,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:121,modifiability,Version,Versions,121,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:523,modifiability,version,versions,523,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:682,modifiability,version,versions,682,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:0,reliability,Restor,Restore,0,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:465,reliability,fail,fails,465,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:8,usability,support,support,8,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:78,usability,support,support,78,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:305,usability,command,command,305,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:509,usability,support,supports,509,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/181:664,usability,support,support,664,"Restore support for GFAL2 versions less than 2.10; Using -D_GFAL2_API_ breaks support for gfal2 versions less than 2.10. Versions prior to 2.10 used _GFAL2_API_ as the include guard in the gfal_api.h file. ```. #ifndef _GFAL2_API_. #define _GFAL2_API_. [...]. #endif. ```. If -D_GFAL2_API_ is used on the command line this means that the include guard is already set, and the declarations in the header file is not seen by the compiler, which means the compilation fails. This was already fixed in a way that supports both versions less than 2.10 and greater than or equal to 2.10 in pull request #106. There is no need to fix the issue again in a way that breaks support for older versions.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/181
https://github.com/root-project/root/pull/182:0,deployability,Updat,Update,0,Update PR of Oliver ;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/182
https://github.com/root-project/root/pull/182:0,safety,Updat,Update,0,Update PR of Oliver ;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/182
https://github.com/root-project/root/pull/182:0,security,Updat,Update,0,Update PR of Oliver ;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/182
https://github.com/root-project/root/pull/183:22,interoperability,conflict,conflict,22,A few changes lost in conflict resolution; Lost when merging #153.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/183
https://github.com/root-project/root/pull/184:0,usability,Document,Documentation,0,"Documentation fixes fixes; Thank you for considering my documentation fixes. Here are the few missing bits that were lost in merging. Note this is split in two commits - one for addressing the mac end-of-line encoding of two files, and one for the rest (which includes a fix to one of the files changed in the first commit).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/184
https://github.com/root-project/root/pull/184:56,usability,document,documentation,56,"Documentation fixes fixes; Thank you for considering my documentation fixes. Here are the few missing bits that were lost in merging. Note this is split in two commits - one for addressing the mac end-of-line encoding of two files, and one for the rest (which includes a fix to one of the files changed in the first commit).",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/184
https://github.com/root-project/root/pull/185:1323,deployability,releas,release-area,1323,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:1353,deployability,releas,release-area,1353,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:140,energy efficiency,Draw,Draw,140,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:414,energy efficiency,Draw,Draw,414,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:892,energy efficiency,Draw,Draw,892,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:939,energy efficiency,Draw,Draw,939,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:980,energy efficiency,Draw,Draw,980,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:192,modifiability,variab,variables,192,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:343,modifiability,variab,variable,343,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:531,modifiability,variab,variable,531,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:858,modifiability,variab,variable,858,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:1007,modifiability,variab,variable,1007,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:1096,modifiability,variab,variable,1096,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:1232,modifiability,variab,variables,1232,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:302,reliability,doe,does,302,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:467,safety,test,tests,467,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:496,safety,test,test,496,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:521,safety,valid,valid,521,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:550,safety,test,testing,550,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:571,safety,test,test,571,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:467,testability,test,tests,467,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:496,testability,test,test,496,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:550,testability,test,testing,550,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:571,testability,test,test,571,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:0,usability,minim,minimal,0,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/185:1455,usability,Feedback,Feedback,1455,"minimal check if branch name could cause problems; The idea is to catch branch names, which will lead to problems when using the tree with `Draw` or `MakeClass`. This can either be the member variables (branch names and `b_`branch names) which have to keep the generated code compilable (and ensure it does what it is expected to do). And the variable names should not lead to confusion with formula evaluation in Draw (e.g. branch names which are pure numbers). The tests suggested here are:. - test if branch name is a valid c++ variable name (w/o testing keywords). - test if branch name begins with ""b_"" (potential problem with MakeClass). on top of that, I also have a [black list](https://github.com/pseyfert/tmva-branch-adder/blob/master/src/blacklist.cpp) of unfortunate branch names: methods of TTrees, which would clash in MakeClass, c++ keywords, variable types, and things TTree::Draw can parse (though I don't see how `TTree::Draw(""cos(x)"")` would clash with `TTree::Draw(""cos"")` if there is a variable named `cos`. Because the function `cos` wouldn't work without argument, and the variable `cos` wouldn't work with argument). I only warn here and don't abort the branch initialisation, not to break third party code (variables with `.` are probably common [e.g. dynamically generated from float](http://lhcb-release-area.web.cern.ch/LHCb-release-area/DOC/davinci/latest_doxygen/d9/d80/_tuple_tool_cone_isolation_8cpp_source.html#l00204) ). Feedback request:. I'm unsure if putting the blacklist into TBranch.cxx is really the best solution to apply here.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/185
https://github.com/root-project/root/pull/187:0,modifiability,Refact,Refactoring,0,Refactoring of DNN in TMVA. Bug fix and improvement in steepest descent algorithm.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/187
https://github.com/root-project/root/pull/187:0,performance,Refactor,Refactoring,0,Refactoring of DNN in TMVA. Bug fix and improvement in steepest descent algorithm.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/187
https://github.com/root-project/root/pull/188:139,deployability,patch,patches,139,Fix GCC 6 compilation (tested on ArchLinux); See https://sft.its.cern.ch/jira/browse/ROOT-8180 . This pull request implements the proposed patches there.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/188
https://github.com/root-project/root/pull/188:23,safety,test,tested,23,Fix GCC 6 compilation (tested on ArchLinux); See https://sft.its.cern.ch/jira/browse/ROOT-8180 . This pull request implements the proposed patches there.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/188
https://github.com/root-project/root/pull/188:139,safety,patch,patches,139,Fix GCC 6 compilation (tested on ArchLinux); See https://sft.its.cern.ch/jira/browse/ROOT-8180 . This pull request implements the proposed patches there.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/188
https://github.com/root-project/root/pull/188:139,security,patch,patches,139,Fix GCC 6 compilation (tested on ArchLinux); See https://sft.its.cern.ch/jira/browse/ROOT-8180 . This pull request implements the proposed patches there.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/188
https://github.com/root-project/root/pull/188:23,testability,test,tested,23,Fix GCC 6 compilation (tested on ArchLinux); See https://sft.its.cern.ch/jira/browse/ROOT-8180 . This pull request implements the proposed patches there.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/188
https://github.com/root-project/root/pull/189:188,interoperability,Mismatch,Mismatched,188,"ROOTR: - fixed example.; Dear Axel,. I think that the example is fixed,. I ran valgrind for it and I dont found critical issues for a segfault like ""Invalid write/read"", ""Invalid free"",. ""Mismatched free"" or ""overlap in memcpy"". http://files.oproject.org/rootr-valgrind.txt. Sorry for my late reply. Best. Omar.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/189
https://github.com/root-project/root/pull/190:0,usability,document,documentation,0,documentation: fix legacy definition name;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/190
https://github.com/root-project/root/pull/191:0,modifiability,refact,refactoring,0,refactoring of DNN in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/191
https://github.com/root-project/root/pull/191:0,performance,refactor,refactoring,0,refactoring of DNN in TMVA;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/191
https://github.com/root-project/root/pull/193:18,integrability,messag,messages,18,"TMVA, fix warning messages;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/193
https://github.com/root-project/root/pull/193:18,interoperability,messag,messages,18,"TMVA, fix warning messages;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/193
https://github.com/root-project/root/pull/194:570,availability,operat,operation,570,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:774,availability,sli,slightly,774,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:814,deployability,API,API,814,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:0,energy efficiency,Reduc,Reduce,0,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:250,energy efficiency,alloc,allocated,250,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:311,energy efficiency,alloc,allocated,311,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:710,energy efficiency,estimat,estimate,710,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1076,energy efficiency,reduc,reduction,1076,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1136,energy efficiency,reduc,reduced,1136,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:637,integrability,buffer,buffer,637,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:700,integrability,buffer,buffer,700,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:814,integrability,API,API,814,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:874,integrability,buffer,buffer,874,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1256,integrability,Event,Event,1256,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1350,integrability,Event,Event,1350,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1496,integrability,event,event,1496,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:814,interoperability,API,API,814,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1630,interoperability,compatib,compatible,1630,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1326,modifiability,paramet,parameters,1326,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1103,performance,time,time,1103,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1131,performance,time,time,1131,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1164,performance,time,time,1164,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:774,reliability,sli,slightly,774,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:868,safety,input,input,868,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:970,safety,test,tests,970,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1060,safety,test,test,1060,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1234,safety,test,tested,1234,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1263,safety,test,test,1263,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1321,safety,test,test,1321,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:82,security,hash,hash,82,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:150,security,hash,hash,150,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:202,security,hash,hash,202,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:509,security,hash,hash,509,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1214,security,hash,hash,1214,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1443,security,Modif,Modified,1443,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:970,testability,test,tests,970,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1060,testability,test,test,1060,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1234,testability,test,tested,1234,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1263,testability,test,test,1263,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:1321,testability,test,test,1321,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/194:868,usability,input,input,868,"Reduce LZMA dictionary size for small baskets; LZMA by default creates very large hash tables for its dictionaries, e.g., at compression level 4, the hash table is 4Mi 4 byte entries, 16 MiB total. The hash table has to be zeroed before use so it is allocated via calloc(), which means all the pages have to be allocated, mapped and written. ROOT baskets are often much smaller than the default LZMA dictionaries; for small baskets, the large dictionary has very little compression benefit, while zeroing the hash table can be more expensive than the actual compression operation. Since R__zipLZMA() is actually being used to compress a buffer of known size, not a stream, we can use the size of the buffer to estimate an appropriate size for the dictionary. This PR uses a slightly more advanced part of the LZMA API to set the dictionary size to 1/4 the size of the input buffer, if that is smaller than the default size from the selected preset compression level. In tests with CMS data, this results in less than 1% increase in the output size and (in one test job) a 25% reduction in job total run time, with LZMA compression time reduced by 80% (all of that time that was being spent in memset() zeroing the hash table). I also tested this with the ""Event"" test program with Brian's changes from #59. With the same test parameters as Brian (""./Event 4000 6 99 1 1000 2""), I get. ZLIB level-6: 14.4 MB/s. Original LZMA level-6: 2.3 MB/s. Modified LZMA level-6: 3.0 MB/s. With 100 tracks per event (and hence smaller baskets) the improvement is from 2.2 MB/s to 3.9 MB/s. This change should be fully transparent and backwards compatible.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/194
https://github.com/root-project/root/pull/195:51,testability,Simpl,Simple,51,Force ON LLVM_ENABLE_CXX1Y when cxx14 is enabled.; Simple fix to force LLVM to use CXX1Y when cxx14 is enabled.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/195
https://github.com/root-project/root/pull/195:51,usability,Simpl,Simple,51,Force ON LLVM_ENABLE_CXX1Y when cxx14 is enabled.; Simple fix to force LLVM to use CXX1Y when cxx14 is enabled.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/195
https://github.com/root-project/root/pull/196:53,deployability,Depend,Depends,53,"Change the way the active area for TAxis is defined; Depends on label size of the axis, but now the lower limit is the default label size. That way, zero size label axes are still click and draggable.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/196
https://github.com/root-project/root/pull/196:53,integrability,Depend,Depends,53,"Change the way the active area for TAxis is defined; Depends on label size of the axis, but now the lower limit is the default label size. That way, zero size label axes are still click and draggable.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/196
https://github.com/root-project/root/pull/196:53,modifiability,Depend,Depends,53,"Change the way the active area for TAxis is defined; Depends on label size of the axis, but now the lower limit is the default label size. That way, zero size label axes are still click and draggable.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/196
https://github.com/root-project/root/pull/196:53,safety,Depend,Depends,53,"Change the way the active area for TAxis is defined; Depends on label size of the axis, but now the lower limit is the default label size. That way, zero size label axes are still click and draggable.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/196
https://github.com/root-project/root/pull/196:53,testability,Depend,Depends,53,"Change the way the active area for TAxis is defined; Depends on label size of the axis, but now the lower limit is the default label size. That way, zero size label axes are still click and draggable.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/196
https://github.com/root-project/root/pull/197:121,deployability,automat,automatically,121,"Create notebooks from tutorials; converttonotebook.py script is added. Also, filter.cxx and the makefile are modified to automatically create notebook folder, wherein the notebooks will be created.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/197
https://github.com/root-project/root/pull/197:77,integrability,filter,filter,77,"Create notebooks from tutorials; converttonotebook.py script is added. Also, filter.cxx and the makefile are modified to automatically create notebook folder, wherein the notebooks will be created.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/197
https://github.com/root-project/root/pull/197:109,security,modif,modified,109,"Create notebooks from tutorials; converttonotebook.py script is added. Also, filter.cxx and the makefile are modified to automatically create notebook folder, wherein the notebooks will be created.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/197
https://github.com/root-project/root/pull/197:121,testability,automat,automatically,121,"Create notebooks from tutorials; converttonotebook.py script is added. Also, filter.cxx and the makefile are modified to automatically create notebook folder, wherein the notebooks will be created.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/197
https://github.com/root-project/root/pull/198:23,safety,avoid,avoid,23,"GetTitle is const now, avoid const_cast;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/198
https://github.com/root-project/root/pull/199:6,energy efficiency,GPU,GPU,6,First GPU implementation of DNNs.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/199
https://github.com/root-project/root/pull/199:6,performance,GPU,GPU,6,First GPU implementation of DNNs.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/199
https://github.com/root-project/root/pull/200:130,safety,Safe,Safe,130,"Rewrite ROOT::Experimental::TDirectory[Entry].h, add cast to base class; Based on [1]. [1] Neri, C, ""Twisting the RTTI System for Safe Dynamic Casts of void\* in C++"", Dr.Dobb's Journal, April 05, 2011. http://drdobbs.com/cpp/229401004.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/200
https://github.com/root-project/root/pull/201:87,integrability,filter,filter,87,"Created link from tutorial pages to open notebooks, improved converttonotebook script; filter.cxx was modified to create a link to the notebooks that are generated by the script. The converttonotebook.py script was modified to use regular expressions to find the functions. Also, comments were added and debugging print-outs were removed.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/201
https://github.com/root-project/root/pull/201:102,security,modif,modified,102,"Created link from tutorial pages to open notebooks, improved converttonotebook script; filter.cxx was modified to create a link to the notebooks that are generated by the script. The converttonotebook.py script was modified to use regular expressions to find the functions. Also, comments were added and debugging print-outs were removed.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/201
https://github.com/root-project/root/pull/201:215,security,modif,modified,215,"Created link from tutorial pages to open notebooks, improved converttonotebook script; filter.cxx was modified to create a link to the notebooks that are generated by the script. The converttonotebook.py script was modified to use regular expressions to find the functions. Also, comments were added and debugging print-outs were removed.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/201
https://github.com/root-project/root/pull/202:289,deployability,updat,updated,289,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/202:289,safety,updat,updated,289,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/202:32,security,modif,modified,32,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/202:115,security,modif,modified,115,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/202:289,security,updat,updated,289,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/202:229,usability,document,documentation,229,"Changed links to notebooks, and modified graph tutorials to create notebooks; The link system to the notebooks was modified to appear in the brief description. The brief description and link to source files were removed from the documentation for files type ""file"". Several tutorials were updated to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/202
https://github.com/root-project/root/pull/203:23,security,modif,modified,23,Added link to SWAN and modified hist tutorials to create notebooks;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/203
https://github.com/root-project/root/pull/204:12,availability,error,errors,12,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/204:12,performance,error,errors,12,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/204:12,safety,error,errors,12,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/204:28,safety,compl,complete,28,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/204:28,security,compl,complete,28,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/204:12,usability,error,errors,12,Fix doxygen errors for code complete.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/204
https://github.com/root-project/root/pull/205:569,deployability,patch,patch,569,"Rewrite ROOT::Experimental::TDirectory[Entry].h, add cast to base class; Based on `TClass::DynamicCast()`. `root` script to check it working https://gist.github.com/BerserkerTroll/b94c2d3e3a5848be7c7dd53e323e1cdb. Besides different cast method, some bugs were fixed, so #200 was actually broken, despite it compiles (nobody has instantiated broken templates). Consider @a885623b7f04d193b299097b189bbb1fd45139e1 (exception-based cast, without bugs) if you want to get rid of `TClass` or apply this https://gist.github.com/BerserkerTroll/b6624c5f74b71293bc9c2059b4e5236a patch to this PR.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/205
https://github.com/root-project/root/pull/205:412,safety,except,exception-based,412,"Rewrite ROOT::Experimental::TDirectory[Entry].h, add cast to base class; Based on `TClass::DynamicCast()`. `root` script to check it working https://gist.github.com/BerserkerTroll/b94c2d3e3a5848be7c7dd53e323e1cdb. Besides different cast method, some bugs were fixed, so #200 was actually broken, despite it compiles (nobody has instantiated broken templates). Consider @a885623b7f04d193b299097b189bbb1fd45139e1 (exception-based cast, without bugs) if you want to get rid of `TClass` or apply this https://gist.github.com/BerserkerTroll/b6624c5f74b71293bc9c2059b4e5236a patch to this PR.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/205
https://github.com/root-project/root/pull/205:569,safety,patch,patch,569,"Rewrite ROOT::Experimental::TDirectory[Entry].h, add cast to base class; Based on `TClass::DynamicCast()`. `root` script to check it working https://gist.github.com/BerserkerTroll/b94c2d3e3a5848be7c7dd53e323e1cdb. Besides different cast method, some bugs were fixed, so #200 was actually broken, despite it compiles (nobody has instantiated broken templates). Consider @a885623b7f04d193b299097b189bbb1fd45139e1 (exception-based cast, without bugs) if you want to get rid of `TClass` or apply this https://gist.github.com/BerserkerTroll/b6624c5f74b71293bc9c2059b4e5236a patch to this PR.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/205
https://github.com/root-project/root/pull/205:569,security,patch,patch,569,"Rewrite ROOT::Experimental::TDirectory[Entry].h, add cast to base class; Based on `TClass::DynamicCast()`. `root` script to check it working https://gist.github.com/BerserkerTroll/b94c2d3e3a5848be7c7dd53e323e1cdb. Besides different cast method, some bugs were fixed, so #200 was actually broken, despite it compiles (nobody has instantiated broken templates). Consider @a885623b7f04d193b299097b189bbb1fd45139e1 (exception-based cast, without bugs) if you want to get rid of `TClass` or apply this https://gist.github.com/BerserkerTroll/b6624c5f74b71293bc9c2059b4e5236a patch to this PR.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/205
https://github.com/root-project/root/pull/206:205,security,modif,modified,205,"Fixed bugs, badges for swan and notebook, graphics tutorials; Bugs in converttonotebook were fixed. The badges of swan and notebook were changed, and the open in a new tab now. The graphics tutorials were modified to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/206
https://github.com/root-project/root/pull/207:38,deployability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,integrability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,interoperability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,modifiability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,reliability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,security,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:38,testability,integr,integration,38,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:133,usability,support,support,133,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/207:196,usability,command,command,196,"Create notebooks from fit, add jsroot integration; Create notebooks from fit tutorials. Bugs were fixed in converttonotebooks. Added support for jsroot. Adding the option `-js` to the `\notebook` command will output the graph in jsrrot. Also added the option `-nodraw` for tutorials that have no graphical output. Finally, changed the notebook badge color from orange to blue.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/207
https://github.com/root-project/root/pull/209:254,availability,error,error,254,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:312,deployability,fail,fails,312,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:13,integrability,filter,filter,13,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:57,integrability,Filter,Filter,57,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:260,integrability,messag,message,260,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:260,interoperability,messag,message,260,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:254,performance,error,error,254,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:64,reliability,doe,doesn,64,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:312,reliability,fail,fails,312,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:254,safety,error,error,254,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:43,security,access,access,43,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:196,security,access,access,196,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:80,usability,command,command,80,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/209:254,usability,error,error,254,"Fixed bug in filter, changed how tutorials access files; Filter doesn't display command line option `-js` and `-nodraw` in the description anymore. Tutorials now use `gROOT->GetTutorialsDir()` to access tutorial files. converttonotebooks now displays an error message that is picked up by jenkins when nbconvert fails.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/209
https://github.com/root-project/root/pull/210:61,modifiability,variab,variable,61,"Added notebooks to tutorials, fixed other notebooks, changed variable name;",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/210
https://github.com/root-project/root/pull/211:230,security,modif,modified,230,"Improved converttonotebooks, made math, tree, and image notebooks; The converttonotebook.py script was overhauled, both in terms of organization as well as in terms of style. Tutorials in the math, tree and image directories were modified to create notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/211
https://github.com/root-project/root/pull/212:185,deployability,patch,patch,185,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:18,integrability,interfac,interface,18,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:18,interoperability,interfac,interface,18,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:18,modifiability,interfac,interface,18,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:98,modifiability,paramet,parameters,98,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:117,performance,perform,performing,117,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:185,safety,patch,patch,185,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:220,safety,test,test,220,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:185,security,patch,patch,185,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:215,testability,unit,unit,215,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:220,testability,test,test,220,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:67,usability,interact,interactively,67,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/212:117,usability,perform,performing,117,Added TChain-like interface to THnBase histograms.; This allows to interactively adjust histogram parameters before. performing a projection to a lower dimensional representation. This patch implements ROOT-4515. A unit test is being added in root-mirror/roottest/pull/6.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/212
https://github.com/root-project/root/pull/213:201,availability,state,statements,201,"Roofit notebooks, doctest in convertonotebook; The roofit tutorials were modified to create notebooks. This involved changing the tab size to 3 on all of them, as well as removing the`ifndef __CINT__` statements. . Also, doctests were created for most of the functions in convertonotebook.py.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/213
https://github.com/root-project/root/pull/213:201,integrability,state,statements,201,"Roofit notebooks, doctest in convertonotebook; The roofit tutorials were modified to create notebooks. This involved changing the tab size to 3 on all of them, as well as removing the`ifndef __CINT__` statements. . Also, doctests were created for most of the functions in convertonotebook.py.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/213
https://github.com/root-project/root/pull/213:73,security,modif,modified,73,"Roofit notebooks, doctest in convertonotebook; The roofit tutorials were modified to create notebooks. This involved changing the tab size to 3 on all of them, as well as removing the`ifndef __CINT__` statements. . Also, doctests were created for most of the functions in convertonotebook.py.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/213
https://github.com/root-project/root/pull/214:96,interoperability,xml,xml,96,"Created notebooks for roostats, foam, matrix, multicore (mp only)...; physics, quadp, spectrum, xml. Also fixed the regex, and added a fix for using the roostats namespace in convertotonotebook.py.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/214
https://github.com/root-project/root/pull/215:52,availability,sli,slightly,52,"Fixing miscellaneous notebooks; Many tutorials were slightly modified to ensure that the converttonotebook script functions properly with them. The convertotonotebook script was modified in regards to how it handles main functions that take arguments. It now places the arguments in a cell of their own, and converts the inside of the function into cells, instead of declaring the whole function and calling it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/215
https://github.com/root-project/root/pull/215:52,reliability,sli,slightly,52,"Fixing miscellaneous notebooks; Many tutorials were slightly modified to ensure that the converttonotebook script functions properly with them. The convertotonotebook script was modified in regards to how it handles main functions that take arguments. It now places the arguments in a cell of their own, and converts the inside of the function into cells, instead of declaring the whole function and calling it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/215
https://github.com/root-project/root/pull/215:61,security,modif,modified,61,"Fixing miscellaneous notebooks; Many tutorials were slightly modified to ensure that the converttonotebook script functions properly with them. The convertotonotebook script was modified in regards to how it handles main functions that take arguments. It now places the arguments in a cell of their own, and converts the inside of the function into cells, instead of declaring the whole function and calling it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/215
https://github.com/root-project/root/pull/215:178,security,modif,modified,178,"Fixing miscellaneous notebooks; Many tutorials were slightly modified to ensure that the converttonotebook script functions properly with them. The convertotonotebook script was modified in regards to how it handles main functions that take arguments. It now places the arguments in a cell of their own, and converts the inside of the function into cells, instead of declaring the whole function and calling it.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/215
https://github.com/root-project/root/pull/216:221,availability,sli,slightly,221,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:337,availability,state,statements,337,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:122,integrability,filter,filter,122,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:337,integrability,state,statements,337,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:221,reliability,sli,slightly,221,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:133,security,modif,modified,133,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:212,security,modif,modified,212,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:177,testability,simpl,simple,177,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/216:177,usability,simpl,simple,177,"Fixes in notebooks, style fixes; Several changes were made. Firstly, several bugs were fixed in converttonotebook.py. The filter was modified to replace the \notebook line in a simple manner. Many tutorials were modified slightly, either ensuring tabs were three spaces as well as adding `-js`or `-nodraw` options. Finally, preprocessor statements involving `__CINT__` were removed from the tutorials, as well as `gStyle->SetPalette(1)`.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/216
https://github.com/root-project/root/pull/217:51,deployability,updat,update,51,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:196,deployability,version,version,196,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:61,integrability,coupl,couple,61,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:196,integrability,version,version,196,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:167,interoperability,compatib,compatibility,167,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:18,modifiability,variab,variable,18,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:61,modifiability,coupl,couple,61,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:105,modifiability,variab,variable,105,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:196,modifiability,version,version,196,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:51,safety,updat,update,51,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:51,security,updat,update,51,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:61,testability,coupl,couple,61,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:0,usability,Aesthet,Aesthetics,0,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:146,usability,clarit,clarity,146,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/217:530,usability,aesthet,aesthetic,530,"Aesthetics fixes, variable names changes, markdown update; A couple broken tutorials were fixed. Several variable names were changed for improved clarity as a well as compatibility with the older version of the interpreter. The syntax `~~~` was replaced with the more universal Markdown syntax ``` ``` ```. Several bug fixes and improvements were made to the convertonotebook script. this includes capitalizing the first letter in every Markdown cell, and ensuring the link in the header is reproduced appropriately. Lastly, many aesthetic improvements were made to the tutorials, including ensuring tabs are three spaces, and styling important comments with Markdown.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/217
https://github.com/root-project/root/pull/219:29,availability,error,error,29,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:169,availability,error,error,169,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:43,deployability,version,versions,43,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:192,deployability,version,versions,192,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:43,integrability,version,versions,43,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:192,integrability,version,versions,192,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:43,modifiability,version,versions,43,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:192,modifiability,version,versions,192,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:29,performance,error,error,29,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:169,performance,error,error,169,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:29,safety,error,error,29,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:169,safety,error,error,169,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:225,security,polic,policy,225,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:274,security,polic,policy,274,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:29,usability,error,error,29,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:169,usability,error,error,169,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/219:264,usability,help,help,264,"Fix leading whitespace CMake error.; CMake versions 2.4 and below silently removed leading and trailing. whitespace from libraries linked with code. This was raising an error. for newer CMake versions, according to new CMake policy CMP004. https://cmake.org/cmake/help/v3.0/policy/CMP0004.html.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/219
https://github.com/root-project/root/pull/220:6,energy efficiency,CPU,CPU,6,First CPU implementation of HashedNets compression algorithm for DNNs; This is an implementation of HashedNets (https://arxiv.org/pdf/1504.04788.pdf) in ROOT. It is built over the existing implementation of DNNs in TMVA and supports multithreading.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/220
https://github.com/root-project/root/pull/220:6,performance,CPU,CPU,6,First CPU implementation of HashedNets compression algorithm for DNNs; This is an implementation of HashedNets (https://arxiv.org/pdf/1504.04788.pdf) in ROOT. It is built over the existing implementation of DNNs in TMVA and supports multithreading.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/220
https://github.com/root-project/root/pull/220:28,security,Hash,HashedNets,28,First CPU implementation of HashedNets compression algorithm for DNNs; This is an implementation of HashedNets (https://arxiv.org/pdf/1504.04788.pdf) in ROOT. It is built over the existing implementation of DNNs in TMVA and supports multithreading.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/220
https://github.com/root-project/root/pull/220:100,security,Hash,HashedNets,100,First CPU implementation of HashedNets compression algorithm for DNNs; This is an implementation of HashedNets (https://arxiv.org/pdf/1504.04788.pdf) in ROOT. It is built over the existing implementation of DNNs in TMVA and supports multithreading.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/220
https://github.com/root-project/root/pull/220:224,usability,support,supports,224,First CPU implementation of HashedNets compression algorithm for DNNs; This is an implementation of HashedNets (https://arxiv.org/pdf/1504.04788.pdf) in ROOT. It is built over the existing implementation of DNNs in TMVA and supports multithreading.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/220
https://github.com/root-project/root/pull/221:137,integrability,filter,filter,137,"PyROOT notebooks, aesthetic fixes; Notebooks are created from PyROOT, which involved changing the converttonotebook.py script as well as filter.cxx. Also, many aesthetic fixes were made, mainly removing unnecessary fill colors.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/221
https://github.com/root-project/root/pull/221:18,usability,aesthet,aesthetic,18,"PyROOT notebooks, aesthetic fixes; Notebooks are created from PyROOT, which involved changing the converttonotebook.py script as well as filter.cxx. Also, many aesthetic fixes were made, mainly removing unnecessary fill colors.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/221
https://github.com/root-project/root/pull/221:160,usability,aesthet,aesthetic,160,"PyROOT notebooks, aesthetic fixes; Notebooks are created from PyROOT, which involved changing the converttonotebook.py script as well as filter.cxx. Also, many aesthetic fixes were made, mainly removing unnecessary fill colors.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/221
https://github.com/root-project/root/pull/222:120,safety,input,input,120,Add the class TSimpleAnalysis; The class TSimpleAnalysis allow to create histograms after reading the arguments from an input file or from the command line.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/222
https://github.com/root-project/root/pull/222:120,usability,input,input,120,Add the class TSimpleAnalysis; The class TSimpleAnalysis allow to create histograms after reading the arguments from an input file or from the command line.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/222
https://github.com/root-project/root/pull/222:143,usability,command,command,143,Add the class TSimpleAnalysis; The class TSimpleAnalysis allow to create histograms after reading the arguments from an input file or from the command line.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/222
https://github.com/root-project/root/pull/223:6,energy efficiency,core,core,6,Multi-core and gpu-accelerated neural networks implementation in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/223
https://github.com/root-project/root/pull/223:15,energy efficiency,gpu,gpu-accelerated,15,Multi-core and gpu-accelerated neural networks implementation in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/223
https://github.com/root-project/root/pull/223:15,performance,gpu,gpu-accelerated,15,Multi-core and gpu-accelerated neural networks implementation in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/223
https://github.com/root-project/root/pull/223:38,performance,network,networks,38,Multi-core and gpu-accelerated neural networks implementation in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/223
https://github.com/root-project/root/pull/223:38,security,network,networks,38,Multi-core and gpu-accelerated neural networks implementation in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/223
https://github.com/root-project/root/pull/224:25,usability,support,support,25,thisroot.sh: improve zsh support; This allows doing. ```. source `root-config --prefix`/bin/thisroot.sh. ```. instead of. ```. cd `root-config --prefix` && source bin/thisroot.sh. ```.,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/224
https://github.com/root-project/root/pull/225:56,usability,support,support,56,"TMVA Doxygen; The TMVA tutorials were Doxygen-ized, and support was added for the creation of their notebooks. Also, small fixes were made to the convertonotebook.py script.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/225
https://github.com/root-project/root/pull/226:0,energy efficiency,GPU,GPU,0,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/226
https://github.com/root-project/root/pull/226:14,energy efficiency,core,core,14,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/226
https://github.com/root-project/root/pull/226:0,performance,GPU,GPU,0,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/226
https://github.com/root-project/root/pull/226:38,performance,network,networks,38,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/226
https://github.com/root-project/root/pull/226:38,security,network,networks,38,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/226
https://github.com/root-project/root/pull/229:228,integrability,filter,filter,228,"Fix TMVA notebooks, parallelize notebooks; TMVA notebooks were fixed. A script that creates the notebooks in parallel is introduced. Note that as of this commit the notebooks are being created twice, the old way via the doxygen filter, and the new parallel way. Once it is ensured that this method works, the old method will be removed. The command for running the script was placed in the doxygen target because if it was made its own target, the necessary folders would not have been created yet.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/229
https://github.com/root-project/root/pull/229:20,performance,parallel,parallelize,20,"Fix TMVA notebooks, parallelize notebooks; TMVA notebooks were fixed. A script that creates the notebooks in parallel is introduced. Note that as of this commit the notebooks are being created twice, the old way via the doxygen filter, and the new parallel way. Once it is ensured that this method works, the old method will be removed. The command for running the script was placed in the doxygen target because if it was made its own target, the necessary folders would not have been created yet.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/229
https://github.com/root-project/root/pull/229:109,performance,parallel,parallel,109,"Fix TMVA notebooks, parallelize notebooks; TMVA notebooks were fixed. A script that creates the notebooks in parallel is introduced. Note that as of this commit the notebooks are being created twice, the old way via the doxygen filter, and the new parallel way. Once it is ensured that this method works, the old method will be removed. The command for running the script was placed in the doxygen target because if it was made its own target, the necessary folders would not have been created yet.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/229
https://github.com/root-project/root/pull/229:248,performance,parallel,parallel,248,"Fix TMVA notebooks, parallelize notebooks; TMVA notebooks were fixed. A script that creates the notebooks in parallel is introduced. Note that as of this commit the notebooks are being created twice, the old way via the doxygen filter, and the new parallel way. Once it is ensured that this method works, the old method will be removed. The command for running the script was placed in the doxygen target because if it was made its own target, the necessary folders would not have been created yet.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/229
https://github.com/root-project/root/pull/229:341,usability,command,command,341,"Fix TMVA notebooks, parallelize notebooks; TMVA notebooks were fixed. A script that creates the notebooks in parallel is introduced. Note that as of this commit the notebooks are being created twice, the old way via the doxygen filter, and the new parallel way. Once it is ensured that this method works, the old method will be removed. The command for running the script was placed in the doxygen target because if it was made its own target, the necessary folders would not have been created yet.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/229
https://github.com/root-project/root/pull/230:33,deployability,Version,Version,33,Add new mathjax tarball for doc. Version 2.6.1; @couet .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/230
https://github.com/root-project/root/pull/230:33,integrability,Version,Version,33,Add new mathjax tarball for doc. Version 2.6.1; @couet .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/230
https://github.com/root-project/root/pull/230:33,modifiability,Version,Version,33,Add new mathjax tarball for doc. Version 2.6.1; @couet .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/230
https://github.com/root-project/root/pull/231:0,energy efficiency,GPU,GPU,0,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/231
https://github.com/root-project/root/pull/231:14,energy efficiency,core,core,14,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/231
https://github.com/root-project/root/pull/231:0,performance,GPU,GPU,0,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/231
https://github.com/root-project/root/pull/231:38,performance,network,networks,38,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/231
https://github.com/root-project/root/pull/231:38,security,network,networks,38,GPU and multi-core accelerated neural networks in TMVA.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/231
https://github.com/root-project/root/pull/232:259,deployability,build,builds,259,"Implement fixes and changes suggested; @couet . /cc @karies . - Rearrange main doc block. - Hiding of axis labels / titles / ticks is different now ( ""A"" instead of setting sizes to 0 on h1), fit drawing is handled explicitly by the class. - copy constructor builds TObject explicit. - a couple of methods were inlined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/232
https://github.com/root-project/root/pull/232:196,energy efficiency,draw,drawing,196,"Implement fixes and changes suggested; @couet . /cc @karies . - Rearrange main doc block. - Hiding of axis labels / titles / ticks is different now ( ""A"" instead of setting sizes to 0 on h1), fit drawing is handled explicitly by the class. - copy constructor builds TObject explicit. - a couple of methods were inlined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/232
https://github.com/root-project/root/pull/232:288,integrability,coupl,couple,288,"Implement fixes and changes suggested; @couet . /cc @karies . - Rearrange main doc block. - Hiding of axis labels / titles / ticks is different now ( ""A"" instead of setting sizes to 0 on h1), fit drawing is handled explicitly by the class. - copy constructor builds TObject explicit. - a couple of methods were inlined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/232
https://github.com/root-project/root/pull/232:288,modifiability,coupl,couple,288,"Implement fixes and changes suggested; @couet . /cc @karies . - Rearrange main doc block. - Hiding of axis labels / titles / ticks is different now ( ""A"" instead of setting sizes to 0 on h1), fit drawing is handled explicitly by the class. - copy constructor builds TObject explicit. - a couple of methods were inlined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/232
https://github.com/root-project/root/pull/232:288,testability,coupl,couple,288,"Implement fixes and changes suggested; @couet . /cc @karies . - Rearrange main doc block. - Hiding of axis labels / titles / ticks is different now ( ""A"" instead of setting sizes to 0 on h1), fit drawing is handled explicitly by the class. - copy constructor builds TObject explicit. - a couple of methods were inlined.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/232
https://github.com/root-project/root/pull/233:18,safety,test,tests,18,"Converttonotebook tests, ratioplot notebooks; Tests and documentation for the converttonotebook script are improved. Also, the new ratioplot tutorials were converted to notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/233
https://github.com/root-project/root/pull/233:46,safety,Test,Tests,46,"Converttonotebook tests, ratioplot notebooks; Tests and documentation for the converttonotebook script are improved. Also, the new ratioplot tutorials were converted to notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/233
https://github.com/root-project/root/pull/233:18,testability,test,tests,18,"Converttonotebook tests, ratioplot notebooks; Tests and documentation for the converttonotebook script are improved. Also, the new ratioplot tutorials were converted to notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/233
https://github.com/root-project/root/pull/233:46,testability,Test,Tests,46,"Converttonotebook tests, ratioplot notebooks; Tests and documentation for the converttonotebook script are improved. Also, the new ratioplot tutorials were converted to notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/233
https://github.com/root-project/root/pull/233:56,usability,document,documentation,56,"Converttonotebook tests, ratioplot notebooks; Tests and documentation for the converttonotebook script are improved. Also, the new ratioplot tutorials were converted to notebooks.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/233
https://github.com/root-project/root/pull/234:165,usability,interact,interactively,165,"Additional improvements on TRatioPlot; - margin on the outer side of the pads (called ""inset""). - = delete. - cleanup. - use option I on graph -> make axes editable interactively!",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/234
https://github.com/root-project/root/pull/235:57,usability,clear,clear,57,add getters for the output of the calculation;  and to clear the TLines instances used for the dashed lines. This is needed to trigger recreation after a pad clear from the outside. @couet .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/235
https://github.com/root-project/root/pull/235:160,usability,clear,clear,160,add getters for the output of the calculation;  and to clear the TLines instances used for the dashed lines. This is needed to trigger recreation after a pad clear from the outside. @couet .,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/235
https://github.com/root-project/root/pull/236:28,integrability,interfac,interface,28,Initial python command line interface to TSimpleAnalysis.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/236
https://github.com/root-project/root/pull/236:28,interoperability,interfac,interface,28,Initial python command line interface to TSimpleAnalysis.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/236
https://github.com/root-project/root/pull/236:28,modifiability,interfac,interface,28,Initial python command line interface to TSimpleAnalysis.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/236
https://github.com/root-project/root/pull/236:15,usability,command,command,15,Initial python command line interface to TSimpleAnalysis.;,MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/236
https://github.com/root-project/root/pull/238:278,availability,down,down,278,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:0,deployability,Updat,Update,0,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:173,deployability,observ,observed,173,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:14,energy efficiency,optim,optimization,14,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:222,energy efficiency,optim,optimization,222,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:241,energy efficiency,optim,optimal,241,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:721,modifiability,scal,scaling,721,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:14,performance,optimiz,optimization,14,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:222,performance,optimiz,optimization,222,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:698,reliability,doe,doesn,698,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:0,safety,Updat,Update,0,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:946,safety,Test,Tests,946,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:0,security,Updat,Update,0,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:173,testability,observ,observed,173,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:191,testability,simpl,simple,191,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:394,testability,simpl,simple,394,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:946,testability,Test,Tests,946,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:955,testability,simpl,simple,955,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:127,usability,learn,learning,127,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:191,usability,simpl,simple,191,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:394,usability,simpl,simple,394,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/238:955,usability,simpl,simple,955,"Update basket optimization; This is work done by Alex Saperstein, and ANL SULI who worked with me. While working on TTreeCache learning, our Summer Intern (Alex Saperstein) observed that for simple TTrees, the basket size optimization isnt optimal for two reasons: 1) rounding down to 512 byte blocks 2) neglecting to accommodate for ROOT offsets stored in the baskets. As a result, e.g. with simple (constant size) float array branches the basket size is to small resulting in two baskets per auto-flush. The change would be pretty straight-forward: tree/tree/src/TTree.cxx. Line. -6583 newBsize = newBsize - newBsize%512;. Should become:. +6583 if (pass) { // only on the second pass so that it doesn't interfere with scaling. +6583 Int_t nevbuf = branch->GetBasket(0)->GetNevBuf();. +6583 newBsize = newBsize + (nevbuf \* sizeof(Int_t) \* 2); // make room for meta data. +6583 newBsize = newBsize - newBsize%512 + 512; // rounds up. +6583 }. Tests on simple data show that with this the baskets end up more appropriately sized so that all the auto-flush data fits.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/238
https://github.com/root-project/root/pull/239:309,modifiability,variab,variables,309,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/239:320,safety,Test,Tested,320,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/239:462,safety,except,except,462,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/239:451,security,ident,identical,451,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/239:320,testability,Test,Tested,320,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/239:151,usability,behavi,behaviour,151,"Parse arithmetic expressions in GDML files again.; For GDML files without arithmetic expressions in them this should be just as fast and give the same behaviour as before. Parsing is implemented by TFormula, including a pre-processing step to mark constant names with ""[]"" so that TFormula recognizes them as variables. Tested by importing and re-exporting a GDML file used by the NOvA experiment in root5 and root6 and checking that the results were identical, except for the naming of the volumes.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/239
https://github.com/root-project/root/pull/240:389,availability,cluster,cluster,389,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:461,availability,operat,operations,461,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:1107,availability,operat,operation,1107,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:138,deployability,fail,fails,138,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:251,deployability,automat,automatically,251,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:389,deployability,cluster,cluster,389,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:105,energy efficiency,optim,optimization,105,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:277,energy efficiency,current,current,277,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:550,energy efficiency,optim,optimization,550,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:1103,energy efficiency,CPU,CPU,1103,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:1205,energy efficiency,optim,optimization,1205,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:663,integrability,event,event,663,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:717,integrability,filter,filter,717,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:945,modifiability,layer,layer,945,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
https://github.com/root-project/root/pull/240:16,performance,cach,cache,16,"Implement ""miss cache"" for TTreeCache; The _miss cache_, implemented in this pull request, implements an optimization when the TTreeCache fails to work. The miss cache will keep track of any branch that has been accessed; when there is a TTC miss, it automatically fetches the current basket for all active branches. This should have a worst case read size equal to the size of the file's cluster size, but potentially a significant savings in the number of IO operations. The latter is extremely useful if we're doing IO on high-latency links. This optimization works well for the ""trigger pattern,"" where the user may examine a number of branches and, when the event contents for those branches passes a particular filter, reads out the remaining branches. If there are 100 additional branches, this would do all reads in a single network round-trip as opposed to 100 round trips. The approach has served us well in CMS and been utilized as a layer on top of ROOT for about 3 years. Unfortunately, we must iterate through a set of branches and find the correct basket. This is not necessarily a cheap CPU operation and may be too expensive if the underlying filesystem is SSD-based. Hence, we turn this optimization off by default.",MatchSource.ISSUE,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/240
