id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/435:31,testability,log,log,31,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:200,usability,error,error,200,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:283,usability,error,error,283,"Hi Alex,. I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error. TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following. ### Load Data. x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0). ### Drop DAPI. x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1). ### Convert to AnnData. adata = sc.AnnData(x). ### Filter cells. sc.pp.filter_cells(adata, min_genes=1). sc.pp.filter_genes(adata, min_cells=1). adata.obs['n_counts'] = adata.X.sum(axis=1). ### Normalize data. sc.pp.log1p(adata). ### PCA. sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata). sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:24,availability,error,error,24,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:64,availability,error,error,64,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:24,performance,error,error,24,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:64,performance,error,error,64,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:24,safety,error,error,24,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:64,safety,error,error,64,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:30,testability,trace,traceback,30,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:24,usability,error,error,24,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:64,usability,error,error,64,Could you post the full error traceback so that I see where the error is raised?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,availability,error,error,23,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1670,availability,sli,sliced,1670,"_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._la",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:234,deployability,modul,module,234,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:299,deployability,Continu,Continuum,299,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:554,deployability,Continu,Continuum,554,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1142,deployability,Continu,Continuum,1142,"----. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1533,deployability,Continu,Continuum,1533,"~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1804,deployability,Continu,Continuum,1804,"legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\bas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2130,deployability,Continu,Continuum,2130,"\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2491,deployability,Continu,Continuum,2491,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2760,deployability,Continu,Continuum,2760,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:3020,deployability,Continu,Continuum,3020,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:750,integrability,compon,components,750,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:750,interoperability,compon,components,750,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:234,modifiability,modul,module,234,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:328,modifiability,pac,packages,328,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:583,modifiability,pac,packages,583,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:750,modifiability,compon,components,750,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1171,modifiability,pac,packages,1171,"ost recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1562,modifiability,pac,packages,1562,"aconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1833,modifiability,pac,packages,1833,"weight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2159,modifiability,pac,packages,2159,"ib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2232,modifiability,layer,layers,2232,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2520,modifiability,pac,packages,2520,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2789,modifiability,pac,packages,2789,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:3049,modifiability,pac,packages,3049,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self). 750 shape = (. 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),. --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars). 753 ). 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l). 148 return 1. 149 else:. --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,performance,error,error,23,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1670,reliability,sli,sliced,1670,"_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx). 723 self._X = None. 724 else:. --> 725 self._init_X_as_view(). 726 . 727 self._la",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,safety,error,error,23,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:207,safety,input,input-,207,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:234,safety,modul,module,234,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:163,testability,Trace,Traceback,163,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,usability,error,error,23,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:207,usability,input,input-,207,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:353,usability,tool,tools,353,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:608,usability,tool,tools,608,"Hi Alex,. Below is the error I get. Thank you for looking at this. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1196,usability,tool,tools,1196,"ipython-input-12-4fad8adf5d00> in <module>. ----> 1 sc.pl.pca(adata, color='CD3D'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in pca(adata, **kwargs). 148 If `show==False` a `matplotlib.Axis` or a list of it. 149 """""". --> 150 return plot_scatter(adata, basis='pca', **kwargs). 151 . 152 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 275 color_vector, categorical = _get_color_values(adata, value_to_plot,. 276 groups=groups, palette=palette,. --> 277 use_raw=use_raw). 278 . 279 # check if higher value points should be plot on top. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\tools\scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw). 658 # check if value to plot is in var. 659 elif use_raw is False and value_to_plot in adata.var_names:. --> 660 color_vector = adata[:, value_to_plot].X. 661 . 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 1307 def __getitem__(self, index):. 1308 """"""Returns a sliced view of the object."""""". -> 1309 return self._getitem_view(index). 1310 . 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1311 def _getitem_view(self, index):. 1312 oidx, vidx = self._normalize_indices(index). -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1314 . 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:44,availability,down,download,44,Sorry for all the trouble. I just wanted to download from your dropbox link but the file wasn't there anymore...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:26,availability,error,error,26,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:386,availability,down,downsampling,386,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:267,deployability,log,log,267,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:271,integrability,transform,transformation,271,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:271,interoperability,transform,transformation,271,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:26,performance,error,error,26,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:26,safety,error,error,26,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:267,safety,log,log,267,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:289,safety,compl,complaining,289,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:267,security,log,log,267,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:289,security,compl,complaining,289,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:267,testability,log,log,267,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:26,usability,error,error,26,"So I just reproduced this error for `sc.pp.log1p()` using my own data after using the `sc.pp.downsample_counts()` function. It might have to do with that? i noticed that `sc.pp.downsample_counts()` returns `np.int64` rather than `np.float64` I reckon that's what the log transformation is complaining about. If I add the line:. ```. adata.X = adata.X.astype(np.float64). ```. after the downsampling call, it works again. Maybe add that to `sc.pp.log1p()`? Or change `sc.pp.downsample_counts()` to return `np.float64`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:152,deployability,version,version,152,It was related to adata conversion that @falexwolf alluded to and specifically affects windows machines (because of changes in numpy). I got the latest version of AnnData and it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:152,integrability,version,version,152,It was related to adata conversion that @falexwolf alluded to and specifically affects windows machines (because of changes in numpy). I got the latest version of AnnData and it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:24,interoperability,convers,conversion,24,It was related to adata conversion that @falexwolf alluded to and specifically affects windows machines (because of changes in numpy). I got the latest version of AnnData and it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:66,interoperability,specif,specifically,66,It was related to adata conversion that @falexwolf alluded to and specifically affects windows machines (because of changes in numpy). I got the latest version of AnnData and it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:152,modifiability,version,version,152,It was related to adata conversion that @falexwolf alluded to and specifically affects windows machines (because of changes in numpy). I got the latest version of AnnData and it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:17,availability,error,error,17,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,integrability,messag,message,23,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:23,interoperability,messag,message,23,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:17,performance,error,error,17,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:17,safety,error,error,17,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:44,safety,input,inputting,44,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:17,usability,error,error,17,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:44,usability,input,inputting,44,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:41,deployability,updat,updated,41,Aha okay. My problem was resolved when I updated the AnnData package for converting pandas dataframe into AnnData object using. '''adata = sc.AnnData(x)''',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:61,modifiability,pac,package,61,Aha okay. My problem was resolved when I updated the AnnData package for converting pandas dataframe into AnnData object using. '''adata = sc.AnnData(x)''',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:41,safety,updat,updated,41,Aha okay. My problem was resolved when I updated the AnnData package for converting pandas dataframe into AnnData object using. '''adata = sc.AnnData(x)''',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:41,security,updat,updated,41,Aha okay. My problem was resolved when I updated the AnnData package for converting pandas dataframe into AnnData object using. '''adata = sc.AnnData(x)''',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:124,availability,down,downsampling,124,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:18,deployability,updat,update,18,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:18,safety,updat,update,18,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:18,security,updat,update,18,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:25,energy efficiency,current,currently,25,"I can confirm that it is currently. <img width=""1124"" alt=""Screen Shot 2019-03-22 at 23 25 14"" src=""https://user-images.githubusercontent.com/13019956/54856264-e4c20900-4cf9-11e9-88f0-15a9ace275f1.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:6,usability,confirm,confirm,6,"I can confirm that it is currently. <img width=""1124"" alt=""Screen Shot 2019-03-22 at 23 25 14"" src=""https://user-images.githubusercontent.com/13019956/54856264-e4c20900-4cf9-11e9-88f0-15a9ace275f1.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:108,usability,user,user-images,108,"I can confirm that it is currently. <img width=""1124"" alt=""Screen Shot 2019-03-22 at 23 25 14"" src=""https://user-images.githubusercontent.com/13019956/54856264-e4c20900-4cf9-11e9-88f0-15a9ace275f1.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:288,availability,down,downsampling,288,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2277,availability,down,downstream,2277,"sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:62,deployability,fail,fail,62,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:226,deployability,fail,failing,226,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:762,deployability,Log,Logarithmize,762,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:806,deployability,log,log,806,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:826,deployability,log,log,826,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:851,deployability,log,logarithm,851,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1208,deployability,updat,updates,1208," a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1224,deployability,depend,depending,1224,"ling inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1224,integrability,depend,depending,1224,"ling inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:862,modifiability,Paramet,Parameters,862,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1224,modifiability,depend,depending,1224,"ling inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:360,performance,time,time,360,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1451,performance,perform,perform,1451,"sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:2229,performance,time,time,2229,"sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:62,reliability,fail,fail,62,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:226,reliability,fail,failing,226,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:762,safety,Log,Logarithmize,762,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:806,safety,log,log,806,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:826,safety,log,log,826,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:851,safety,log,logarithm,851,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1208,safety,updat,updates,1208," a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1224,safety,depend,depending,1224,"ling inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:583,security,control,control,583,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:610,security,control,control,610,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:762,security,Log,Logarithmize,762,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:806,security,log,log,806,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:826,security,log,log,826,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:851,security,log,logarithm,851,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1208,security,updat,updates,1208," a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:583,testability,control,control,583,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:610,testability,control,control,610,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:762,testability,Log,Logarithmize,762,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:806,testability,log,log,806,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:826,testability,log,log,826,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:851,testability,log,logarithm,851,"No matter what it returns, it definitely shouldn't make stuff fail. I think that `downsample_counts` was returning integers before the most recent PR as well. iirc, I made `downsample_counts` use integers because a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_count",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1224,testability,depend,depending,1224,"ling inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1451,usability,perform,perform,1451,"sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>. <summary> 🍝 </summary>. ```python. def log1p(data, copy=False, chunked=False, chunk_size=None):. """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters. ----------. data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond. to cells and columns to genes. copy : `bool`, optional (default: `False`). If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. Returns. -------. Returns or updates `data`, depending on `copy`. """""". if copy:. if not isinstance(data, AnnData):. data = data.astype(np.floating). data = data.copy(). elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):. raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):. if issparse(X):. np.log1p(X.data, out=X.data). else:. np.log1p(X, out=X). return X. if isinstance(data, AnnData):. if not np.issubdtype(data.X.dtype, np.floating):. data.X = data.X.astype(np.floating, copy=False). if chunked:. for chunk, start, end in data.chunked_X(chunk_size):. data.X[start:end] = _log1p(chunk). else:. _log1p(data.X). else:. _log1p(data). return data if copy else None. ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:152,safety,input,input,152,"Originally everything was `np.float32` in scanpy, but as of a recent anndata commit (somewhere between 0.6.11 and 0.6.12), that was changed and now the input data precision is left up to the user. Which functions hard code `np.float32`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:152,usability,input,input,152,"Originally everything was `np.float32` in scanpy, but as of a recent anndata commit (somewhere between 0.6.11 and 0.6.12), that was changed and now the input data precision is left up to the user. Which functions hard code `np.float32`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:191,usability,user,user,191,"Originally everything was `np.float32` in scanpy, but as of a recent anndata commit (somewhere between 0.6.11 and 0.6.12), that was changed and now the input data precision is left up to the user. Which functions hard code `np.float32`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:319,deployability,log,logarithmize,319,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:165,integrability,transform,transforming,165,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:212,integrability,transform,transformed,212,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:165,interoperability,transform,transforming,165,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:212,interoperability,transform,transformed,212,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:405,modifiability,paramet,parameter,405,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:106,performance,time,time,106,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:289,safety,except,except,289,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:319,safety,log,logarithmize,319,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:18,security,hardcod,hardcoded,18,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:319,security,log,logarithmize,319,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:319,testability,log,logarithmize,319,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:140,usability,learn,learn,140,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:277,usability,user,user,277,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:33,availability,error,error,33,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:237,availability,error,error,237,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:693,deployability,modul,module,693,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:693,modifiability,modul,module,693,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:875,modifiability,pac,packages,875,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1012,modifiability,layer,layers,1012,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1325,modifiability,pac,packages,1325,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1698,modifiability,paramet,parameter,1698,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:33,performance,error,error,33,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:237,performance,error,error,237,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:33,safety,error,error,33,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:237,safety,error,error,237,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:665,safety,input,input-,665,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:693,safety,modul,module,693,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:484,testability,trace,traceback,484,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:621,testability,Trace,Traceback,621,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:33,usability,error,error,33,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:237,usability,error,error,237,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:665,usability,input,input-,665,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place. 	. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. Here's the traceback:. ```pytb. Normalizing counts per cell. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-136-3305b6c650f4> in <module>. 2 pbmc.X = pbmc.raw.X. 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500). ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace). 166 adata.obs[key_added] = counts_per_cell. 167 if hasattr(adata.X, '__itruediv__'):. --> 168 _normalize_data(adata.X, counts_per_cell, target_sum). 169 else:. 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy). 14 after = np.median(counts[counts>0]) if after is None else after. 15 counts += (counts == 0). ---> 16 counts /= after. 17 if issparse(X):. 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```. ```. >>> pbmc.X. <700x765 sparse matrix of type '<class 'numpy.int64'>'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:321,safety,input,input,321,"@dhb2128, as a workaround, this should work:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). pbmc.X = pbmc.X.astype(float). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. I've just opened a PR to fix `normalize_total` not working with integer input values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:321,usability,input,input,321,"@dhb2128, as a workaround, this should work:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc.X = pbmc.raw.X. sc.pp.downsample_counts(pbmc, counts_per_cell=500). pbmc.X = pbmc.X.astype(float). sc.pp.normalize_total(pbmc, target_sum=1e4). ```. I've just opened a PR to fix `normalize_total` not working with integer input values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:699,availability,error,error,699,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:854,availability,error,error,854,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1221,availability,operat,operation,1221,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1197,energy efficiency,draw,drawback,1197,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:84,interoperability,share,share,84,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:302,interoperability,specif,specifically,302,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:484,interoperability,specif,specifying,484,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:699,performance,error,error,699,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:854,performance,error,error,854,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:520,safety,input,input,520,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:699,safety,error,error,699,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:854,safety,error,error,854,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:867,safety,prevent,prevented,867,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:867,security,preven,prevented,867,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:520,usability,input,input,520,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:699,usability,error,error,699,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:854,usability,error,error,854,"Hi there,. stumbled on this by chance when debugging a similar problem - though I'd share my gained insight:. As @LuckyMD already pointed out [here](https://github.com/theislab/scanpy/issues/435#issuecomment-475722334), the root of the problem is feeding `np.int64` into `sc.preprocessing.log1p`. More specifically, the problem occurs in `log1p_array` [here](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318). When specifying `out` in `np.log1p`, the input types need to be castable. However, `np.log1p` returns `np.floatX` (type code double precision `'d'`) which cannot be cast to `np.int64` (type code long integer `'l'`). The error is reproducible with this small snippet of code:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). np.log1p(x=a, out=a). ```. The error can be prevented like this:. ```python. import numpy. a = np.zeros(shape=(1, 1), dtype='int64'). a = np.log1p(x=a). ```. In the case of `scanpy`, this would mean replacing [this](https://github.com/theislab/scanpy/blob/8829f2b80b7b347d4d933f3eaa1a8d959f35cd60/scanpy/preprocessing/_simple.py#L318) line of code by `X = np.log1p(X)`. The drawback being that the operation is no longer `inplace`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:149,interoperability,specif,specific,149,"@WeilerP I'm pretty sure that should work right now. I actually think issue this has been solved, but just wasn't closed. Here's an example for this specific case:. ```python. import scanpy as sc, numpy as np. from scipy import sparse. adata = sc.AnnData(. np.abs(sparse.random(100, 100, density=0.1, dtype=int, format=""csr"")),. dtype=int,. ). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.int64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. sc.pp.log1p(adata). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.float64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. ```. Basically, we just try to make `inplace` refer to the anndata object, and be truly inplace on the array when possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:312,interoperability,format,format,312,"@WeilerP I'm pretty sure that should work right now. I actually think issue this has been solved, but just wasn't closed. Here's an example for this specific case:. ```python. import scanpy as sc, numpy as np. from scipy import sparse. adata = sc.AnnData(. np.abs(sparse.random(100, 100, density=0.1, dtype=int, format=""csr"")),. dtype=int,. ). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.int64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. sc.pp.log1p(adata). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.float64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. ```. Basically, we just try to make `inplace` refer to the anndata object, and be truly inplace on the array when possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:474,interoperability,format,format,474,"@WeilerP I'm pretty sure that should work right now. I actually think issue this has been solved, but just wasn't closed. Here's an example for this specific case:. ```python. import scanpy as sc, numpy as np. from scipy import sparse. adata = sc.AnnData(. np.abs(sparse.random(100, 100, density=0.1, dtype=int, format=""csr"")),. dtype=int,. ). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.int64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. sc.pp.log1p(adata). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.float64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. ```. Basically, we just try to make `inplace` refer to the anndata object, and be truly inplace on the array when possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:635,interoperability,format,format,635,"@WeilerP I'm pretty sure that should work right now. I actually think issue this has been solved, but just wasn't closed. Here's an example for this specific case:. ```python. import scanpy as sc, numpy as np. from scipy import sparse. adata = sc.AnnData(. np.abs(sparse.random(100, 100, density=0.1, dtype=int, format=""csr"")),. dtype=int,. ). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.int64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. sc.pp.log1p(adata). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.float64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. ```. Basically, we just try to make `inplace` refer to the anndata object, and be truly inplace on the array when possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:114,usability,close,closed,114,"@WeilerP I'm pretty sure that should work right now. I actually think issue this has been solved, but just wasn't closed. Here's an example for this specific case:. ```python. import scanpy as sc, numpy as np. from scipy import sparse. adata = sc.AnnData(. np.abs(sparse.random(100, 100, density=0.1, dtype=int, format=""csr"")),. dtype=int,. ). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.int64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. sc.pp.log1p(adata). display(adata.X). # <100x100 sparse matrix of type '<class 'numpy.float64'>'. # 	with 1000 stored elements in Compressed Sparse Row format>. ```. Basically, we just try to make `inplace` refer to the anndata object, and be truly inplace on the array when possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:220,deployability,fail,fails,220,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:424,deployability,modul,module,424,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:496,integrability,wrap,wrapper,496,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:765,integrability,wrap,wrapper,765,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:496,interoperability,wrapper,wrapper,496,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:765,interoperability,wrapper,wrapper,765,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:424,modifiability,modul,module,424,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:594,modifiability,pac,packages,594,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:863,modifiability,pac,packages,863,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:1039,modifiability,paramet,parameter,1039,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:220,reliability,fail,fails,220,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:424,safety,modul,module,424,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:360,testability,Trace,Traceback,360,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO. Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash. >>> adata = sc.AnnData(. np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),. dtype=int,. ). >>> sc.pp.log1p(adata). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata. X = log1p(X, copy=False, base=base). File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper. return dispatch(args[0].__class__)(*args, **kw). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array. np.log1p(X, out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/436:157,deployability,api,api,157,"Yes, you're right, they are the z-scores used to calculate the p-values! I added a note in the docs to clarify this: https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.rank_genes_groups.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/436
https://github.com/scverse/scanpy/issues/436:157,integrability,api,api,157,"Yes, you're right, they are the z-scores used to calculate the p-values! I added a note in the docs to clarify this: https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.rank_genes_groups.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/436
https://github.com/scverse/scanpy/issues/436:157,interoperability,api,api,157,"Yes, you're right, they are the z-scores used to calculate the p-values! I added a note in the docs to clarify this: https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.rank_genes_groups.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/436
https://github.com/scverse/scanpy/issues/437:94,performance,time,time,94,"We already had this issue 2.5 years ago: https://github.com/theislab/scanpy/issues/36, at the time, of course, with a very different docs backend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437
https://github.com/scverse/scanpy/issues/437:23,integrability,configur,configurable,23,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437
https://github.com/scverse/scanpy/issues/437:23,modifiability,configur,configurable,23,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437
https://github.com/scverse/scanpy/issues/437:166,reliability,doe,does,166,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437
https://github.com/scverse/scanpy/issues/437:23,security,configur,configurable,23,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437
https://github.com/scverse/scanpy/issues/438:65,testability,trace,traceback,65,"Sorry, hard to figure out what's going on. Yes, please paste the traceback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:268,deployability,modul,module,268,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1339,deployability,log,log,1339,"ata_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1481,deployability,scale,scale,1481,"/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1691,deployability,log,log,1691,"genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1894,deployability,log,log,1894,"ot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/vari",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2864,deployability,contain,contain,2864,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2880,deployability,observ,observation,2880,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1481,energy efficiency,scale,scale,1481,"/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:401,integrability,batch,batch,401,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2928,interoperability,format,format,2928,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:268,modifiability,modul,module,268,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:469,modifiability,pac,packages,469,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:829,modifiability,pac,packages,829,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1258,modifiability,pac,packages,1258,"> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1449,modifiability,layer,layer,1449,"b/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1481,modifiability,scal,scale,1481,"/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1712,modifiability,layer,layer,1712,"1 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1718,modifiability,layer,layer,1718,"pby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1800,modifiability,pac,packages,1800,"scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1915,modifiability,layer,layer,1915,"roups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1957,modifiability,layer,layers,1957,"**kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return posit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1964,modifiability,layer,layer,1964,". 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.va",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2128,modifiability,pac,packages,2128,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2457,modifiability,pac,packages,2457,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2765,modifiability,pac,packages,2765,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2892,modifiability,variab,variables,2892,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:401,performance,batch,batch,401,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1481,performance,scale,scale,1481,"/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:241,safety,input,input-,241,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:268,safety,modul,module,268,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1339,safety,log,log,1339,"ata_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1691,safety,log,log,1691,"genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1894,safety,log,log,1894,"ot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/vari",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1339,security,log,log,1339,"ata_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1691,security,log,log,1691,"genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1894,security,log,log,1894,"ot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/vari",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:91,testability,trace,traceback,91,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:197,testability,Trace,Traceback,197,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1339,testability,log,log,1339,"ata_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1691,testability,log,log,1691,"genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1894,testability,log,log,1894,"ot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/vari",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2880,testability,observ,observation,2880,"olin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 510 . 511 def __getitem__(self, index):. --> 512 oidx, vidx = self._normalize_indices(index). 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). 538 obs, var = super(Raw, self)._unpack_index(packed_index). 539 obs = _normalize_index(obs, self._adata.obs_names). --> 540 var = _normalize_index(var, self.var_names). 541 return obs, var. 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). 270 raise KeyError(. 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 272 .format(index)). 273 return positions.values. 274 else:. ```. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:241,usability,input,input-,241,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:297,usability,tool,tools,297,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:494,usability,tool,tools,494,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:854,usability,tool,tools,854,"Yeah it is pretty cryptical, but I hoped it happened already to some of you :) Here is the traceback. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-25-c1200217276b> in <module>. ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). 439 . 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). 442 . 443 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 305 from ..anndata import stacked_violin. 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). 308 . 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 819 if isinstance(var_names, str):. 820 var_names = [var_names]. --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 822 . 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:519,deployability,modul,module,519,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1623,deployability,log,log,1623,"pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1765,deployability,scale,scale,1765,"y in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1981,deployability,log,log,1981,"upby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2192,deployability,log,log,2192," n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3209,deployability,contain,contain,3209,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3225,deployability,observ,observation,3225,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1765,energy efficiency,scale,scale,1765,"y in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:654,integrability,batch,batch,654,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3409,integrability,sub,subscribed,3409,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3275,interoperability,format,format,3275,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:519,modifiability,modul,module,519,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:727,modifiability,pac,packages,727,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1099,modifiability,pac,packages,1099," you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1542,modifiability,pac,packages,1542,"ing.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1733,modifiability,layer,layer,1733,"scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1765,modifiability,scal,scale,1765,"y in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2002,modifiability,layer,layer,2002," show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2008,modifiability,layer,layer,2008,"show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2098,modifiability,pac,packages,2098,"kages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2213,modifiability,layer,layer,2213,", show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2257,modifiability,layer,layers,2257,"import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2264,modifiability,layer,layer,2264,"stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2441,modifiability,pac,packages,2441,"). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2784,modifiability,pac,packages,2784,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3106,modifiability,pac,packages,3106,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3237,modifiability,variab,variables,3237,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:654,performance,batch,batch,654,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1765,performance,scale,scale,1765,"y in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:492,safety,input,input-,492,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:519,safety,modul,module,519,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1623,safety,log,log,1623,"pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1981,safety,log,log,1981,"upby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2192,safety,log,log,2192," n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1623,security,log,log,1623,"pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1981,security,log,log,1981,"upby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2192,security,log,log,2192," n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3632,security,auth,auth,3632,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:342,testability,trace,traceback,342,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:446,testability,Trace,Traceback,446,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1623,testability,log,log,1623,"pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1981,testability,log,log,1981,"upby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:2192,testability,log,log,2192," n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:3225,testability,observ,observation,3225,"_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). > 1983 matrix = adata[:, var_names].layers[layer]. > 1984 elif use_raw:. > -> 1985 matrix = adata.raw[:, var_names].X. > 1986 else:. > 1987 matrix = adata[:, var_names].X. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). > 510. > 511 def __getitem__(self, index):. > --> 512 oidx, vidx = self._normalize_indices(index). > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. > 514 else: X = self._adata.file['raw.X'][oidx, vidx]. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index). > 538 obs, var = super(Raw, self)._unpack_index(packed_index). > 539 obs = _normalize_index(obs, self._adata.obs_names). > --> 540 var = _normalize_index(var, self.var_names). > 541 return obs, var. > 542. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names). > 270 raise KeyError(. > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'. > --> 272 .format(index)). > 273 return positions.values. > 274 else:. >. > Cheers,. > Samuele. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:492,usability,input,input-,492,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:550,usability,tool,tools,550,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:752,usability,tool,tools,752,"Looks like if for some reason the gene names that you want to plot are not. in your anndata object. can you run `sc.pl.umap(all_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1124,usability,tool,tools,1124,"_data_flt_clst, color=['PRM2'])` ? On Wed, Jan 23, 2019 at 9:07 AM Samuele Soraggi <notifications@github.com>. wrote:. > Yeah it is pretty cryptical, but I hoped it happened already to some of. > you :) Here is the traceback. >. > ---------------------------------------------------------------------------. > KeyError Traceback (most recent call last). > <ipython-input-25-c1200217276b> in <module>. > ----> 1 sc.plotting.tools.rank_genes_groups_stacked_violin(all_data_bygroup, n_genes=5, save='.pdf', use_raw=True, groupby='batch', stripplot=True). >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in rank_genes_groups_stacked_violin(adata, groups, n_genes, groupby, key, show, save, **kwds). > 439. > 440 _rank_genes_groups_plot(adata, plot_type='stacked_violin', groups=groups, n_genes=n_genes,. > --> 441 groupby=groupby, key=key, show=show, save=save, **kwds). > 442. > 443. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). > 305 from ..anndata import stacked_violin. > 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,. > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds). > 308. > 309 elif plot_type == 'tracksplot':. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). > 819 if isinstance(var_names, str):. > 820 var_names = [var_names]. > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). > 822. > 823 if 'color' in kwds:. >. > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/annd",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:86,integrability,messag,message,86,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:86,interoperability,messag,message,86,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:251,interoperability,format,format,251,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:153,usability,visual,visualize,153,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:9,integrability,sub,subsetted,9,"Have you subsetted your AnnData object to highly variable genes, while keeping the full dataset in `.raw`? In that case it could be that genes that are found as markers via `rank_genes_groups`, are not in `adata.var_names`, but only in `adata.raw.var_names` and therefore cannot be found by the plotting function. I've previously encountered issues with this, but I thought it had been solved now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:49,modifiability,variab,variable,49,"Have you subsetted your AnnData object to highly variable genes, while keeping the full dataset in `.raw`? In that case it could be that genes that are found as markers via `rank_genes_groups`, are not in `adata.var_names`, but only in `adata.raw.var_names` and therefore cannot be found by the plotting function. I've previously encountered issues with this, but I thought it had been solved now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:236,deployability,contain,contain,236,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:53,integrability,sub,subsetted,53,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:119,interoperability,mismatch,mismatching,119,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:78,modifiability,variab,variable,78,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:211,modifiability,layer,layer,211,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:222,modifiability,layer,layers,222,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic. However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:50,modifiability,layer,layer,50,I'm not entirely sure what you mean by plotting a layer here. You mean that you run `rank_genes_groups` on a layer? Or is plotting a layer completely unrelated to `rank_genes_groups`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:109,modifiability,layer,layer,109,I'm not entirely sure what you mean by plotting a layer here. You mean that you run `rank_genes_groups` on a layer? Or is plotting a layer completely unrelated to `rank_genes_groups`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:133,modifiability,layer,layer,133,I'm not entirely sure what you mean by plotting a layer here. You mean that you run `rank_genes_groups` on a layer? Or is plotting a layer completely unrelated to `rank_genes_groups`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:139,safety,compl,completely,139,I'm not entirely sure what you mean by plotting a layer here. You mean that you run `rank_genes_groups` on a layer? Or is plotting a layer completely unrelated to `rank_genes_groups`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:139,security,compl,completely,139,I'm not entirely sure what you mean by plotting a layer here. You mean that you run `rank_genes_groups` on a layer? Or is plotting a layer completely unrelated to `rank_genes_groups`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:42,deployability,contain,contain,42,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:35,modifiability,layer,layers,35,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:133,modifiability,variab,variable,133,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:194,modifiability,layer,layers,194,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:251,modifiability,variab,variable,251,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:298,modifiability,variab,variable,298,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:404,usability,document,documentation,404,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:247,availability,sli,slight,247,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:273,availability,cluster,clustering,273,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:273,deployability,cluster,clustering,273,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:87,integrability,filter,filtering,87,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:212,integrability,filter,filter,212,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:302,performance,time,time,302,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:247,reliability,sli,slight,247,"Hi @SamueleSoraggi @fidelram . I came to the same problem---unable to find genes after filtering by HVGs. Did you figure it out and go through? . Or I can only use unfiltered matrix to plot? However, if I do not filter matrix with HVG, there is a slight difference between clustering and take a lot of time running `sc.tl.rank_genes_groups`. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:51,modifiability,paramet,parameter,51,"@MichaelPeibo how did you run the HVG? There is as parameter called `subset_data` or something like that, which will allow you to keep all genes and only flag the HVG.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:73,integrability,sub,subset,73,"Hi @fidelram . Thanks for pointing it out! I used default parameters of `subset=False` in `scanpy.pp.highly_variable_genes`. Actually, I think I found where the trick is. I used `.concatenate` to combine multiple datasets, and default parameter of `join` is `inner`, which will only filter common colomns/genes? Following this, I got some interesting genes missing when I did violin plot;. In the second time, I tried to set `join=outer` , which will keep more genes I think? Following this, I got some interesting genes found when I did violin plot;. Am I doing the meaningful or rationale way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:283,integrability,filter,filter,283,"Hi @fidelram . Thanks for pointing it out! I used default parameters of `subset=False` in `scanpy.pp.highly_variable_genes`. Actually, I think I found where the trick is. I used `.concatenate` to combine multiple datasets, and default parameter of `join` is `inner`, which will only filter common colomns/genes? Following this, I got some interesting genes missing when I did violin plot;. In the second time, I tried to set `join=outer` , which will keep more genes I think? Following this, I got some interesting genes found when I did violin plot;. Am I doing the meaningful or rationale way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:58,modifiability,paramet,parameters,58,"Hi @fidelram . Thanks for pointing it out! I used default parameters of `subset=False` in `scanpy.pp.highly_variable_genes`. Actually, I think I found where the trick is. I used `.concatenate` to combine multiple datasets, and default parameter of `join` is `inner`, which will only filter common colomns/genes? Following this, I got some interesting genes missing when I did violin plot;. In the second time, I tried to set `join=outer` , which will keep more genes I think? Following this, I got some interesting genes found when I did violin plot;. Am I doing the meaningful or rationale way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:235,modifiability,paramet,parameter,235,"Hi @fidelram . Thanks for pointing it out! I used default parameters of `subset=False` in `scanpy.pp.highly_variable_genes`. Actually, I think I found where the trick is. I used `.concatenate` to combine multiple datasets, and default parameter of `join` is `inner`, which will only filter common colomns/genes? Following this, I got some interesting genes missing when I did violin plot;. In the second time, I tried to set `join=outer` , which will keep more genes I think? Following this, I got some interesting genes found when I did violin plot;. Am I doing the meaningful or rationale way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:404,performance,time,time,404,"Hi @fidelram . Thanks for pointing it out! I used default parameters of `subset=False` in `scanpy.pp.highly_variable_genes`. Actually, I think I found where the trick is. I used `.concatenate` to combine multiple datasets, and default parameter of `join` is `inner`, which will only filter common colomns/genes? Following this, I got some interesting genes missing when I did violin plot;. In the second time, I tried to set `join=outer` , which will keep more genes I think? Following this, I got some interesting genes found when I did violin plot;. Am I doing the meaningful or rationale way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:123,modifiability,layer,layer,123,"I stilll encounter the problem, even when I set `adata.raw=adata`. However, it works with `use_raw=false` or by choosing a layer when I run the `scanpy.plotting.rank_genes_groups function` and does not when I use other plotting functions like `sc.plotting.rank_genes_groups_dotplot`. It is weird, I guess they are based on the same way of looking at the rank of genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:193,reliability,doe,does,193,"I stilll encounter the problem, even when I set `adata.raw=adata`. However, it works with `use_raw=false` or by choosing a layer when I run the `scanpy.plotting.rank_genes_groups function` and does not when I use other plotting functions like `sc.plotting.rank_genes_groups_dotplot`. It is weird, I guess they are based on the same way of looking at the rank of genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/pull/439:32,interoperability,distribut,distributed,32,BTW I have successfully run the distributed tests with this change (`pytest scanpy/tests/test_preprocessing_distributed.py`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:44,safety,test,tests,44,BTW I have successfully run the distributed tests with this change (`pytest scanpy/tests/test_preprocessing_distributed.py`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:83,safety,test,tests,83,BTW I have successfully run the distributed tests with this change (`pytest scanpy/tests/test_preprocessing_distributed.py`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:44,testability,test,tests,44,BTW I have successfully run the distributed tests with this change (`pytest scanpy/tests/test_preprocessing_distributed.py`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:83,testability,test,tests,83,BTW I have successfully run the distributed tests with this change (`pytest scanpy/tests/test_preprocessing_distributed.py`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:93,deployability,depend,dependencies,93,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:93,integrability,depend,dependencies,93,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:93,modifiability,depend,dependencies,93,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:6,safety,Test,Tests,6,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:93,safety,depend,dependencies,93,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:115,safety,test,tests,115,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:234,safety,test,tests,234,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:6,testability,Test,Tests,6,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:93,testability,depend,dependencies,93,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:115,testability,test,tests,115,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:234,testability,test,tests,234,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:75,deployability,instal,installed,75,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:88,deployability,instal,install,88,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:159,deployability,instal,install,159,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:195,deployability,instal,install,195,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:23,safety,test,tests,23,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:23,testability,test,tests,23,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```. pip install dask[array] zappy zarr. pip install pytest. ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:241,safety,reme,remember,241,"@flying-sheep: Do you agree that we should add this to the travis setup? I thought about creating a `requirements_tests.txt` as for `anndata` and simply adding the line to `.travis.yml`. Good solution? Maybe it's even your solution, I don't remember. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:146,testability,simpl,simply,146,"@flying-sheep: Do you agree that we should add this to the travis setup? I thought about creating a `requirements_tests.txt` as for `anndata` and simply adding the line to `.travis.yml`. Good solution? Maybe it's even your solution, I don't remember. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:146,usability,simpl,simply,146,"@flying-sheep: Do you agree that we should add this to the travis setup? I thought about creating a `requirements_tests.txt` as for `anndata` and simply adding the line to `.travis.yml`. Good solution? Maybe it's even your solution, I don't remember. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:48,deployability,instal,install,48,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:442,deployability,instal,installed,442,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:242,safety,test,test,242,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:218,testability,simpl,simply,218,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:242,testability,test,test,242,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:218,usability,simpl,simply,218,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:2,safety,compl,completely,2,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:48,safety,test,test,48,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:109,safety,test,tests,109,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:2,security,compl,completely,2,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:30,testability,simpl,simply,30,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:48,testability,test,test,48,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:109,testability,test,tests,109,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:30,usability,simpl,simply,30,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/issues/440:79,performance,cach,cache,79,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:143,performance,memor,memory,143,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:176,reliability,doe,does,176,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:272,testability,understand,understand,272,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:143,usability,memor,memory,143,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,. Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:86,testability,trace,traceback,86,"It should work and I've plotted using backed mode quite a bit. But you're right, your traceback suggests that something got broke.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:156,deployability,updat,update,156,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :). I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:18,performance,time,time,18,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :). I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:156,safety,updat,update,156,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :). I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:156,security,updat,update,156,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :). I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/pull/441:78,availability,slo,sloshed,78,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:78,reliability,slo,sloshed,78,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:112,safety,compl,completely,112,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:112,security,compl,completely,112,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:142,usability,user,user,142,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:203,usability,intuit,intuitive,203,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:294,availability,cluster,clustering,294,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:407,availability,cluster,cluster,407,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:119,deployability,version,version,119,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:151,deployability,manag,managed,151,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:294,deployability,cluster,clustering,294,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:407,deployability,cluster,cluster,407,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:151,energy efficiency,manag,managed,151,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:119,integrability,version,version,119,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:119,modifiability,version,version,119,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:484,performance,time,times,484,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:525,performance,time,times,525,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:151,safety,manag,managed,151,"Haha, this convention has nothing to do with destiny. :wink: Also the code in Scanpy solely started off Laleh's Matlab version. Unfortunately, I never managed to actually dig into the destiny code... The convention is the typical convention used since the early days of knn graphs and spectral clustering, hence I say ""canonical usage"". Fun that the [following reference](https://scholar.google.com/scholar?cluster=8671001140798821162&hl=en&as_sdt=0,5&sciodt=0,5), which is cited >6K times and which I've sent around 100s of times since I stumbled across it in fall 2015, isn't yet fully acknowledged in the field. We also put it into the DPT paper. It starts with outlining the ""different types of similarity graphs"" = the convention mentioned above. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:60,deployability,version,version,60,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:60,integrability,version,version,60,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:60,modifiability,version,version,60,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:112,usability,help,helping,112,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:142,usability,efficien,efficient,142,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:154,availability,operat,operate,154,"It's only more compact when you have a specific symmetric sparse matrix class. Otherwise it's pretty much the same. The difference is that it's easier to operate on an adjacency matrix for many tasks, while the dual matrix storage is only good for determining the nth nearest neighbor per cell directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:39,interoperability,specif,specific,39,"It's only more compact when you have a specific symmetric sparse matrix class. Otherwise it's pretty much the same. The difference is that it's easier to operate on an adjacency matrix for many tasks, while the dual matrix storage is only good for determining the nth nearest neighbor per cell directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,deployability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:86,energy efficiency,Cool,Cool,86,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,integrability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,interoperability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,modifiability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,reliability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,security,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:345,testability,integr,integration,345,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:45,usability,help,helping,45,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:75,usability,efficien,efficient,75,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/441:143,usability,efficien,efficient,143,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/442:182,availability,error,error,182,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:97,interoperability,specif,specifying,97,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:182,performance,error,error,182,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:182,safety,error,error,182,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:75,usability,behavi,behavior,75,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:182,usability,error,error,182,"I don’t consider it breaking. If I understod you right, the only change in behavior are that not specifying a genome works now in cases where there’s only one. No longer throwing an error is a perfectly fine change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:306,availability,error,error,306,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:501,availability,error,error,501,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:583,availability,error,error,583,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:203,deployability,contain,contains,203,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:363,deployability,contain,containing,363,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:35,integrability,coupl,couple,35,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:553,integrability,filter,filtered,553,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:182,interoperability,format,formatted,182,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:350,interoperability,format,format,350,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:35,modifiability,coupl,couple,35,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:306,performance,error,error,306,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:501,performance,error,error,501,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:583,performance,error,error,583,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:306,safety,error,error,306,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:501,safety,error,error,501,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:583,safety,error,error,583,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:35,testability,coupl,couple,35,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:42,usability,behavi,behavior,42,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:306,usability,error,error,306,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:501,usability,error,error,501,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:583,usability,error,error,583,"I thought it was breaking due to a couple behavior changes:. One case where the results would be different is the call `sc.read_10x_h5(h5pth)`, where the file at `h5pth` is a legacy formatted file which contains `mm10` and `hg38` genomes. Prior to this PR, only the `mm10` genome would be read in. Now, an error is thrown. . If the file had the `v3` format (also containing two genomes), now values for features from both genomes would be read in, instead of just `mm10`. Even better than removing an error, previously `v3` files would get all the vars filtered out and not throw an error!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:70,availability,replic,replicate,70,"OK! You explained that the behavior you have for v3 isn’t possible to replicate for legacy, so I’m sure this is fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:27,usability,behavi,behavior,27,"OK! You explained that the behavior you have for v3 isn’t possible to replicate for legacy, so I’m sure this is fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:28,availability,replic,replicate,28,"It's definitely possible to replicate, I don't know if it makes sense as a default. If the legacy files with multiple genomes almost always had the same cells aligned against all those genomes, it could make sense to read everything in by default. I'm not sure if that's the case though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:131,availability,error,error,131,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:115,interoperability,specif,specify,115,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:131,performance,error,error,131,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:42,reliability,doe,doesn,42,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:131,safety,error,error,131,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:102,usability,user,user,102,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:131,usability,error,error,131,"Ha, that’s what I meant, that you said it doesn’t make sense. > If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. I very much agree!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:172,deployability,releas,release,172,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:310,deployability,releas,release,310,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:329,integrability,batch,batches,329,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:346,integrability,sub,subversions,346,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:329,performance,batch,batches,329,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:362,security,auth,author,362,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:140,usability,behavi,behavior,140,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/issues/443:37,reliability,doe,does,37,It's like what cellranger and seurat does,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:254,integrability,sub,subscribed,254,"Can you post an example? Or a link to a page were this functionality is. shown? On Fri, Jan 25, 2019 at 2:14 AM jiawen wang <notifications@github.com>. wrote:. > It's like what cellranger and seurat does. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/443#issuecomment-457419267>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1V2EmItORhrwVZdJJQnR4rhWhlGwks5vGlpogaJpZM4aN6fm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:199,reliability,doe,does,199,"Can you post an example? Or a link to a page were this functionality is. shown? On Fri, Jan 25, 2019 at 2:14 AM jiawen wang <notifications@github.com>. wrote:. > It's like what cellranger and seurat does. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/443#issuecomment-457419267>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1V2EmItORhrwVZdJJQnR4rhWhlGwks5vGlpogaJpZM4aN6fm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:477,security,auth,auth,477,"Can you post an example? Or a link to a page were this functionality is. shown? On Fri, Jan 25, 2019 at 2:14 AM jiawen wang <notifications@github.com>. wrote:. > It's like what cellranger and seurat does. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/443#issuecomment-457419267>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1V2EmItORhrwVZdJJQnR4rhWhlGwks5vGlpogaJpZM4aN6fm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:100,modifiability,paramet,parameters,100,"A similar function corresponding to sc.pl in scanpy is FeaturePlot function in Seurat. The detailed parameters of this function are as follows:. FeaturePlot (object, features.plot, min.cutoff = NA, max.cutoff = NA, cols.use = c(""grey"", ""blue"")). expresssion value under min.cutoff will be shown as grey, higher than max.cutoff will be shown as gradually changing blue color. here is an example:. ![seurat](https://user-images.githubusercontent.com/29703450/51781961-775f8500-215b-11e9-9da3-87586b388bcb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:414,usability,user,user-images,414,"A similar function corresponding to sc.pl in scanpy is FeaturePlot function in Seurat. The detailed parameters of this function are as follows:. FeaturePlot (object, features.plot, min.cutoff = NA, max.cutoff = NA, cols.use = c(""grey"", ""blue"")). expresssion value under min.cutoff will be shown as grey, higher than max.cutoff will be shown as gradually changing blue color. here is an example:. ![seurat](https://user-images.githubusercontent.com/29703450/51781961-775f8500-215b-11e9-9da3-87586b388bcb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:24,modifiability,paramet,parameters,24,"@fidelram vmin and vmax parameters in sc.pl.umap() can meet my needs, there is no need to modify the function. sorry to trouble you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/443:90,security,modif,modify,90,"@fidelram vmin and vmax parameters in sc.pl.umap() can meet my needs, there is no need to modify the function. sorry to trouble you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/443
https://github.com/scverse/scanpy/issues/445:136,availability,avail,available,136,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:157,deployability,version,versions,157,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:157,integrability,version,versions,157,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:157,modifiability,version,versions,157,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:136,reliability,availab,available,136,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:36,safety,test,tests,36,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:136,safety,avail,available,136,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:461,safety,test,tests,461,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:136,security,availab,available,136,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:36,testability,test,tests,36,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:461,testability,test,tests,461,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/445:278,usability,learn,learn,278,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are. ```. scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/446:128,deployability,api,api,128,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:139,deployability,api,api,139,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:263,deployability,log,logfoldchange,263,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:339,energy efficiency,current,currently,339,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,integrability,api,api,128,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:139,integrability,api,api,139,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,interoperability,api,api,128,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:139,interoperability,api,api,139,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:231,safety,compl,completely,231,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:263,safety,log,logfoldchange,263,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:231,security,compl,completely,231,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:263,security,log,logfoldchange,263,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:263,testability,log,logfoldchange,263,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:466,usability,tool,tools,466,"Thank you for the kind words! Hm, it's in the [docs in the returns section](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.rank_genes_groups.html). Should it be somewhere else? But you're right, I'm also not completely happy with the name `logfoldchange`. We'll harmonize with what's out there at some point and I'm currently tending to `log2FC` because @davidsebfischer started to using that in `diffxpy` and I guess it's also used in some R tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:45,integrability,coupl,couple,45,"Whoops, sorry, I read that returns section a couple of times but somehow missed the `log2` bit! Apologies. . However, I agree with you that renaming the `log2FC` would remove any ambiguity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:45,modifiability,coupl,couple,45,"Whoops, sorry, I read that returns section a couple of times but somehow missed the `log2` bit! Apologies. . However, I agree with you that renaming the `log2FC` would remove any ambiguity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:55,performance,time,times,55,"Whoops, sorry, I read that returns section a couple of times but somehow missed the `log2` bit! Apologies. . However, I agree with you that renaming the `log2FC` would remove any ambiguity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:45,testability,coupl,couple,45,"Whoops, sorry, I read that returns section a couple of times but somehow missed the `log2` bit! Apologies. . However, I agree with you that renaming the `log2FC` would remove any ambiguity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:199,reliability,doe,does,199,"@falexwolf, yes I think overall fold changes with 2 as a basis are more intuitive than with e as a basis for most people, similar to basis=10, but basis 2 gives a more sensible dynamic range than 10 does on gene expression data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:72,usability,intuit,intuitive,72,"@falexwolf, yes I think overall fold changes with 2 as a basis are more intuitive than with e as a basis for most people, similar to basis=10, but basis 2 gives a more sensible dynamic range than 10 does on gene expression data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,deployability,log,logfoldchange,128,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:16,safety,compl,completely,16,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,safety,log,logfoldchange,128,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:16,security,compl,completely,16,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,security,log,logfoldchange,128,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:128,testability,log,logfoldchange,128,"Thanks David, I completely agree, but I was really just talking about the naming convention: `log2FC` vs. `log2foldchange` vs. `logfoldchange` vs. other possibilities... 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:89,usability,help,help,89,Hi all - I just saw this and I agree that `log2FC` is a better name. I would be happy to help with this small change if necessary!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/447:849,availability,cluster,clusters,849,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:1046,availability,cluster,cluster,1046,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:849,deployability,cluster,clusters,849,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:1046,deployability,cluster,cluster,1046,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:1517,integrability,sub,subscribed,1517,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:164,interoperability,coordinat,coordinates,164,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:376,interoperability,coordinat,coordinate,376,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:565,modifiability,pac,package,565,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:144,reliability,doe,does,144,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:722,reliability,doe,does,722,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:1714,security,auth,auth,1714,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:153,testability,understand,understand,153,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:180,testability,simpl,simple,180,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:388,testability,simpl,simple,388,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:180,usability,simpl,simple,180,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:388,usability,simpl,simple,388,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/447:1373,usability,user,user-images,1373,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track. but ist not because it does not understand coordinates. It simple groups. the cells by the given groupby condition and then plots the value of each. gene in a separate track. The y value is the gene expression (or in your. case the ATAC-seq value). The x coordinate, simple puts all cells one after. the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,. >. > Thanks for this amazing package. >. > I have been playing with scanpy on scATACSeq data generated from 10x. And. > in comparison to the cellranger analysis, I think analysis scanpy does. > pretty descent job and adds more possibilities. I would like to displays. > some peaks that are highly present if some clusters using the genome. > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is. > it possible to the same thing but with the peak averaged for all cells. > within the same cluster? >. > import matplotlib.pyplot as plt. > genes =['chr15:101708546_101718131','chr11:117961932_117970696',. > 'chr19:5821847_5852441','chr15:101422873_101429606',. > 'chr17:39842811_39849028','chr13:6108971_6109684']. > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]). >. > [image: atacseq]. > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/447>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447
https://github.com/scverse/scanpy/issues/448:28,integrability,coupl,couple,28,it started happening only a couple of days ago. @falexwolf is it because of pandas 0.2.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/448
https://github.com/scverse/scanpy/issues/448:28,modifiability,coupl,couple,28,it started happening only a couple of days ago. @falexwolf is it because of pandas 0.2.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/448
https://github.com/scverse/scanpy/issues/448:28,testability,coupl,couple,28,it started happening only a couple of days ago. @falexwolf is it because of pandas 0.2.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/448
https://github.com/scverse/scanpy/issues/448:19,security,ident,identifying,19,Thank you guys for identifying and fixing this! :smile:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/448
https://github.com/scverse/scanpy/issues/449:1553,availability,avail,available,1553,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1517,energy efficiency,current,currently,1517,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:295,integrability,protocol,protocol,295,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:522,integrability,sub,subset,522,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:738,integrability,sub,subset,738,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:875,integrability,sub,subset,875,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:905,integrability,Batch,Batch,905,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1031,integrability,batch,batch-correction,1031,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1433,integrability,topic,topics,1433,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1544,integrability,pub,publicly,1544,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:295,interoperability,protocol,protocol,295,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:343,interoperability,heterogen,heterogeneity,343,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:120,modifiability,variab,variable,120,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:626,modifiability,variab,variable,626,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:669,modifiability,variab,variable,669,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:833,modifiability,paramet,parameters,833,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:905,performance,Batch,Batch,905,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1031,performance,batch,batch-correction,1031,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:513,reliability,doe,does,513,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1451,reliability,pra,practices,1451,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1553,reliability,availab,available,1553,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:767,safety,input,input,767,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:857,safety,input,inputs,857,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1533,safety,review,review,1533,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1553,safety,avail,available,1553,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:63,security,auth,authors,63,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1553,security,availab,available,1553,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:1533,testability,review,review,1533,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:635,usability,indicat,indicating,635,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:767,usability,input,input,767,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:857,usability,input,inputs,857,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:95,modifiability,variab,variable,95,"Hi @LuckyMD, thanks for the recommendations we (@SharkieJones) will try lowering the number of variable genes (we ranked based on variance) and will look through the tutorials.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/450:144,availability,down,downgrading,144,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:102,deployability,upgrad,upgrading,102,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:133,deployability,version,version,133,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:202,deployability,version,version,202,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:133,integrability,version,version,133,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:202,integrability,version,version,202,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:102,modifiability,upgrad,upgrading,102,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:133,modifiability,version,version,133,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:202,modifiability,version,version,202,"Same problem here. So glad that I found this ticket. From flying-sheep's commit, it looks like either upgrading scanpy to the newest version or downgrading pandas would work. There is also some anndata version requirement going up, no idea why.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:566,availability,sli,slightly,566,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:307,modifiability,reu,reused,307,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:525,modifiability,variab,variable,525,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:566,reliability,sli,slightly,566,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:175,safety,test,tests,175,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:250,security,hardcod,hardcoded,250,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:175,testability,test,tests,175,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:585,usability,behavi,behavior,585,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file). 2. I pulled his changes, which covered the writing portion of the needed fixes. 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2. 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:64,availability,error,error,64,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1219,availability,sli,slice,1219,"put-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1847,availability,toler,tolerance,1847,"ries.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1969,availability,toler,tolerance,1969,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2130,availability,toler,tolerance,2130,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2277,availability,toler,tolerance,2277,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2287,availability,toler,tolerance,2287,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2501,availability,toler,tolerance,2501,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:247,deployability,modul,module,247,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:764,deployability,log,logg,764,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2852,deployability,instal,installed,2852,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:845,energy efficiency,core,core,845,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1083,energy efficiency,core,core,1083,"`. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1347,energy efficiency,core,core,1347,"terminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1705,energy efficiency,core,core,1705,"n_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueEr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2071,energy efficiency,core,core,2071,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2429,energy efficiency,core,core,2429,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:553,integrability,sub,subset,553,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:247,modifiability,modul,module,247,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:397,modifiability,pac,packages,397,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:829,modifiability,pac,packages,829,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1067,modifiability,pac,packages,1067,"han above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1331,modifiability,pac,packages,1331,"p=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1689,modifiability,pac,packages,1689,"er_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(tar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2055,modifiability,pac,packages,2055,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2413,modifiability,pac,packages,2413,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2770,modifiability,pac,packages,2770,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:64,performance,error,error,64,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1175,performance,reindex,reindex,1175,"ceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1365,performance,reindex,reindex,1365,"canpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1428,performance,reindex,reindex,1428,"y_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/cor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1455,performance,reindex,reindex,1455,"ly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1528,performance,reindex,reindex,1528,"nes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1724,performance,reindex,reindex,1724,". 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1763,performance,perform,perform,1763,"g.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1775,performance,reindex,reindex,1775,"pyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2226,performance,reindex,reindex,2226,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2457,performance,reindex,reindex,2457,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2587,performance,reindex,reindex,2587,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2719,performance,reindex,reindex,2719,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1219,reliability,sli,slice,1219,"put-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1847,reliability,toleran,tolerance,1847,"ries.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1969,reliability,toleran,tolerance,1969,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2130,reliability,toleran,tolerance,2130,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2277,reliability,toleran,tolerance,2277,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2287,reliability,toleran,tolerance,2287,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2501,reliability,toleran,tolerance,2501,"heck_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:64,safety,error,error,64,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:220,safety,input,input-,220,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:247,safety,modul,module,247,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:764,safety,log,logg,764,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1193,safety,except,except,1193,"nt call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1200,safety,Except,Exception,1200,"ast). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:764,security,log,logg,764,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:176,testability,Trace,Traceback,176,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:764,testability,log,logg,764,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:64,usability,error,error,64,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:220,usability,input,input-,220,"Dear All,. running the tutorial `pbmc3k.ipynb`. I get a similar error than above:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-ea8d9dc47463> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 115 # a normalized disperion of 1. 116 one_gene_per_bin = disp_std_bin.isnull(). --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(). 118 if len(gen_indices) > 0:. 119 logg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1763,usability,perform,perform,1763,"g.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key). 909 key = check_bool_indexer(self.index, key). 910 . --> 911 return self._get_with(key). 912 . 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key). 951 return self.loc[key]. 952 . --> 953 return self.reindex(key). 954 except Exception:. 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). 3732 @Appender(generic.NDFrame.reindex.__doc__). 3733 def reindex(self, index=None, **kwargs):. -> 3734 return super(Series, self).reindex(index=index, **kwargs). 3735 . 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4354 # perform the reindex on the axes. 4355 return self._reindex_axes(axes, level, limit, tolerance, method,. -> 4356 fill_value, copy).__finalize__(self). 4357 . 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4367 ax = self._get_axis(a). 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,. -> 4369 tolerance=tolerance, method=method). 4370 . 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance). 501 else:. 502 if not target.is_unique:. --> 503 raise ValueError(""cannot reindex with a non-unique indexer""). 504 . 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer. ```. These th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:30,deployability,instal,install,30,"Hi @helios,. You will have to install scanpy from github to use the fix for this. The latest release (1.3.7) does not yet include the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:93,deployability,releas,release,93,"Hi @helios,. You will have to install scanpy from github to use the fix for this. The latest release (1.3.7) does not yet include the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:109,reliability,doe,does,109,"Hi @helios,. You will have to install scanpy from github to use the fix for this. The latest release (1.3.7) does not yet include the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:54,usability,confirm,confirm,54,"Hi @LuckyMD,. thanks. I was doing exactly that. I can confirm that everything works as expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:74,deployability,releas,releases,74,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:169,deployability,releas,release,169,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:36,security,control,controlled,36,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:36,testability,control,controlled,36,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:228,deployability,releas,release,228,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:142,interoperability,coordinat,coordinate,142,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:29,safety,test,test,29,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:29,testability,test,test,29,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:34,testability,coverag,coverage,34,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:192,deployability,manag,manage,192,"I did, and then I realized that we have the `import scanpy as sc` change and more features, so I called it 1.4. Btw: could you please add me as owner to scanpy and anndata on PyPI? then I can manage releases and delete files on PyPI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:199,deployability,releas,releases,199,"I did, and then I realized that we have the `import scanpy as sc` change and more features, so I called it 1.4. Btw: could you please add me as owner to scanpy and anndata on PyPI? then I can manage releases and delete files on PyPI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:192,energy efficiency,manag,manage,192,"I did, and then I realized that we have the `import scanpy as sc` change and more features, so I called it 1.4. Btw: could you please add me as owner to scanpy and anndata on PyPI? then I can manage releases and delete files on PyPI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:192,safety,manag,manage,192,"I did, and then I realized that we have the `import scanpy as sc` change and more features, so I called it 1.4. Btw: could you please add me as owner to scanpy and anndata on PyPI? then I can manage releases and delete files on PyPI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:16,testability,plan,planned,16,"Wasn't entirely planned to have 1.4 exactly now, but, fine, we have some stuff already 😄 I briefly tweeted about it: https://twitter.com/falexwolf/status/1093140419997822978",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:147,usability,statu,status,147,"Wasn't entirely planned to have 1.4 exactly now, but, fine, we have some stuff already 😄 I briefly tweeted about it: https://twitter.com/falexwolf/status/1093140419997822978",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/453:44,deployability,contain,contain,44,sure. could you please edit your comment to contain a link to said discussion?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:183,availability,state,state,183,"I've edited my comment. I'd also be very in favor of the differential expression API getting reworked (#61). It's possible diffxpy should cover this, but I'm not really sure what the state of its development is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:81,deployability,API,API,81,"I've edited my comment. I'd also be very in favor of the differential expression API getting reworked (#61). It's possible diffxpy should cover this, but I'm not really sure what the state of its development is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:81,integrability,API,API,81,"I've edited my comment. I'd also be very in favor of the differential expression API getting reworked (#61). It's possible diffxpy should cover this, but I'm not really sure what the state of its development is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:183,integrability,state,state,183,"I've edited my comment. I'd also be very in favor of the differential expression API getting reworked (#61). It's possible diffxpy should cover this, but I'm not really sure what the state of its development is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:81,interoperability,API,API,81,"I've edited my comment. I'd also be very in favor of the differential expression API getting reworked (#61). It's possible diffxpy should cover this, but I'm not really sure what the state of its development is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:0,safety,Compl,Completely,0,"Completely agreed, @ivirshup! The `inplace` and `copy` change is not just about renaming. See my edit of the check box list above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:0,security,Compl,Completely,0,"Completely agreed, @ivirshup! The `inplace` and `copy` change is not just about renaming. See my edit of the check box list above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/454:46,deployability,version,versions,46,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:55,deployability,depend,depending,55,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:46,integrability,version,versions,46,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:55,integrability,depend,depending,55,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:72,interoperability,platform,platform,72,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:46,modifiability,version,versions,46,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:55,modifiability,depend,depending,55,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:55,safety,depend,depending,55,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:55,testability,depend,depending,55,"hmm, I’m not sure if it’s possible to require versions depending on the platform. where’s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,usability,close,closest,26,I think the issue on h5py closest to this might be [this](https://github.com/h5py/h5py/issues/1142). . I have not opened a seperate issue there yet. Do you think it is caused by scanpy or h5py?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:58,deployability,instal,install,58,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:39,testability,simpl,simply,39,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:39,usability,simpl,simply,39,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:96,deployability,instal,installed,96,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,deployability,depend,dependencies,122,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,integrability,depend,dependencies,122,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:42,modifiability,pac,package,42,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,modifiability,depend,dependencies,122,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,safety,depend,dependencies,122,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,testability,depend,dependencies,122,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies? https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:7,deployability,instal,install,7,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,deployability,depend,dependencies,69,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,integrability,depend,dependencies,69,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,modifiability,depend,dependencies,69,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,safety,depend,dependencies,69,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,testability,depend,dependencies,69,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:61,availability,error,error,61,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:84,deployability,instal,install,84,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:61,performance,error,error,61,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:61,safety,error,error,61,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:61,usability,error,error,61,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:35,availability,avail,availability,35,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:111,deployability,instal,install,111,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:14,modifiability,pac,package,14,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:22,reliability,doe,doesn,22,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:35,reliability,availab,availability,35,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:35,safety,avail,availability,35,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:35,security,availab,availability,35,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:7,deployability,instal,install,7,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:113,deployability,depend,dependency,113,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:113,integrability,depend,dependency,113,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:113,modifiability,depend,dependency,113,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:113,safety,depend,dependency,113,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:124,safety,compl,complains,124,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:124,security,compl,complains,124,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:113,testability,depend,dependency,113,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:92,deployability,instal,installation,92,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:114,deployability,instal,install,114,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:257,deployability,instal,installed,257,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:313,deployability,instal,install,313,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:374,deployability,instal,install,374,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:397,deployability,instal,install,397,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:421,deployability,instal,install,421,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:441,deployability,instal,install,441,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:468,deployability,instal,install,468,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:491,deployability,instal,install,491,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:526,deployability,instal,install,526,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:271,modifiability,pac,packages,271,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:353,safety,test,test,353,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:353,testability,test,test,353,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:180,usability,command,command,180,"I had a same issue. My environment is. ```. windows10. python3.8.8 (conda env). ```. scanpy installation . `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda. here is my install procedure. ```. conda create -n test python=3.8. pip install ipykernel. pip install jupyterlab. pip install scanpy. pip install python-igraph. pip install leidenalg. pip install fa2. ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:126,deployability,instal,installation,126,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:149,deployability,instal,install,149,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:304,deployability,instal,installed,304,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:362,deployability,instal,install,362,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:433,deployability,instal,install,433,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:458,deployability,instal,install,458,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:484,deployability,instal,install,484,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:506,deployability,instal,install,506,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:535,deployability,instal,install,535,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:560,deployability,instal,install,560,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:603,deployability,instal,install,603,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:318,modifiability,pac,packages,318,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:410,safety,test,test,410,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:410,testability,test,test,410,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:221,usability,command,command,221,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:717,usability,stop,stopped,717,"> . > . > I had a same issue. > . > My environment is. > . > ```. > windows10. > python3.8.8 (conda env). > ```. > . > scanpy installation. > `conda install -c conda-forge -c bioconda scanpy`. > . > It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). > . > To solve this, I just installed all packages using pip, not conda. > here is my install procedure. > . > ```. > conda create -n test python=3.8. > pip install ipykernel. > pip install jupyterlab. > pip install scanpy. > pip install python-igraph. > pip install leidenalg. > pip install fa2. > ```. > . > I tired a lot of install and environment combination, but always there was a problem with conda. Thanks! my scanpy was working but stopped reinstalling everything in a new environment again with pip got it working as you suggested .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:24,availability,error,error,24,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:122,availability,down,download,122,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:439,availability,down,downloaded,439,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:713,availability,down,download,713,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:286,deployability,instal,install,286,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:331,deployability,instal,install,331,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:594,deployability,build,build,594,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:771,deployability,build,build,771,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:822,deployability,instal,installing,822,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:24,performance,error,error,24,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:24,safety,error,error,24,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:24,usability,error,error,24,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:341,usability,user,user,341,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:777,usability,tool,tools,777,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:804,usability,Visual,Visual,804,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:954,usability,user,users,954,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,availability,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:130,availability,down,download,130,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:449,availability,down,downloaded,449,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:737,availability,down,download,737,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1017,availability,error,error,1017,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:296,deployability,instal,install,296,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:341,deployability,instal,install,341,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:618,deployability,build,build,618,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:795,deployability,build,build,795,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:846,deployability,instal,installing,846,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,performance,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1017,performance,error,error,1017,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,safety,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1017,safety,error,error,1017,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1043,safety,except,except,1043,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,usability,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:351,usability,user,user,351,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:801,usability,tool,tools,801,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:828,usability,Visual,Visual,828,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:978,usability,user,users,978,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1017,usability,error,error,1017,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system. thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:125,availability,down,download,125,"Had this issue again recently using python 3.7, and the solution above wasn't enough to solve it. Turns out I also needed to download the tables .whl file: `pip install .\h5py-2.10.0-cp37-cp37m-win_amd64.whl .\tables-3.6.1-cp37-cp37m-win_amd64.whl numpy==1.20.0 --user --force-reinstall`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:161,deployability,instal,install,161,"Had this issue again recently using python 3.7, and the solution above wasn't enough to solve it. Turns out I also needed to download the tables .whl file: `pip install .\h5py-2.10.0-cp37-cp37m-win_amd64.whl .\tables-3.6.1-cp37-cp37m-win_amd64.whl numpy==1.20.0 --user --force-reinstall`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:264,usability,user,user,264,"Had this issue again recently using python 3.7, and the solution above wasn't enough to solve it. Turns out I also needed to download the tables .whl file: `pip install .\h5py-2.10.0-cp37-cp37m-win_amd64.whl .\tables-3.6.1-cp37-cp37m-win_amd64.whl numpy==1.20.0 --user --force-reinstall`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,availability,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:130,availability,down,download,130,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:449,availability,down,downloaded,449,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:737,availability,down,download,737,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:296,deployability,instal,install,296,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:341,deployability,instal,install,341,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:618,deployability,build,build,618,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:795,deployability,build,build,795,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:846,deployability,instal,installing,846,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,performance,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,safety,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:26,usability,error,error,26,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:351,usability,user,user,351,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:801,usability,tool,tools,801,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:828,usability,Visual,Visual,828,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:978,usability,user,users,978,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:990,usability,help,helped,990,"> In case anyone has this error again, here is what worked for me:. > . > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > * scanpy should work now. > . > This worked on mine and also on a colleagues windows laptop. > . > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:19,availability,error,error,19,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:173,availability,error,error,173,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:69,deployability,instal,install,69,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:220,deployability,instal,install,220,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:304,modifiability,pac,packages,304,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:19,performance,error,error,19,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:173,performance,error,error,173,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:19,safety,error,error,19,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:173,safety,error,error,173,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:258,testability,spy,spyder,258,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:19,usability,error,error,19,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:158,usability,experien,experience,158,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:173,usability,error,error,173,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:33,availability,error,error,33,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:251,availability,error,error,251,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:359,availability,down,download,359,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:680,availability,down,downloaded,680,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:972,availability,down,download,972,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:78,deployability,instal,install,78,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:147,deployability,instal,install,147,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:194,deployability,version,version,194,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:527,deployability,instal,install,527,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:572,deployability,instal,install,572,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:853,deployability,build,build,853,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1030,deployability,build,build,1030,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1081,deployability,instal,installing,1081,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:177,integrability,sub,subsequent,177,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:194,integrability,version,version,194,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:202,interoperability,conflict,conflict,202,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:194,modifiability,version,version,194,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:33,performance,error,error,33,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:251,performance,error,error,251,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:33,safety,error,error,33,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:251,safety,error,error,251,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:33,usability,error,error,33,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:251,usability,error,error,251,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:582,usability,user,user,582,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1036,usability,tool,tools,1036,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1063,usability,Visual,Visual,1063,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1213,usability,user,users,1213,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:1231,usability,help,helped,1231,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:. > > . > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl. > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded). > > * scanpy should work now. > > . > > This worked on mine and also on a colleagues windows laptop. > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. > . > this helped me out as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/455:59,deployability,releas,release,59,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:113,deployability,version,version,113,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:144,deployability,releas,release,144,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:113,integrability,version,version,113,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:113,modifiability,version,version,113,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:153,modifiability,paramet,parameter,153,@ivirshup Just to clarify... would you not have to have `'Tnnt2'` either as a column in `.obs` or in `.var_names` if you want to use it with the `color` parameter? Or does `gene_symbols` also allow you to look for a `color` parameter in the `gene_symbols` `.var` column?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:224,modifiability,paramet,parameter,224,@ivirshup Just to clarify... would you not have to have `'Tnnt2'` either as a column in `.obs` or in `.var_names` if you want to use it with the `color` parameter? Or does `gene_symbols` also allow you to look for a `color` parameter in the `gene_symbols` `.var` column?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:167,reliability,doe,does,167,@ivirshup Just to clarify... would you not have to have `'Tnnt2'` either as a column in `.obs` or in `.var_names` if you want to use it with the `color` parameter? Or does `gene_symbols` also allow you to look for a `color` parameter in the `gene_symbols` `.var` column?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:14,energy efficiency,cool,cool,14,That's really cool! A nice additional functionality to what was first introduced in `pl.scatter`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:528,availability,error,error,528,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:111,deployability,continu,continue,111,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:630,deployability,modul,module,630,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1226,deployability,observ,observation,1226," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1273,deployability,observ,observation,1273," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1132,interoperability,format,format,1132," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:630,modifiability,modul,module,630,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:735,modifiability,pac,packages,735,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:891,modifiability,pac,packages,891,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1049,modifiability,pac,packages,1049,"the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, viol",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1252,modifiability,variab,variable,1252," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:528,performance,error,error,528,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1314,performance,time,timepoint,1314," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:528,safety,error,error,528,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:592,safety,input,input-,592,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:630,safety,modul,module,630,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1220,safety,valid,valid,1220," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1267,safety,Valid,Valid,1267," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:541,testability,Trace,Traceback,541,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1226,testability,observ,observation,1226," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1273,testability,observ,observation,1273," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:67,usability,close,closed,67,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:193,usability,prefer,prefer,193,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:275,usability,mous,mouse,275,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:528,usability,error,error,528,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:592,usability,input,input-,592,"Hello, I'm having a bit of trouble with this. I know the issues is closed, but I thought it might be better to continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be abl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:2071,usability,help,help,2071," continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:. `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`. It plots no problem. However, when I call:. `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`. I get the following error:. ```. Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. 'louvain'],. dtype='object'). ```. Inspecting adata.var[""gene_name""] give:. ```. index. ENSMUSG00000002459 Rgs20. ENSMUSG00000033740 St18. ENSMUSG00000067879 3110035E14Rik. ENSMUSG00000025912 Mybl1. ENSMUSG00000016918 Sulf1. ENSMUSG00000025938 Slco5a1. ENSMUSG00000025930 Msc. ENSMUSG00000025921 Rdh10. ENSMUSG00000025777 Gdap1. ENSMUSG00000025776 Crispld1. ENSMUSG00000025927 Tfap2b. ENSMUSG00000025931 Paqr8. ENSMUSG00000026158 Ogfrl1. ... ```. I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:648,availability,error,error,648,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:224,deployability,continu,continue,224,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:761,deployability,modul,module,761,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1421,deployability,observ,observation,1421,"n adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receivin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1468,deployability,observ,observation,1468,".umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this threa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:2450,integrability,sub,subscribed,2450,"5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/455#issuecomment-472756442>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RNhQOjQK5W1y5e32ifr1ltXXdmtks5vWgxygaJpZM4aczVa>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1316,interoperability,format,format,1316,"have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:761,modifiability,modul,module,761,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:882,modifiability,pac,packages,882,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1054,modifiability,pac,packages,1054,"9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1228,modifiability,pac,packages,1228," this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of tr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1447,modifiability,variab,variable,1447,". When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:648,performance,error,error,648,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1509,performance,time,timepoint,1509,"]). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:648,safety,error,error,648,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:723,safety,input,input-,723,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:761,safety,modul,module,761,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1415,safety,valid,valid,1415,"the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1462,safety,Valid,Valid,1462,". > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:2673,security,auth,auth,2673,"5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/455#issuecomment-472756442>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RNhQOjQK5W1y5e32ifr1ltXXdmtks5vWgxygaJpZM4aczVa>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:661,testability,Trace,Traceback,661,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1421,testability,observ,observation,1421,"n adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receivin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1468,testability,observ,observation,1468,".umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this threa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:177,usability,close,closed,177,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:309,usability,prefer,prefer,309,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:390,usability,mous,mouse,390,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:648,usability,error,error,648,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:723,usability,input,input-,723,"should be gene_symbols in plural. On Thu, Mar 14, 2019 at 9:46 AM csijcs <notifications@github.com> wrote:. > Hello, I'm having a bit of trouble with this. I know the issues is closed,. > but I thought it might be better to continue this discussion rather than. > start a new one, though I can do that if you prefer. I have an AnnData. > object adata with ensembl ids as adata.var_name and mouse gene symbols. > under the column adata.var[“gene_name”]. When I call:. > sc.pl.umap(adata, color=['ENSMUSG00000074637']). > It plots no problem. However, when I call:. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). > I get the following error:. >. > Traceback (most recent call last):. >. >. >. > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:2361,usability,help,help,2361,"5cc5d6>"", line 1, in <module>. >. > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap. >. > return plot_scatter(adata, basis='umap', **kwargs). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter. >. > use_raw=use_raw, gene_symbols=gene_symbols). >. >. >. > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values. >. > .format(value_to_plot, adata.obs.columns)). >. >. >. > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',. >. > 'louvain'],. >. > dtype='object'). >. >. > Inspecting adata.var[""gene_name""] give:. >. > index. >. > ENSMUSG00000002459 Rgs20. >. > ENSMUSG00000033740 St18. >. > ENSMUSG00000067879 3110035E14Rik. >. > ENSMUSG00000025912 Mybl1. >. > ENSMUSG00000016918 Sulf1. >. > ENSMUSG00000025938 Slco5a1. >. > ENSMUSG00000025930 Msc. >. > ENSMUSG00000025921 Rdh10. >. > ENSMUSG00000025777 Gdap1. >. > ENSMUSG00000025776 Crispld1. >. > ENSMUSG00000025927 Tfap2b. >. > ENSMUSG00000025931 Paqr8. >. > ENSMUSG00000026158 Ogfrl1. >. > ... >. >. > I'm not sure what I'm doing wrong here. I can do just about anything using. > the ensembl ids, but I am having a lot of trouble using the gene symbols. I. > would like to be able to use the gene symbols in the plots for umap,. > violin, pca, etc. Any help would be much appreciated. Thanks! >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/455#issuecomment-472756442>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RNhQOjQK5W1y5e32ifr1ltXXdmtks5vWgxygaJpZM4aczVa>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:244,usability,help,helpful,244,"Ok great, thanks. Similarly, I still can't call by gene name with `sc.pl.violin`. `sc.pl.violin(adata, ['ENSMUSG00000074637'], groupby='louvain')` works, but not . `sc.pl.violin(adata, ['Sox2'], groupby='louvain')`. Another thing that would be helpful is if `sc.tl.rank_genes_groups` would use gene_symbols instead of the Ensembl ID. Is it possible? It seems like these re getting lost in the Louvain grouping, but I can't see in the documentation how to retain or recall them. Would appreciate your help. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:434,usability,document,documentation,434,"Ok great, thanks. Similarly, I still can't call by gene name with `sc.pl.violin`. `sc.pl.violin(adata, ['ENSMUSG00000074637'], groupby='louvain')` works, but not . `sc.pl.violin(adata, ['Sox2'], groupby='louvain')`. Another thing that would be helpful is if `sc.tl.rank_genes_groups` would use gene_symbols instead of the Ensembl ID. Is it possible? It seems like these re getting lost in the Louvain grouping, but I can't see in the documentation how to retain or recall them. Would appreciate your help. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:500,usability,help,help,500,"Ok great, thanks. Similarly, I still can't call by gene name with `sc.pl.violin`. `sc.pl.violin(adata, ['ENSMUSG00000074637'], groupby='louvain')` works, but not . `sc.pl.violin(adata, ['Sox2'], groupby='louvain')`. Another thing that would be helpful is if `sc.tl.rank_genes_groups` would use gene_symbols instead of the Ensembl ID. Is it possible? It seems like these re getting lost in the Louvain grouping, but I can't see in the documentation how to retain or recall them. Would appreciate your help. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:102,availability,down,downstream,102,"@fidelram knows more about the plotting code than me, but here's some recommendations:. * Many of the downstream plotting functions for `rank_genes_groups` take `gene_symbols` arguments. See the docs for functions like: `sc.pl.rank_genes_groups_*`. * The `gene_symbols` argument is recent, and is gradually being added to functions. * You can always set `var_names` to be gene symbols. I prefer keeping `var_names` as unique, canonical identifiers, but sometimes do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:436,security,ident,identifiers,436,"@fidelram knows more about the plotting code than me, but here's some recommendations:. * Many of the downstream plotting functions for `rank_genes_groups` take `gene_symbols` arguments. See the docs for functions like: `sc.pl.rank_genes_groups_*`. * The `gene_symbols` argument is recent, and is gradually being added to functions. * You can always set `var_names` to be gene symbols. I prefer keeping `var_names` as unique, canonical identifiers, but sometimes do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:388,usability,prefer,prefer,388,"@fidelram knows more about the plotting code than me, but here's some recommendations:. * Many of the downstream plotting functions for `rank_genes_groups` take `gene_symbols` arguments. See the docs for functions like: `sc.pl.rank_genes_groups_*`. * The `gene_symbols` argument is recent, and is gradually being added to functions. * You can always set `var_names` to be gene symbols. I prefer keeping `var_names` as unique, canonical identifiers, but sometimes do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:215,availability,error,error,215,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:501,availability,error,error,501,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:658,availability,cluster,clustering,658,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:672,availability,sli,slightly,672,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:658,deployability,cluster,clustering,658,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:66,integrability,messag,message,66,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:66,interoperability,messag,message,66,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:77,modifiability,Variab,Variable,77,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:215,performance,error,error,215,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:501,performance,error,error,501,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:672,reliability,sli,slightly,672,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:215,safety,error,error,215,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:501,safety,error,error,501,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:215,usability,error,error,215,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:501,usability,error,error,501,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:764,usability,document,documentation,764,"I tried to set `var_names` from gene_symbols, and I get a warning message:. `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:. `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```. File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(). ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:20,availability,cluster,clustering,20,I have noticed that clustering is always slightly different on different systems... even when using exactly the same code. This will have to do with the underlying numerical libraries I assume. I don't think that has anything to do with gene names.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:41,availability,sli,slightly,41,I have noticed that clustering is always slightly different on different systems... even when using exactly the same code. This will have to do with the underlying numerical libraries I assume. I don't think that has anything to do with gene names.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:20,deployability,cluster,clustering,20,I have noticed that clustering is always slightly different on different systems... even when using exactly the same code. This will have to do with the underlying numerical libraries I assume. I don't think that has anything to do with gene names.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:41,reliability,sli,slightly,41,I have noticed that clustering is always slightly different on different systems... even when using exactly the same code. This will have to do with the underlying numerical libraries I assume. I don't think that has anything to do with gene names.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:4,availability,sli,slight,4,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:159,availability,cluster,clustering,159,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:446,availability,sli,slight,446,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:159,deployability,cluster,clustering,159,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:525,energy efficiency,draw,drawn,525,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:345,integrability,sub,subject,345,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:4,reliability,sli,slight,4,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:446,reliability,sli,slight,446,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:39,usability,learn,learn,39,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:31,availability,cluster,clustering,31,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:336,availability,cluster,clustering,336,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:409,availability,cluster,cluster,409,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:31,deployability,cluster,clustering,31,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:336,deployability,cluster,clustering,336,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:409,deployability,cluster,cluster,409,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:185,integrability,sub,subclustering,185,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:210,reliability,doe,does,210,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:293,reliability,doe,does,293,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:246,safety,test,test,246,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:369,safety,test,tested,369,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:246,testability,test,test,246,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:369,testability,test,tested,369,"I have gotten fairly different clustering results when using `svd_solver='arpack'` in all but 1 case actually. The biological interpretation is still roughly the same, but the depth of subclustering you can do does differ. Based on a preliminary test, using arpack for all `sc.pp.pca()` calls does improve the reproducibility, although clustering results still differ (tested on Fedora 25 and Fedora 28, e.g. cluster sizes change by 100-200 cells). I can show you the differences when you're around next if you like. This is definitely a discussion for a different thread though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:74,availability,cluster,clustering,74,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:113,availability,cluster,clustering,113,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:170,availability,sli,slight,170,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:74,deployability,cluster,clustering,74,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:113,deployability,cluster,clustering,113,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:170,reliability,sli,slight,170,"Thanks for the heads up! Still, the reason for this is not related to the clustering algorithm, but the PCA. The clustering is greedy and will show large deviations upon slight changes of the graph, but otherwise, it's deterministic. I don't know whether there is a way around it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:92,deployability,contain,container,92,Unless you write your own system agnostic numerical libraries or run everything in a docker container... probably not. So just to clarify... you do expect using `sc.pp.pca()` with `svd_solver='arpack'` to not be 100% reproducible across systems as well then?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:783,deployability,modul,module,783,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:146,energy efficiency,core,core,146,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:876,energy efficiency,core,core,876,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1040,energy efficiency,core,core,1040,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:130,modifiability,pac,packages,130,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:783,modifiability,modul,module,783,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:860,modifiability,pac,packages,860,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1024,modifiability,pac,packages,1024,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:659,safety,except,exception,659,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:707,safety,except,exception,707,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:783,safety,modul,module,783,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:475,security,hash,hashtable,475,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:588,security,hash,hashtable,588,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:26,testability,Trace,Traceback,26,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:719,testability,Trace,Traceback,719,"`adata.var[""gene_name""]`. Traceback (most recent call last):. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc. return self._engine.get_loc(casted_key). File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item. File ""pandas/_libs/hashtable_class_helper.pxi"", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item. KeyError: 'gene_name'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py"", line 3458, in __getitem__. indexer = self.columns.get_loc(key). File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc. raise KeyError(key) from err. KeyError: 'gene_name'. Did something change in scanpy ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:19,availability,error,error,19,I get an identical error as @hemantgujar,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:19,performance,error,error,19,I get an identical error as @hemantgujar,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:19,safety,error,error,19,I get an identical error as @hemantgujar,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:9,security,ident,identical,9,I get an identical error as @hemantgujar,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:19,usability,error,error,19,I get an identical error as @hemantgujar,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/456:252,availability,down,downstream,252,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:336,availability,down,downstream,336,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:547,availability,operat,operation,547,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:441,energy efficiency,measur,measured,441,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:52,integrability,sub,subtracting,52,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:319,interoperability,format,format,319,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:523,performance,perform,performing,523,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:183,usability,command,command,183,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:368,usability,command,command,368,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:523,usability,perform,performing,523,"Hi, I have fixed the issue. It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis. I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor. So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/pull/457:15,deployability,updat,update,15,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:29,integrability,plug-in,plug-ins,29,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:253,integrability,sub,subscribed,253,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:378,integrability,event,event-,378,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:29,interoperability,plug,plug-ins,29,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:15,safety,updat,update,15,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:15,security,updat,update,15,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:468,security,auth,auth,468,"@falexwolf any update on the plug-ins idea for scanpy? On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:42,deployability,API,API,42,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:125,deployability,API,API,125,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:258,deployability,API,API,258,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:290,deployability,instal,install,290,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:443,deployability,scale,scales,443,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:404,energy efficiency,sustainab,sustainable,404,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:443,energy efficiency,scale,scales,443,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:42,integrability,API,API,42,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:56,integrability,interfac,interfaces,56,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:125,integrability,API,API,125,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:258,integrability,API,API,258,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:335,integrability,interfac,interfaces,335,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:42,interoperability,API,API,42,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:56,interoperability,interfac,interfaces,56,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:125,interoperability,API,API,125,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:258,interoperability,API,API,258,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:335,interoperability,interfac,interfaces,335,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:56,modifiability,interfac,interfaces,56,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:311,modifiability,maintain,maintained,311,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:335,modifiability,interfac,interfaces,335,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:358,modifiability,pac,packages,358,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:443,modifiability,scal,scales,443,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:443,performance,scale,scales,443,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:311,safety,maintain,maintained,311,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:133,security,access,accessible,133,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/pull/457:370,usability,clear,clearly,370,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457
https://github.com/scverse/scanpy/issues/458:739,integrability,sub,subscribed,739,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:325,modifiability,paramet,parameters,325,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:529,modifiability,paramet,parameters,529,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:652,safety,input,inputs,652,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:936,security,auth,auth,936,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:336,usability,document,documented,336,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:444,usability,document,documented,444,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:595,usability,document,documentation,595,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:652,usability,input,inputs,652,"Hi Bérénice, ncols, wspace and hspace should work although they are only. meaningful when more than one plot is present. I think that right_margin. and left_margin were used in the initial implementation but no longer. On Mon, Feb 4, 2019 at 9:50 AM Bérénice Batut <notifications@github.com>. wrote:. > Hi,. >. > Some of the parameters documented online for pl.scatter are not accepted. > by the method: ncols, wspace, hspace. And some are not documented:. > right_margin, left_margin. >. > The difference seems to come from the parameters described in. > {scatter_bulk} and used for pl.scatter documentation. >. > What should be fixed scatter_bulk or inputs for pl.scatter? >. > Bérénice. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/458>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bLyz9mimlEZzQ3hbnyiW45MtBHJks5vJ_Q_gaJpZM4agxAk>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:268,deployability,version,version,268,"`right_margin` and `left_margin` are still listed as parameters for `pl.scatter`: . https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. And `ncols`, `wspace` and `hspace` are not accepted (with the current version on conda) 😟",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:260,energy efficiency,current,current,260,"`right_margin` and `left_margin` are still listed as parameters for `pl.scatter`: . https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. And `ncols`, `wspace` and `hspace` are not accepted (with the current version on conda) 😟",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:268,integrability,version,version,268,"`right_margin` and `left_margin` are still listed as parameters for `pl.scatter`: . https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. And `ncols`, `wspace` and `hspace` are not accepted (with the current version on conda) 😟",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:53,modifiability,paramet,parameters,53,"`right_margin` and `left_margin` are still listed as parameters for `pl.scatter`: . https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. And `ncols`, `wspace` and `hspace` are not accepted (with the current version on conda) 😟",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:268,modifiability,version,version,268,"`right_margin` and `left_margin` are still listed as parameters for `pl.scatter`: . https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. And `ncols`, `wspace` and `hspace` are not accepted (with the current version on conda) 😟",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:42,deployability,instal,installation,42,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:78,deployability,version,version,78,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:448,deployability,version,version,448,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:70,energy efficiency,current,current,70,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:440,energy efficiency,current,current,440,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:78,integrability,version,version,78,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:448,integrability,version,version,448,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:78,modifiability,version,version,78,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:230,modifiability,paramet,parameters,230,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:448,modifiability,version,version,448,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:. > . > right_margin and left_margin are still listed as parameters for pl.scatter:. > . > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49. > . > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟. > . > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:35,deployability,version,version,35,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:193,deployability,automat,automatically,193,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:35,integrability,version,version,35,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:10,modifiability,maintain,maintaining,10,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:35,modifiability,version,version,35,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:165,reliability,Doe,Does,165,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:185,reliability,Doe,Does,185,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:10,safety,maintain,maintaining,10,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:193,testability,automat,automatically,193,"We're not maintaining the bioconda version, but it looks as if it was up to date: https://bioconda.github.io/recipes/scanpy/README.html. How is it working, anyways? Does somebody know? Does it automatically pull from PyPI?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:45,deployability,instal,install,45,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:158,deployability,instal,installation,158,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:385,deployability,version,version,385,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:385,integrability,version,version,385,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:385,modifiability,version,version,385,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:358,safety,except,except,358,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:301,security,control,control,301,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:301,testability,control,control,301,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:251,usability,prefer,prefer,251,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:286,usability,custom,custom,286,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:58,deployability,automat,automatically,58,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:150,deployability,automat,automatically,150,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:30,reliability,Doe,Does,30,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:50,reliability,Doe,Does,50,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:58,testability,automat,automatically,58,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:150,testability,automat,automatically,150,"> How is it working, anyways? Does somebody know? Does it automatically pull from PyPI? yes: bioconda/bioconda-recipes#12958. > This pull request was automatically generated by [bioconda-utils](https://github.com/bioconda/bioconda-utils).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:45,deployability,automat,automatic,45,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:55,deployability,updat,update,55,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:55,safety,updat,update,55,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:55,security,updat,update,55,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:45,testability,automat,automatic,45,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:636,deployability,api,api,636,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:324,integrability,compon,components,324,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:636,integrability,api,api,636,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:324,interoperability,compon,components,324,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:636,interoperability,api,api,636,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:258,modifiability,layer,layers,258,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:324,modifiability,compon,components,324,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:39,reliability,doe,does,39,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:150,reliability,doe,does,150,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:672,reliability,doe,does,672,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:131,usability,document,documentation,131,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:. `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:112,deployability,API,API,112,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:112,integrability,API,API,112,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:112,interoperability,API,API,112,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:210,testability,simpl,simplified,210,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:210,usability,simpl,simplified,210,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:72,modifiability,layer,layers,72,"Sure, I can have a look at the docs. I've been using it because of the `layers` argument since `pl.umap` does not seem to have it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:105,reliability,doe,does,105,"Sure, I can have a look at the docs. I've been using it because of the `layers` argument since `pl.umap` does not seem to have it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:19,modifiability,layer,layers,19,"@fabianrost84 The 'layers' were not added to anndata until recently and had not been implemented in all functions. However, adding this functionality to the different scatter plot functions is straightforward. Let me prepare a quick PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:88,reliability,doe,does,88,"As @bebatut pointed out, `pl.scatter` uses `doc_scatter_bulk` for the documentation. It does not seem to make sense to remove `ncols` from `doc_scatter_bulk` as it is used for `pl.umap` and so on. How should the docs for `pl.scatter` be corrected? Should we remove `doc_scatter_bulk` and copy-paste the relevant documentation from `doc_scatter_bulk` to the `pl.scatter` doc string? At least until `pl.scatter` is in good shape again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:70,usability,document,documentation,70,"As @bebatut pointed out, `pl.scatter` uses `doc_scatter_bulk` for the documentation. It does not seem to make sense to remove `ncols` from `doc_scatter_bulk` as it is used for `pl.umap` and so on. How should the docs for `pl.scatter` be corrected? Should we remove `doc_scatter_bulk` and copy-paste the relevant documentation from `doc_scatter_bulk` to the `pl.scatter` doc string? At least until `pl.scatter` is in good shape again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:312,usability,document,documentation,312,"As @bebatut pointed out, `pl.scatter` uses `doc_scatter_bulk` for the documentation. It does not seem to make sense to remove `ncols` from `doc_scatter_bulk` as it is used for `pl.umap` and so on. How should the docs for `pl.scatter` be corrected? Should we remove `doc_scatter_bulk` and copy-paste the relevant documentation from `doc_scatter_bulk` to the `pl.scatter` doc string? At least until `pl.scatter` is in good shape again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:89,usability,document,documentation,89,@fabianrost84 The best would be to remove `doc_scatter_bulk` and copy paste the relevant documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:205,modifiability,variab,variable,205,> copy paste. please never say those words when speaking about code again :stuck_out_tongue_winking_eye: . No but seriously: There’s at least 6 reasons not to do that and to introduce a second (temporary) variable instead: https://github.com/theislab/scanpy/pull/557#issuecomment-476512533,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/460:105,usability,help,help,105,"Thank you for the prompt reply, I am going to open a pull request. I can work on points 1 and 4. May you help me with the other 2 points? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:290,deployability,log,logfc,290,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:473,deployability,log,logreg,473,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:556,deployability,log,logreg,556,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:65,modifiability,paramet,parameters,65,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:245,modifiability,paramet,parameters,245,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:79,performance,tune,tune,79,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:290,safety,log,logfc,290,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:473,safety,log,logreg,473,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:556,safety,log,logreg,556,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:290,security,log,logfc,290,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:473,security,log,logreg,473,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:556,security,log,logreg,556,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:290,testability,log,logfc,290,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:473,testability,log,logreg,473,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:556,testability,log,logreg,556,"@andrea-tango . Really awesome! I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:68,integrability,pub,publicly,68,"Interesting, @MichaelPeibo! Would you share these results somewhere publicly? A notebook on GitHub? @andrea-tango. Yes, we should get the functionality of 2 and 3 functionality into `rank_genes_groups`. @Koncopd, could you work on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:38,interoperability,share,share,38,"Interesting, @MichaelPeibo! Would you share these results somewhere publicly? A notebook on GitHub? @andrea-tango. Yes, we should get the functionality of 2 and 3 functionality into `rank_genes_groups`. @Koncopd, could you work on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:417,safety,test,test,417,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:427,safety,test,test,427,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:593,safety,test,test,593,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:417,testability,test,test,417,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:427,testability,test,test,427,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:593,testability,test,test,593,"@MichaelPeibo @falexwolf I started working on points 2 and 3, but it is better if you will work on these points. I wrote the code for points 1 and 4. In order to generate volcano plots, I calculated the log2FC relying on the `diffxpy` library. I can push again the code for tSNE and also the code for volcano plots. Please, check the `rank_genes_groups` function. Considering 2 groups of cells and using the Wilcoxon test (`de.test.wilcoxon`) provided by the `diffxpy` library, I obtained different marker genes with respect to those calculated by using `rank_genes_groups` function (Wilcoxon test). Many thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:686,availability,avail,available,686,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:177,deployability,depend,dependency,177,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:202,deployability,depend,depends,202,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:249,deployability,depend,dependency,249,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:707,deployability,scale,scale,707,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:707,energy efficiency,scale,scale,707,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:177,integrability,depend,dependency,177,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:202,integrability,depend,depends,202,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:249,integrability,depend,dependency,249,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:297,integrability,wrap,wrapper,297,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:297,interoperability,wrapper,wrapper,297,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:177,modifiability,depend,dependency,177,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:202,modifiability,depend,depends,202,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:249,modifiability,depend,dependency,249,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:707,modifiability,scal,scale,707,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:707,performance,scale,scale,707,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:975,performance,time,time,975,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:686,reliability,availab,available,686,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:177,safety,depend,dependency,177,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:202,safety,depend,depends,202,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:249,safety,depend,dependency,249,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:686,safety,avail,available,686,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:847,safety,test,test,847,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1116,safety,Test,Test,1116,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1166,safety,test,tests,1166,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1227,safety,test,tests,1227,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1284,safety,test,tests,1284,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:686,security,availab,available,686,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:177,testability,depend,dependency,177,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:202,testability,depend,depends,202,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:249,testability,depend,dependency,249,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:847,testability,test,test,847,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1116,testability,Test,Test,1116,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1166,testability,test,tests,1166,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1227,testability,test,tests,1227,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1284,testability,test,tests,1284,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1344,testability,simpl,simple,1344,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1344,usability,simpl,simple,1344,"Great, thank you, @andrea-tango and @Koncopd! @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away? Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests? We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:59,deployability,depend,dependencies,59,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,deployability,depend,dependency,104,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:59,integrability,depend,dependencies,59,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,integrability,depend,dependency,104,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:534,integrability,pub,public,534,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:588,integrability,repositor,repository,588,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:610,integrability,coupl,couple,610,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:588,interoperability,repositor,repository,588,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:59,modifiability,depend,dependencies,59,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,modifiability,depend,dependency,104,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:185,modifiability,paramet,parameter,185,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:610,modifiability,coupl,couple,610,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:59,safety,depend,dependencies,59,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,safety,depend,dependency,104,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:426,safety,test,tests,426,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:59,testability,depend,dependencies,59,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,testability,depend,dependency,104,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:426,testability,test,tests,426,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:610,testability,coupl,couple,610,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency! > would you make a PR? I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away? I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests? I tried them on data coming from the lab in which I am working. I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository. Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:31,availability,downtim,downtime,31,"Oh, thanks! Sorry for the long downtime, the whole family was sick... I'm going through the PR now. The tests question was actually targeted towards @davidsebfischer, but thanks anyways! The comparison question was also targeted to @davidsebfischer, @tcallies. But if you do it, @andrea-tango, awesome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:31,deployability,downtim,downtime,31,"Oh, thanks! Sorry for the long downtime, the whole family was sick... I'm going through the PR now. The tests question was actually targeted towards @davidsebfischer, but thanks anyways! The comparison question was also targeted to @davidsebfischer, @tcallies. But if you do it, @andrea-tango, awesome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:31,reliability,downtim,downtime,31,"Oh, thanks! Sorry for the long downtime, the whole family was sick... I'm going through the PR now. The tests question was actually targeted towards @davidsebfischer, but thanks anyways! The comparison question was also targeted to @davidsebfischer, @tcallies. But if you do it, @andrea-tango, awesome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,safety,test,tests,104,"Oh, thanks! Sorry for the long downtime, the whole family was sick... I'm going through the PR now. The tests question was actually targeted towards @davidsebfischer, but thanks anyways! The comparison question was also targeted to @davidsebfischer, @tcallies. But if you do it, @andrea-tango, awesome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:104,testability,test,tests,104,"Oh, thanks! Sorry for the long downtime, the whole family was sick... I'm going through the PR now. The tests question was actually targeted towards @davidsebfischer, but thanks anyways! The comparison question was also targeted to @davidsebfischer, @tcallies. But if you do it, @andrea-tango, awesome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:346,usability,tool,tools,346,"Very good suggestions. One small question:. > * `positive`, if it is True, the function should return only positive marker genes for each population. Isn't this `rankby_abs=False` which we have by default in `rank_genes_groups`? (and which was actually called `only_positive=True` previously https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py#L100)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:278,integrability,pub,public,278,"Hi @falexwolf . I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:334,integrability,pub,public,334,"Hi @falexwolf . I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:37,testability,simpl,simpler,37,"Hi @falexwolf . I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:37,usability,simpl,simpler,37,"Hi @falexwolf . I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:362,energy efficiency,model,model,362,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:411,energy efficiency,model,model,411,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:98,integrability,batch,batchglm,98,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1597,integrability,wrap,wrapping,1597,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:376,interoperability,standard,standard,376,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1431,interoperability,standard,standard,1431,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:98,performance,batch,batchglm,98,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:278,safety,test,tests,278,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:311,safety,test,tests,311,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:338,safety,test,tests,338,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:385,safety,test,tests,385,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:433,safety,test,test,433,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:451,safety,test,test,451,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:592,safety,test,test,592,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:622,safety,test,tests,622,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:673,safety,test,test,673,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:741,safety,test,test,741,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:915,safety,test,test,915,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:964,safety,test,test,964,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:972,safety,test,test,972,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1045,safety,test,test,1045,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1151,safety,test,test,1151,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1220,safety,test,test,1220,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1237,safety,avoid,avoid,1237,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1333,safety,test,tests,1333,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1456,safety,test,testing,1456,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1613,safety,test,tests,1613,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:362,security,model,model,362,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:411,security,model,model,411,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:278,testability,test,tests,278,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:306,testability,unit,unit,306,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:311,testability,test,tests,311,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:338,testability,test,tests,338,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:385,testability,test,tests,385,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:433,testability,test,test,433,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:451,testability,test,test,451,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:592,testability,test,test,592,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:622,testability,test,tests,622,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:673,testability,test,test,673,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:736,testability,unit,unit,736,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:741,testability,test,test,741,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:915,testability,test,test,915,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:964,testability,test,test,964,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:972,testability,test,test,972,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1045,testability,test,test,1045,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1151,testability,test,test,1151,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1220,testability,test,test,1220,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1328,testability,unit,unit,1328,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1333,testability,test,tests,1333,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1456,testability,test,testing,1456,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1613,testability,test,tests,1613,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:132,availability,cluster,clusters,132,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:620,availability,error,error,620,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:748,availability,down,downloaded,748,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:132,deployability,cluster,clusters,132,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:740,deployability,version,version,740,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:740,integrability,version,version,740,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:785,integrability,repositor,repository,785,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:785,interoperability,repositor,repository,785,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:740,modifiability,version,version,740,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:620,performance,error,error,620,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:531,safety,test,test,531,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:541,safety,test,test,541,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:620,safety,error,error,620,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:723,safety,test,test,723,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:644,security,ident,identical,644,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:36,testability,simpl,simple,36,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:531,testability,test,test,531,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:541,testability,test,test,541,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:723,testability,test,test,723,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:36,usability,simpl,simple,36,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:403,usability,user,user-images,403,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:620,usability,error,error,620,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:. `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]. `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,. `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""). `. there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip. I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:378,usability,custom,custom,378,"Just a note... I also have some code to make a volcano plot in line [111] [here](https://github.com/theislab/single-cell-tutorial/blob/master/Case-study_Mouse-intestinal-epithelium.ipynb). Don't know if this is still needed, but I thought I would see if someone cares. @davidsebfischer do you allow labels in your volcano plot function? And can it take any object, or only some custom `diffxpy` ones? If these things aren't a problem I'd prefer to use yours tbh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:438,usability,prefer,prefer,438,"Just a note... I also have some code to make a volcano plot in line [111] [here](https://github.com/theislab/single-cell-tutorial/blob/master/Case-study_Mouse-intestinal-epithelium.ipynb). Don't know if this is still needed, but I thought I would see if someone cares. @davidsebfischer do you allow labels in your volcano plot function? And can it take any object, or only some custom `diffxpy` ones? If these things aren't a problem I'd prefer to use yours tbh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:96,deployability,log,logfc,96,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:388,deployability,updat,updated,388,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:43,integrability,filter,filtering,43,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:299,reliability,doe,does,299,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:96,safety,log,logfc,96,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:388,safety,updat,updated,388,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:96,security,log,logfc,96,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:388,security,updat,updated,388,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:96,testability,log,logfc,96,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:38,integrability,filter,filter,38,does `sc.tl.filter_rank_genes_groups` filter the `sc.tl.rank_genes_groups` result? Or does it recompute? The former would not alleviate the multiple testing burden.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:0,reliability,doe,does,0,does `sc.tl.filter_rank_genes_groups` filter the `sc.tl.rank_genes_groups` result? Or does it recompute? The former would not alleviate the multiple testing burden.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:86,reliability,doe,does,86,does `sc.tl.filter_rank_genes_groups` filter the `sc.tl.rank_genes_groups` result? Or does it recompute? The former would not alleviate the multiple testing burden.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:149,safety,test,testing,149,does `sc.tl.filter_rank_genes_groups` filter the `sc.tl.rank_genes_groups` result? Or does it recompute? The former would not alleviate the multiple testing burden.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:149,testability,test,testing,149,does `sc.tl.filter_rank_genes_groups` filter the `sc.tl.rank_genes_groups` result? Or does it recompute? The former would not alleviate the multiple testing burden.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:37,integrability,filter,filtered,37,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:328,integrability,filter,filter,328,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:115,modifiability,paramet,parameters,115,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:0,reliability,doe,does,0,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:292,reliability,doe,does,292,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:377,reliability,doe,does,377,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:134,safety,test,tested,134,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:443,safety,test,testing,443,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:717,security,auth,auth,717,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,testability,simpl,simply,20,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:134,testability,test,tested,134,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:443,testability,test,testing,443,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,usability,simpl,simply,20,"does not recompute, simply saves the filtered data under. adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be. tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>. wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups. > result? Or does it recompute? The former would not alleviate the multiple. > testing burden. >. > —. > You are receiving this because you commented. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:208,modifiability,paramet,parameter,208,"If p-values are regarded as a valuable output rather than just the ranks, it might be worth recomputing as thresholding would ease the multiple testing burden. I guess that's the idea behind the `min_pCells` parameter request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:144,safety,test,testing,144,"If p-values are regarded as a valuable output rather than just the ranks, it might be worth recomputing as thresholding would ease the multiple testing burden. I guess that's the idea behind the `min_pCells` parameter request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:144,testability,test,testing,144,"If p-values are regarded as a valuable output rather than just the ranks, it might be worth recomputing as thresholding would ease the multiple testing burden. I guess that's the idea behind the `min_pCells` parameter request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:122,integrability,filter,filtering,122,"@LuckyMD your tutorial is very interesting! I agree with you regarding the `min_pCells` parameter, it should be used as a filtering step before calculating the marker genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:88,modifiability,paramet,parameter,88,"@LuckyMD your tutorial is very interesting! I agree with you regarding the `min_pCells` parameter, it should be used as a filtering step before calculating the marker genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:121,deployability,log,logreg,121,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:152,deployability,log,logfoldchanges,152,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:172,deployability,log,logfoldchanges,172,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:212,deployability,log,logreg,212,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:137,reliability,doe,doesn,137,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:236,reliability,doe,doesn,236,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:121,safety,log,logreg,121,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:152,safety,log,logfoldchanges,152,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:172,safety,log,logfoldchanges,172,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:212,safety,log,logreg,212,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:121,security,log,logreg,121,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:152,security,log,logfoldchanges,152,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:172,security,log,logfoldchanges,172,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:212,security,log,logreg,212,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:121,testability,log,logreg,121,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:152,testability,log,logfoldchanges,152,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:172,testability,log,logfoldchanges,172,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:212,testability,log,logreg,212,@falexwolf @andrea-tango . I have a question regarding point 2 (log2FC values in `rank_genes_groups`). I see that only `'logreg'` method doesn't return logfoldchanges. But logfoldchanges don't seem natural for `'logreg'` as this method doesn't even use `reference` for calculation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:312,deployability,log,logfoldchanges,312,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:361,deployability,log,logy,361,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:701,deployability,log,logfoldchanges,701,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:901,deployability,log,logfoldchanges,901,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1138,deployability,log,logfoldchanges,1138,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1187,deployability,log,logy,1187,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:27,integrability,topic,topic,27,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:312,safety,log,logfoldchanges,312,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:361,safety,log,logy,361,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:425,safety,Compl,Complete,425,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:701,safety,log,logfoldchanges,701,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:901,safety,log,logfoldchanges,901,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1138,safety,log,logfoldchanges,1138,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1187,safety,log,logy,1187,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:312,security,log,logfoldchanges,312,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:361,security,log,logy,361,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:425,security,Compl,Complete,425,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:701,security,log,logfoldchanges,701,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:901,security,log,logfoldchanges,901,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1138,security,log,logfoldchanges,1138,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1187,security,log,logy,1187,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:312,testability,log,logfoldchanges,312,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:361,testability,log,logy,361,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:701,testability,log,logfoldchanges,701,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:901,testability,log,logfoldchanges,901,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1138,testability,log,logfoldchanges,1138,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1187,testability,log,logy,1187,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:113,usability,interact,interactive,113,"Sorry this is a little off topic, but it's something I've found useful:. @LuckyMD and anyone else looking for an interactive volcano plot, I've been using [`hvplot`](https://hvplot.pyviz.org) in my notebooks. A volcano plot can be made from DE data frame with something like:. ```python. de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. <details>. <summary> Complete example using scanpy </summary>. ```python. import pandas as pd. import numpy as np. import hvplot.pandas. import scanpy as sc. def rank_genes_groups_df(adata, group, pval_cutoff : float =None, logfc_cutoff=None): . d = pd.DataFrame() . for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']: . d[k] = adata.uns[""rank_genes_groups""][k][group] . if pval_cutoff is not None: . d = d[d[""pvals_adj""] < pval_cutoff] . if logfc_cutoff is not None: . d = d[d[""logfoldchanges""].abs() > logfc_cutoff] . return d. pbmcs = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(pbmcs, ""bulk_labels"", n_genes=pbmcs.var_names.size). de_df = rank_genes_groups_df(pbmcs, ""CD34+""). de_df.hvplot.scatter(. ""logfoldchanges"", ""pvals_adj"", . flip_yaxis=True, logy=True, . hover_cols=[""names""]. ). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:19,energy efficiency,cool,cool,19,"@ivirshup, this is cool and very useful. Would you make a notebook ""Interactive plotting"" for http://scanpy-tutorials.readthedocs.io? And we'd link to it from https://scanpy.readthedocs.io/en/latest/tutorials.html as done for the other notebooks. You could also add the example from https://github.com/theislab/scanpy/issues/510#issuecomment-473506882. As time passes, this could grow. But these two little examples are a very useful start, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:356,performance,time,time,356,"@ivirshup, this is cool and very useful. Would you make a notebook ""Interactive plotting"" for http://scanpy-tutorials.readthedocs.io? And we'd link to it from https://scanpy.readthedocs.io/en/latest/tutorials.html as done for the other notebooks. You could also add the example from https://github.com/theislab/scanpy/issues/510#issuecomment-473506882. As time passes, this could grow. But these two little examples are a very useful start, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:68,usability,Interact,Interactive,68,"@ivirshup, this is cool and very useful. Would you make a notebook ""Interactive plotting"" for http://scanpy-tutorials.readthedocs.io? And we'd link to it from https://scanpy.readthedocs.io/en/latest/tutorials.html as done for the other notebooks. You could also add the example from https://github.com/theislab/scanpy/issues/510#issuecomment-473506882. As time passes, this could grow. But these two little examples are a very useful start, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:10,deployability,updat,updating,10,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,deployability,log,logreg,20,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:10,safety,updat,updating,10,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,safety,log,logreg,20,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:10,security,updat,updating,10,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,security,log,logreg,20,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:20,testability,log,logreg,20,"@Koncopd, updating `logreg` with a proper implementation accounting for `reference` is another story. We can talk about it sometime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/pull/461:56,integrability,compon,components,56,"I tested the code, now it is possible to calculate more components for the tSNE embedding method",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/461:56,interoperability,compon,components,56,"I tested the code, now it is possible to calculate more components for the tSNE embedding method",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/461:56,modifiability,compon,components,56,"I tested the code, now it is possible to calculate more components for the tSNE embedding method",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/461:2,safety,test,tested,2,"I tested the code, now it is possible to calculate more components for the tSNE embedding method",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/461:2,testability,test,tested,2,"I tested the code, now it is possible to calculate more components for the tSNE embedding method",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/462:110,deployability,fail,fail,110,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:357,energy efficiency,reduc,reduce,357,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:245,performance,parallel,parallel,245,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:258,performance,cach,cached,258,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:308,performance,parallel,parallelize,308,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:337,performance,cach,caches,337,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:376,performance,time,times,376,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:110,reliability,fail,fail,110,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:62,testability,understand,understand,62,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following? > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:61,deployability,version,versions,61,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:242,energy efficiency,reduc,reduce,242,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:61,integrability,version,versions,61,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:61,modifiability,version,versions,61,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:176,performance,cach,caching,176,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:214,performance,cach,cache,214,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:261,performance,time,times,261,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:328,performance,cach,cache,328,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:378,performance,parallel,parallel,378,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:393,performance,cach,cached,393,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:23,usability,support,supporting,23,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:314,usability,user,user,314,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:204,performance,perform,performance-critical,204,OK got it! I still think `@jit` is too opaque – how should you know that some innocent-looking change results in a loop no longer being compiled? I think we should use `@njit` to be sure we have compiled performance-critical parts going forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:204,usability,perform,performance-critical,204,OK got it! I still think `@jit` is too opaque – how should you know that some innocent-looking change results in a loop no longer being compiled? I think we should use `@njit` to be sure we have compiled performance-critical parts going forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:86,deployability,fail,failed,86,I believe numba will always throw a warning if some part of the requested compilation failed. We could add tests for compilation based on this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:86,reliability,fail,failed,86,I believe numba will always throw a warning if some part of the requested compilation failed. We could add tests for compilation based on this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:107,safety,test,tests,107,I believe numba will always throw a warning if some part of the requested compilation failed. We could add tests for compilation based on this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:107,testability,test,tests,107,I believe numba will always throw a warning if some part of the requested compilation failed. We could add tests for compilation based on this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:22,safety,test,test,22,"No, looks good and we test the QC metrics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:22,testability,test,test,22,"No, looks good and we test the QC metrics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/issues/465:363,energy efficiency,Current,Currently,363,"if you use swap_axes then the columns becomes the rows and you can use the. row_palette. Would that work for you? On Thu, Feb 7, 2019 at 2:44 PM alexmascension <notifications@github.com>. wrote:. > Hi,. >. > When using the sc.pl.stacked_violin and. > sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to. > implement a column_palette option? Currently the row_palette plots colors. > according to the genes, which is really helpful to colorize genes by sets. > Still, it would be interesting to plot the violin colors according to the. > groups using a column_palette option. I guess that one option should. > override the other. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/465>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ZrZsou64pt03lp1KIxNC2XmELpMks5vLC2rgaJpZM4anUiU>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:700,integrability,sub,subscribed,700,"if you use swap_axes then the columns becomes the rows and you can use the. row_palette. Would that work for you? On Thu, Feb 7, 2019 at 2:44 PM alexmascension <notifications@github.com>. wrote:. > Hi,. >. > When using the sc.pl.stacked_violin and. > sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to. > implement a column_palette option? Currently the row_palette plots colors. > according to the genes, which is really helpful to colorize genes by sets. > Still, it would be interesting to plot the violin colors according to the. > groups using a column_palette option. I guess that one option should. > override the other. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/465>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ZrZsou64pt03lp1KIxNC2XmELpMks5vLC2rgaJpZM4anUiU>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:897,security,auth,auth,897,"if you use swap_axes then the columns becomes the rows and you can use the. row_palette. Would that work for you? On Thu, Feb 7, 2019 at 2:44 PM alexmascension <notifications@github.com>. wrote:. > Hi,. >. > When using the sc.pl.stacked_violin and. > sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to. > implement a column_palette option? Currently the row_palette plots colors. > according to the genes, which is really helpful to colorize genes by sets. > Still, it would be interesting to plot the violin colors according to the. > groups using a column_palette option. I guess that one option should. > override the other. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/465>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ZrZsou64pt03lp1KIxNC2XmELpMks5vLC2rgaJpZM4anUiU>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:445,usability,help,helpful,445,"if you use swap_axes then the columns becomes the rows and you can use the. row_palette. Would that work for you? On Thu, Feb 7, 2019 at 2:44 PM alexmascension <notifications@github.com>. wrote:. > Hi,. >. > When using the sc.pl.stacked_violin and. > sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to. > implement a column_palette option? Currently the row_palette plots colors. > according to the genes, which is really helpful to colorize genes by sets. > Still, it would be interesting to plot the violin colors according to the. > groups using a column_palette option. I guess that one option should. > override the other. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/465>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ZrZsou64pt03lp1KIxNC2XmELpMks5vLC2rgaJpZM4anUiU>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:17,availability,error,error,17,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:172,availability,error,error,172,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:1175,availability,cluster,clusters,1175,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:270,deployability,modul,module,270,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:459,deployability,log,log,459,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:601,deployability,scale,scale,601,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:794,deployability,scale,scale,794,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:800,deployability,scale,scale,800,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:1175,deployability,cluster,clusters,1175,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:601,energy efficiency,scale,scale,601,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:794,energy efficiency,scale,scale,794,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:800,energy efficiency,scale,scale,800,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:270,modifiability,modul,module,270,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:378,modifiability,pac,packages,378,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:569,modifiability,layer,layer,569,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:601,modifiability,scal,scale,601,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:710,modifiability,variab,variable,710,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:794,modifiability,scal,scale,794,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:800,modifiability,scal,scale,800,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:17,performance,error,error,17,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:172,performance,error,error,172,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:601,performance,scale,scale,601,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:794,performance,scale,scale,794,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:800,performance,scale,scale,800,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:17,safety,error,error,17,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:172,safety,error,error,172,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:243,safety,input,input-,243,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:270,safety,modul,module,270,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:459,safety,log,log,459,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:459,security,log,log,459,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:199,testability,Trace,Traceback,199,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:459,testability,log,log,459,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:17,usability,error,error,17,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:172,usability,error,error,172,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:243,usability,input,input-,243,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:1030,usability,user,user,1030,"Hi,. We get this error without swapping axes:. The code is . ```. genes = [""DES"", ""CD34"", ""COL1A1""]. sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ). ```. The error is . ```. IndexError Traceback (most recent call last). <ipython-input-35-8f09494e5255> in <module>. ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds). 929 axs_list.append(ax). 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,. --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds). 932 . 933 if stripplot:. IndexError: list index out of range. ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:21,deployability,fail,fail,21,"Hi,. I'm sorry but I fail to see how would PR #425 solve the problem. Could you give me a hint on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:21,reliability,fail,fail,21,"Hi,. I'm sorry but I fail to see how would PR #425 solve the problem. Could you give me a hint on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:90,usability,hint,hint,90,"Hi,. I'm sorry but I fail to see how would PR #425 solve the problem. Could you give me a hint on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:150,availability,cluster,cluster,150,"I feel like the behavior around this has since (a) changed a lot (especially defaults), and (b) now does the expected thing when you want to color by cluster. Closing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:150,deployability,cluster,cluster,150,"I feel like the behavior around this has since (a) changed a lot (especially defaults), and (b) now does the expected thing when you want to color by cluster. Closing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:100,reliability,doe,does,100,"I feel like the behavior around this has since (a) changed a lot (especially defaults), and (b) now does the expected thing when you want to color by cluster. Closing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:16,usability,behavi,behavior,16,"I feel like the behavior around this has since (a) changed a lot (especially defaults), and (b) now does the expected thing when you want to color by cluster. Closing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/pull/466:91,performance,time,time,91,@falexwolf . This is just a small test for the existing pca. You asked me to write it some time ago.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:34,safety,test,test,34,@falexwolf . This is just a small test for the existing pca. You asked me to write it some time ago.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:34,testability,test,test,34,@falexwolf . This is just a small test for the existing pca. You asked me to write it some time ago.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/467:169,availability,error,error,169,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:67,integrability,wrap,wrapper,67,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:104,integrability,compon,components,104,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:133,integrability,wrap,wrapper,133,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:67,interoperability,wrapper,wrapper,67,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:104,interoperability,compon,components,104,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:133,interoperability,wrapper,wrapper,133,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:104,modifiability,compon,components,104,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:169,performance,error,error,169,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:169,safety,error,error,169,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:169,usability,error,error,169,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:92,availability,error,error,92,Ah? The code seems like it just returns an empty list when there’s no results. Where is the error thrown?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:92,performance,error,error,92,Ah? The code seems like it just returns an empty list when there’s no results. Where is the error thrown?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:92,safety,error,error,92,Ah? The code seems like it just returns an empty list when there’s no results. Where is the error thrown?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:92,usability,error,error,92,Ah? The code seems like it just returns an empty list when there’s no results. Where is the error thrown?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:220,deployability,modul,module,220,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1050,energy efficiency,core,core,1050,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1374,energy efficiency,core,core,1374,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1596,energy efficiency,core,core,1596,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:818,integrability,sub,subgraph,818,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1688,interoperability,mismatch,mismatch,1688,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1776,interoperability,format,format,1776,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1872,interoperability,mismatch,mismatch,1872,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:220,modifiability,modul,module,220,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:473,modifiability,pac,packages,473,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1034,modifiability,pac,packages,1034,"eive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receivi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1358,modifiability,pac,packages,1358,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1580,modifiability,pac,packages,1580,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:193,safety,input,input-,193,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:220,safety,modul,module,220,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:233,safety,test,test,233,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:1213,safety,except,except,1213,"no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 values for the `pd.Dataframe`, but receiving 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:549,security,sign,significant,549,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:12,testability,trace,traceback,12,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:149,testability,Trace,Traceback,149,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:233,testability,test,test,233,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:193,usability,input,input-,193,"This is the traceback I get when I receive no results:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-91-65d9edda0106> in <module>(). 1 test = gprofiler(list(module_membership['MEpink']), . 2 custom_bg=list(np.unique([v.split(""."")[0] for v in var.index.tolist()])),. ----> 3 organism='mmusculus', correction_method='fdr', src_filter=['GO:BP']). ~/anaconda3/lib/python3.6/site-packages/gprofiler/__init__.py in gprofiler(query, organism, ordered_query, significant, exclude_iea, region_query, max_p_value, max_set_size, correction_method, hier_filtering, domain_size, custom_bg, numeric_ns, no_isects, png_fn, include_graph, src_filter). 147 ""query.size"", ""overlap.size"", ""recall"", ""precision"",. 148 ""term.id"", ""domain"", ""subgraph.number"", ""term.name"",. --> 149 ""relative.depth"", ""intersection""]. 150 enrichment.index = enrichment['term.id']. 151 numeric_columns = [""query.number"", ""p.value"", ""term.size"",. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __setattr__(self, name, value). 4387 try:. 4388 object.__getattribute__(self, name). -> 4389 return object.__setattr__(self, name, value). 4390 except AttributeError:. 4391 pass. pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__(). ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in _set_axis(self, axis, labels). 644 . 645 def _set_axis(self, axis, labels):. --> 646 self._data.set_axis(axis, labels). 647 self._clear_item_cache(). 648 . ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in set_axis(self, axis, new_labels). 3321 raise ValueError(. 3322 'Length mismatch: Expected axis has {old} elements, new '. -> 3323 'values have {new} elements'.format(old=old_len, new=new_len)). 3324 . 3325 self.axes[axis] = new_labels. ValueError: Length mismatch: Expected axis has 0 elements, new values have 14 elements. ```. I guess it has to do with the output expecting 14 valu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:271,reliability,Doe,Does,271,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:227,safety,test,tests,227,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:316,safety,test,tests,316,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:227,testability,test,tests,227,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:316,testability,test,tests,316,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:60,usability,prefer,prefer,60,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:292,usability,prefer,preferred,292,"How about an empty `DataFrame` instead of `None`? I think I prefer `len(results) == 0` to `results is None`. Also, I need to look into the arguments to `gprofiler` a bit more before this is ready to merge. I'd also like to add tests, but probably ones that are optional. Does `scanpy` have a preferred way of adding tests that don't run by default?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:368,deployability,modul,module,368,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:457,energy efficiency,core,core,457,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:630,interoperability,format,format,630,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:368,modifiability,modul,module,368,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:441,modifiability,pac,packages,441,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:342,safety,input,input-,342,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:368,safety,modul,module,368,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:298,testability,Trace,Traceback,298,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:342,usability,input,input-,342,"Eh, good question what would be better. I think both work, but if we return an empty one, we have to make sure that the column names and types are still correct. ```py. In [4]: bool(pd.DataFrame(dict(a=[]))) . ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-4-5108fad10cc0> in <module>. ----> 1 bool(pd.DataFrame(dict(a=[]))). /usr/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self). 1477 raise ValueError(""The truth value of a {0} is ambiguous. "". 1478 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."". -> 1479 .format(self.__class__.__name__)). 1480 . 1481 __bool__ = __nonzero__. ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). In [5]: pd.DataFrame(dict(a=[])).empty . Out[5]: True. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:40,reliability,stabil,stability,40,"I liked returning a data frame for type stability (i.e. this method always returns a data frame). However, you bring up a good point about the columns and dtypes of the data frame being part of it's type. Because it'd add more complexity that it's worth and type stability isn't that much of a thing in python, I've gone ahead with `None`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:263,reliability,stabil,stability,263,"I liked returning a data frame for type stability (i.e. this method always returns a data frame). However, you bring up a good point about the columns and dtypes of the data frame being part of it's type. Because it'd add more complexity that it's worth and type stability isn't that much of a thing in python, I've gone ahead with `None`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:227,safety,compl,complexity,227,"I liked returning a data frame for type stability (i.e. this method always returns a data frame). However, you bring up a good point about the columns and dtypes of the data frame being part of it's type. Because it'd add more complexity that it's worth and type stability isn't that much of a thing in python, I've gone ahead with `None`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:227,security,compl,complexity,227,"I liked returning a data frame for type stability (i.e. this method always returns a data frame). However, you bring up a good point about the columns and dtypes of the data frame being part of it's type. Because it'd add more complexity that it's worth and type stability isn't that much of a thing in python, I've gone ahead with `None`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:112,availability,servic,services,112,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:112,deployability,servic,services,112,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:132,deployability,depend,depend,132,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:188,deployability,updat,update,188,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:234,deployability,pipelin,pipelines,234,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:357,deployability,version,version,357,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:443,deployability,API,API,443,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:522,deployability,API,API,522,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:23,energy efficiency,Profil,Profiler,23,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:419,energy efficiency,Profil,Profiler,419,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:112,integrability,servic,services,112,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:132,integrability,depend,depend,132,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:234,integrability,pipelin,pipelines,234,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:248,integrability,wrap,wrappers,248,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:357,integrability,version,version,357,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:443,integrability,API,API,443,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:522,integrability,API,API,522,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:248,interoperability,wrapper,wrappers,248,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:443,interoperability,API,API,443,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:522,interoperability,API,API,522,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:112,modifiability,servic,services,112,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:132,modifiability,depend,depend,132,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:178,modifiability,extens,extensive,178,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:290,modifiability,pac,packages,290,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:357,modifiability,version,version,357,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:486,modifiability,pac,package,486,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:534,modifiability,pac,package,534,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:23,performance,Profil,Profiler,23,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:419,performance,Profil,Profiler,419,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:132,safety,depend,depend,132,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:188,safety,updat,update,188,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:70,security,team,team,70,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:188,security,updat,update,188,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:380,security,access,access,380,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:132,testability,depend,depend,132,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:558,usability,progress,progress,558,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:654,usability,help,help,654,"Hey! I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,. Liis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:140,deployability,API,API,140,@liiskolb Thanks for letting us know. Where can I track the python package at? Could you also point me towards the old and new docs for the API? I'd like to check what's different. Thanks! @flying-sheep I'm thinking I'll drop the `enrich` function from this PR until this is resolved. Thoughts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:140,integrability,API,API,140,@liiskolb Thanks for letting us know. Where can I track the python package at? Could you also point me towards the old and new docs for the API? I'd like to check what's different. Thanks! @flying-sheep I'm thinking I'll drop the `enrich` function from this PR until this is resolved. Thoughts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:140,interoperability,API,API,140,@liiskolb Thanks for letting us know. Where can I track the python package at? Could you also point me towards the old and new docs for the API? I'd like to check what's different. Thanks! @flying-sheep I'm thinking I'll drop the `enrich` function from this PR until this is resolved. Thoughts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:67,modifiability,pac,package,67,@liiskolb Thanks for letting us know. Where can I track the python package at? Could you also point me towards the old and new docs for the API? I'd like to check what's different. Thanks! @flying-sheep I'm thinking I'll drop the `enrich` function from this PR until this is resolved. Thoughts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:69,deployability,API,API,69,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:259,deployability,api,apis,259,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:47,integrability,pub,public,47,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:69,integrability,API,API,69,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:259,integrability,api,apis,259,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:69,interoperability,API,API,69,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:259,interoperability,api,apis,259,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:345,interoperability,format,format,345,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:32,modifiability,pac,package,32,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:282,modifiability,paramet,parameters,282,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:154,usability,help,help,154,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:425,usability,tool,tool,425,"@ivirshup . Our official Python package is not public yet. . The old API description is here: https://biit.cs.ut.ee/gprofiler_archive2/r1760_e93_eg40/web/help.cgi?help_id=55 . and the new one with Python examples is here: https://biit.cs.ut.ee/gprofiler/page/apis. So, not only the parameters changed but also the request type, paths and output format (it's JSON now). . Let me know if you have any other questions about the tool, results, enrichment analysis, etc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:110,deployability,API,API,110,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:57,energy efficiency,current,currently,57,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:94,energy efficiency,profil,profiler,94,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:110,integrability,API,API,110,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:110,interoperability,API,API,110,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:94,performance,profil,profiler,94,"Hi,. This is very relevant information for me as well. I currently use valentine svensson's g:profiler python API (https://github.com/vals/python-gprofiler) in my single-cell tutorial (https://github.com/theislab/single-cell-tutorial). I'm guessing that will be similarly outdated soon... I would thus be quite keen for this to be included in scanpy @ivirshup ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:42,deployability,API,APIs,42,"I made some changes regarding how we want APIs defined from now on. @ivirshup I hope I didn’t make any mistakes, I’m still a bit sick. Could you please check my changes in 4550142723e631e20d3e5e83be2fca0003b73571?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:42,integrability,API,APIs,42,"I made some changes regarding how we want APIs defined from now on. @ivirshup I hope I didn’t make any mistakes, I’m still a bit sick. Could you please check my changes in 4550142723e631e20d3e5e83be2fca0003b73571?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:42,interoperability,API,APIs,42,"I made some changes regarding how we want APIs defined from now on. @ivirshup I hope I didn’t make any mistakes, I’m still a bit sick. Could you please check my changes in 4550142723e631e20d3e5e83be2fca0003b73571?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:78,deployability,releas,released,78,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:314,deployability,updat,updating,314,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:34,energy efficiency,estimat,estimate,34,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:62,modifiability,pac,package,62,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:314,safety,updat,updating,314,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:314,security,updat,updating,314,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure! Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:47,deployability,releas,released,47,"@ivirshup We estimate that the package will be released around 15th of April. So, in a month or so. . If this is ok, then I'll let you know when it is out:)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:13,energy efficiency,estimat,estimate,13,"@ivirshup We estimate that the package will be released around 15th of April. So, in a month or so. . If this is ok, then I'll let you know when it is out:)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:31,modifiability,pac,package,31,"@ivirshup We estimate that the package will be released around 15th of April. So, in a month or so. . If this is ok, then I'll let you know when it is out:)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:192,deployability,version,version,192,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:311,deployability,API,API,311,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:321,deployability,releas,released,321,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:421,deployability,provis,provisional,421,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:395,energy efficiency,current,current,395,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:192,integrability,version,version,192,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:311,integrability,API,API,311,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:311,interoperability,API,API,311,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:192,modifiability,version,version,192,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released. 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:18,deployability,updat,updated,18,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:48,deployability,version,version,48,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:90,deployability,API,API,90,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:48,integrability,version,version,48,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:90,integrability,API,API,90,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:90,interoperability,API,API,90,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:48,modifiability,version,version,48,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:18,safety,updat,updated,18,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:18,security,updat,updated,18,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API. See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:148,usability,help,helpful,148,"@ivirshup Sorry, I didn't see this until just now. I guess @liiskolb's comment addresses your options though... as it's out now, it would be really helpful to have in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:17,deployability,updat,updates,17,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:158,integrability,wrap,wrapper,158,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:119,interoperability,conflict,conflicts,119,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:158,interoperability,wrapper,wrapper,158,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:17,safety,updat,updates,17,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:17,security,updat,updates,17,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:479,usability,person,personally,479,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this? Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:15,energy efficiency,current,current,15,"@flying-sheep, current thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:313,deployability,version,version,313,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:344,deployability,version,version,344,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:420,deployability,version,version,420,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:452,deployability,version,version,452,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:527,deployability,releas,release,527,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:569,deployability,updat,updates,569,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:775,deployability,updat,update,775,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:126,integrability,wrap,wrapper,126,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:313,integrability,version,version,313,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:344,integrability,version,version,344,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:420,integrability,version,version,420,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:452,integrability,version,version,452,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:87,interoperability,conflict,conflicts,87,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:126,interoperability,wrapper,wrapper,126,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:313,modifiability,version,version,313,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:344,modifiability,version,version,344,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:420,modifiability,version,version,420,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:452,modifiability,version,version,452,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:788,modifiability,pac,packages,788,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:569,safety,updat,updates,569,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:775,safety,updat,update,775,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:569,security,updat,updates,569,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:775,security,updat,update,775,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:636,usability,experien,experience,636,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:160,deployability,instal,installed,160,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:486,deployability,version,version,486,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:540,deployability,version,version,540,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:705,deployability,instal,installed,705,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:486,integrability,version,version,486,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:540,integrability,version,version,540,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:553,interoperability,format,format,553,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:78,modifiability,pac,packages,78,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:486,modifiability,version,version,486,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:540,modifiability,version,version,540,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:427,reliability,doe,does,427,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:473,safety,detect,detect,473,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:624,safety,input,inputs,624,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:473,security,detect,detect,473,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:316,usability,experien,experienced,316,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:624,usability,input,inputs,624,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:260,deployability,instal,installed,260,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:38,modifiability,pac,packages,38,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:125,modifiability,pac,package,125,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:225,security,control,control,225,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:225,testability,control,control,225,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/issues/468:126,reliability,doe,doesn,126,"Well, of course it should play nice with `importlib.reload`! It was an easy fix, I just needed to make sure that the function doesn’t cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:85,deployability,updat,updated,85,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:165,modifiability,pac,package,165,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:392,reliability,doe,does,392,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:85,safety,updat,updated,85,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:177,safety,compl,complex,177,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:85,security,updat,updated,85,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:177,security,compl,complex,177,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload??? But the following worked for me. . ```. ipython. In [1]: %load_ext autoreload. In [2]: %autoreload 2. ln [3]: import scanpy as sc. ln [4]: sc.plotting._tools.scatterplots.tr_test(). [does nothing as expected, then change source to print something out]. ln [5]: sc.plotting._tools.scatterplots.tr_test(). hellya!!! ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:210,deployability,upgrad,upgraded,210,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:226,deployability,version,versions,226,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:226,integrability,version,versions,226,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:111,modifiability,extens,extensions,111,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:210,modifiability,upgrad,upgraded,210,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:226,modifiability,version,versions,226,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:19,reliability,doe,does,19,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/472:114,deployability,log,logging,114,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:161,deployability,log,logging,161,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:478,deployability,log,logging,478,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:571,deployability,Log,Logs,571,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:805,deployability,log,logger,805,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:832,deployability,log,logged,832,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1116,deployability,log,logger,1116,"ng. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1385,deployability,log,logger,1385,"rovenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1497,deployability,log,logged,1497,"le quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertoo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2330,deployability,log,logging,2330,"l_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2588,deployability,log,logger,2588,"y as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2615,deployability,log,logged,2615,"s.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_reco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2738,deployability,log,log,2738,"3dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3241,deployability,log,logged,3241,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3249,deployability,log,logger,3249,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3574,deployability,log,logger,3574,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3711,deployability,log,logged,3711,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:496,integrability,coupl,couple,496,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:727,integrability,wrap,wraps,727,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:848,integrability,wrap,wraps,848,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2465,integrability,wrap,wraps,2465,"turn func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2867,integrability,wrap,wraps,2867,"foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:496,modifiability,coupl,couple,496,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:419,performance,perform,performance,419,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:609,performance,time,time,609,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:773,performance,time,time,773,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1891,performance,time,timedelta,1891," call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2187,performance,time,timedelta,2187,"*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2556,performance,time,time,2556,"return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:4156,performance,time,timedelta,4156,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:114,safety,log,logging,114,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:161,safety,log,logging,161,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:478,safety,log,logging,478,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:571,safety,Log,Logs,571,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:805,safety,log,logger,805,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:832,safety,log,logged,832,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1116,safety,log,logger,1116,"ng. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1385,safety,log,logger,1385,"rovenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1497,safety,log,logged,1497,"le quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertoo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2290,safety,compl,complicated,2290,"=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2330,safety,log,logging,2330,"l_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2588,safety,log,logger,2588,"y as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2615,safety,log,logged,2615,"s.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_reco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2738,safety,log,log,2738,"3dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3241,safety,log,logged,3241,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3249,safety,log,logger,3249,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3574,safety,log,logger,3574,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3711,safety,log,logged,3711,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:114,security,log,logging,114,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:161,security,log,logging,161,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:280,security,Control,Control,280,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:478,security,log,logging,478,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:571,security,Log,Logs,571,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:805,security,log,logger,805,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:832,security,log,logged,832,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1116,security,log,logger,1116,"ng. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1385,security,log,logger,1385,"rovenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1497,security,log,logged,1497,"le quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertoo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2290,security,compl,complicated,2290,"=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2330,security,log,logging,2330,"l_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2588,security,log,logger,2588,"y as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2615,security,log,logged,2615,"s.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_reco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2738,security,log,log,2738,"3dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3241,security,log,logged,3241,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3249,security,log,logger,3249,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3574,security,log,logger,3574,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3711,security,log,logged,3711,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:114,testability,log,logging,114,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:161,testability,log,logging,161,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:280,testability,Control,Control,280,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:478,testability,log,logging,478,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:496,testability,coupl,couple,496,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:555,testability,Simpl,Simple,555,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:571,testability,Log,Logs,571,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:805,testability,log,logger,805,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:832,testability,log,logged,832,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1116,testability,log,logger,1116,"ng. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1385,testability,log,logger,1385,"rovenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1497,testability,log,logged,1497,"le quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertoo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2330,testability,log,logging,2330,"l_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2588,testability,log,logger,2588,"y as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2615,testability,log,logged,2615,"s.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_reco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2738,testability,log,log,2738,"3dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo. # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3241,testability,log,logged,3241,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3249,testability,log,logger,3249,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3574,testability,log,logger,3574,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:3711,testability,log,logged,3711,"turned_adata_id=4940502352. ```. </details>. <details>. <summary>More complicated example with argument value logging</summary>. ```python. from anndata import AnnData. from copy import copy. from datetime import datetime. from functools import wraps. import inspect. from itertools import chain. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(logged_args=None):. """""". Params. ------. logged_args : list[str], optional (default: `None`). Names of arguments to log. """""". if logged_args is None:. logged_args = []. def logged_decorator(func):. argnames = inspect.getfullargspec(func).args. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):. if type(val) is AnnData:. logged_params[param] = id(val). elif param in logged_args:. logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,. logged_args=logged_params, call_id=call_id). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(. called_func=func.__name__, elapsed=dt,. ). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. return logged_decorator. # Usage. @logged(logged_args=[""x""]). def foo(adata, x, copy=True):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True). # 2019-02-13 19:35.48 call call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo logged_args={'adata': 4476410456, 'x': 1}. # 2019-02-13 19:35.48 call_finish call_id=UUID('f7623504-31c5-4afa-ae26-4f58fc5341a8') called_func=foo elapsed=datetime.timedelta(microseconds=507880) returned_adata_id=5064494384. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:354,usability,custom,customized,354,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:376,usability,user,user,376,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:419,usability,perform,performance,419,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:555,usability,Simpl,Simple,555,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where. * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python. from anndata import AnnData. from datetime import datetime. from functools import wraps. from structlog import get_logger. from time import sleep. import uuid. logger = get_logger(). def logged(func):. @wraps(func). def func_wrapper(*args, **kwargs):. call_id = uuid.uuid4() # So we can always match call start with call end. call_start_record = dict(call_id=call_id, called_func=func.__name__). if type(args[0]) is AnnData:. call_start_record[""adata_id""] = id(args[0]). logger.msg(""call"", **call_start_record). t0 = datetime.now(). output = func(*args, **kwargs). dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt). if type(output) is AnnData:. call_finish_record[""returned_adata_id""] = id(output). logger.msg(""call_finish"", **call_finish_record, call_id=call_id). return output. return func_wrapper. # Usage. @logged. def foo(adata, x, copy=False):. sleep(0.5). if copy: return adata.copy(). import scanpy as sc. pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1). # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo. # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777). foo(pbmcs, 1, copy=True);. # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:464,availability,operat,operations,464,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:58,deployability,log,logging,58,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:291,deployability,API,API,291,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:320,deployability,log,logging,320,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:291,integrability,API,API,291,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:291,interoperability,API,API,291,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:227,modifiability,deco,decorators,227,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:58,safety,log,logging,58,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:320,safety,log,logging,320,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:58,security,log,logging,58,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:320,security,log,logging,320,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:58,testability,log,logging,58,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:320,testability,log,logging,320,"Interesting ideas. I actually didn't notice scanpy has no logging implemented - this would indeed be useful and could already solve half the problem indeed. However, I doubt the best way to go about this would be post hoc with decorators etc, but rather intrinsically throughout the various API functions. Regardless of logging, I still think that having something which is intrinsically attached to the object would have the advantage of knowing the exact set of operations solely from the h5ad file/AnnData object itself. Don't know if people are actually out there are also sharing these or not but it could be useful from that perspective too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:674,availability,consist,consistency,674,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:714,availability,consist,consistency,714,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:859,availability,operat,operations,859,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:17,deployability,log,logging,17,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:729,deployability,log,logged,729,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1739,deployability,log,logs,1739,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1862,deployability,log,logging,1862,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1913,deployability,log,logging,1913,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2129,deployability,log,log,2129,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2149,deployability,automat,automatically,2149,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:617,integrability,abstract,abstracting,617,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:736,integrability,messag,messages,736,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:933,integrability,wrap,wrapped,933,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:736,interoperability,messag,messages,736,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:184,modifiability,variab,variable,184,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:589,modifiability,deco,decorator,589,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:617,modifiability,abstract,abstracting,617,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:764,modifiability,concern,concerns,764,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:948,modifiability,deco,decorator,948,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1171,performance,cach,cache,1171,", [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1642,performance,cach,cache,1642,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1950,performance,cach,cache,1950,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:7,reliability,doe,does,7,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:17,safety,log,logging,17,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:729,safety,log,logged,729,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1739,safety,log,logs,1739,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1862,safety,log,logging,1862,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1913,safety,log,logging,1913,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2129,safety,log,log,2129,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:17,security,log,logging,17,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:729,security,log,logged,729,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1739,security,log,logs,1739,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1798,security,access,accessible,1798,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1862,security,log,logging,1862,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1913,security,log,logging,1913,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2129,security,log,log,2129,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:17,testability,log,logging,17,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:729,testability,log,logged,729,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:764,testability,concern,concerns,764,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1739,testability,log,logs,1739,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1862,testability,log,logging,1862,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1913,testability,log,logging,1913,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2129,testability,log,log,2129,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2149,testability,automat,automatically,2149,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:674,usability,consist,consistency,674,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:714,usability,consist,consistency,714,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1817,usability,help,helper,1817,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:2005,usability,help,helper,2005,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). ```. Should result in a set of (psuedo-)records like:. ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}. {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}. {""call"": ""log1p"", ""adata_id"": id(1)}. {""call"": ""pca"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:229,availability,consist,consistent,229,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:428,availability,operat,operations,428,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:912,availability,operat,operations,912,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:19,deployability,log,logging,19,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:63,deployability,log,logfile,63,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:240,deployability,log,logging,240,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:314,deployability,log,logging,314,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:753,deployability,scale,scale,753,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:823,deployability,log,logging,823,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:995,deployability,scale,scale,995,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1186,deployability,log,log,1186,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:378,energy efficiency,cool,cool,378,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:753,energy efficiency,scale,scale,753,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:995,energy efficiency,scale,scale,995,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:753,modifiability,scal,scale,753,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:995,modifiability,scal,scale,995,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:439,performance,perform,performed,439,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:664,performance,cach,cache,664,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:715,performance,cach,cache,715,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:753,performance,scale,scale,753,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:782,performance,cach,cache,782,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:995,performance,scale,scale,995,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1066,performance,cach,cache,1066,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:19,safety,log,logging,19,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:63,safety,log,logfile,63,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:240,safety,log,logging,240,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:314,safety,log,logging,314,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:823,safety,log,logging,823,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1186,safety,log,log,1186,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:19,security,log,logging,19,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:63,security,log,logfile,63,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:240,security,log,logging,240,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:314,security,log,logging,314,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:823,security,log,logging,823,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1186,security,log,log,1186,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:19,testability,log,logging,19,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:63,testability,log,logfile,63,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:240,testability,log,logging,240,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:314,testability,log,logging,314,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:823,testability,log,logging,823,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:1186,testability,log,log,1186,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:175,usability,interact,interactively,175,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:229,usability,consist,consistent,229,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:439,usability,perform,performed,439,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization? e.g.:. ```python. adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""). sc.pp.normalize_per_cell(adata, 1000). sc.pp.log1p(adata). sc.pp.pca(adata). adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""). sc.pp.scale(adata). adata.write(""./cache/01_simple_process.h5ad""). print(sc.logging.get_operations(adata_id=id(adata))). ```. would probably forget the first set of operations? ```. # Where id(1) is a stand in for value like `id(adata)`. {""call"": ""scale"", ""adata_id"": id(1)}. {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}. ```. I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:252,deployability,log,logs,252,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:297,deployability,log,logs,297,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:486,deployability,log,logging,486,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:521,deployability,log,logger,521,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:624,deployability,log,logic,624,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:767,deployability,log,logged,767,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:914,deployability,log,logging,914,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:942,deployability,log,logs,942,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:637,modifiability,deco,decorator,637,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:252,safety,log,logs,252,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:297,safety,log,logs,297,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:357,safety,compl,complicated,357,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:486,safety,log,logging,486,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:521,safety,log,logger,521,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:624,safety,log,logic,624,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:767,safety,log,logged,767,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:914,safety,log,logging,914,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:942,safety,log,logs,942,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:142,security,sign,signify,142,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:252,security,log,logs,252,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:297,security,log,logs,297,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:357,security,compl,complicated,357,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:486,security,log,logging,486,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:521,security,log,logger,521,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:624,security,log,logic,624,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:728,security,modif,modified,728,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:767,security,log,logged,767,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:914,security,log,logging,914,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:942,security,log,logs,942,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:252,testability,log,logs,252,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:297,testability,log,logs,297,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:486,testability,log,logging,486,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:494,testability,context,context,494,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:521,testability,log,logger,521,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:624,testability,log,logic,624,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:767,testability,log,logged,767,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:914,testability,log,logging,914,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:942,testability,log,logs,942,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:514,usability,custom,custom,514,"Yeah that's what I was thinking for tracking between serializations. I figure there could be a boolean argument like `exhaustive` which would signify whether you want this particular AnnData or all previous `AnnData`s this could be derived from in the logs. I think it'll be possible to write the logs to some field in an object. There is a question of how complicated this would be to implement, which I haven't quite figured out yet. Maybe you'd add a reference to the AnnData to the logging context, and make a custom logger which decides where to write based on that? Alternatively, maybe this just gets handled by some logic in the decorator. So after a method is called, there's a flag about whether to add records to the modified object. Of course, nothing is logged persistently by default, so it's already an extra step to enable. It's possible sharing could just need two extra steps, ""enable persistent logging"" and ""send them the logs"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:4,deployability,log,logging,4,"`sc.logging.get_operations` with `exhaustive` would be great, but if one could find a way to store the same persistently or in the object too upon the user's request that would cover all the ground.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:4,safety,log,logging,4,"`sc.logging.get_operations` with `exhaustive` would be great, but if one could find a way to store the same persistently or in the object too upon the user's request that would cover all the ground.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:4,security,log,logging,4,"`sc.logging.get_operations` with `exhaustive` would be great, but if one could find a way to store the same persistently or in the object too upon the user's request that would cover all the ground.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:4,testability,log,logging,4,"`sc.logging.get_operations` with `exhaustive` would be great, but if one could find a way to store the same persistently or in the object too upon the user's request that would cover all the ground.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:151,usability,user,user,151,"`sc.logging.get_operations` with `exhaustive` would be great, but if one could find a way to store the same persistently or in the object too upon the user's request that would cover all the ground.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/473:324,integrability,sub,subscribed,324,"Can you post an example showing the problem that we could reproduce? scanpy. includes some datasets that can be used for this cases. On Mon, Feb 11, 2019 at 7:58 AM yyoshiaki <notifications@github.com> wrote:. > Reopened #473 <https://github.com/theislab/scanpy/issues/473>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/473#event-2129784560>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XSW2xK7T-LJxKAklSaxzgM7L_Wuks5vMRSXgaJpZM4azXnm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:451,integrability,event,event-,451,"Can you post an example showing the problem that we could reproduce? scanpy. includes some datasets that can be used for this cases. On Mon, Feb 11, 2019 at 7:58 AM yyoshiaki <notifications@github.com> wrote:. > Reopened #473 <https://github.com/theislab/scanpy/issues/473>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/473#event-2129784560>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XSW2xK7T-LJxKAklSaxzgM7L_Wuks5vMRSXgaJpZM4azXnm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:541,security,auth,auth,541,"Can you post an example showing the problem that we could reproduce? scanpy. includes some datasets that can be used for this cases. On Mon, Feb 11, 2019 at 7:58 AM yyoshiaki <notifications@github.com> wrote:. > Reopened #473 <https://github.com/theislab/scanpy/issues/473>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/473#event-2129784560>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1XSW2xK7T-LJxKAklSaxzgM7L_Wuks5vMRSXgaJpZM4azXnm>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:188,modifiability,paramet,parameter,188,Is this related to having white in the color palette for plotting? I have noticed this as well. You can always create your own color palettes using `matplotlib.colors` and the `color_map` parameter. Although I'm not sure how this works with categorical data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:340,testability,verif,verified,340,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:501,testability,verif,verify,501,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:564,testability,simpl,simple,564,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:118,usability,user,user-images,118,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:453,usability,person,personal,453,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:564,usability,simpl,simple,564,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. I verified the same thing on another environment, but it didn't happen. So this might be critically relating to my personal environment. I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python. %matplotlib inline. # 2 lines below solved the facecolor problem. # import matplotlib as mpl. # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:241,modifiability,paramet,parameter,241,"Sorry for opening an issue over and over. I noticed that the declear below is inconpatible with `set_figure_params`. ```python. import matplotlib as mpl. mpl.rcParams['figure.facecolor'] = 'white'. ```. If possible, I hope facecolor setting parameter is added https://github.com/theislab/scanpy/blob/master/scanpy/settings.py#L85-L136. Best,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:284,integrability,messag,message,284,"What do you mean? In which way is it incompatible? In matplotlib/matplotlib#9698 it’s said that. > The goal is to ultimately replace setting `savefig.transparent` by. `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:37,interoperability,incompatib,incompatible,37,"What do you mean? In which way is it incompatible? In matplotlib/matplotlib#9698 it’s said that. > The goal is to ultimately replace setting `savefig.transparent` by. `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:284,interoperability,messag,message,284,"What do you mean? In which way is it incompatible? In matplotlib/matplotlib#9698 it’s said that. > The goal is to ultimately replace setting `savefig.transparent` by. `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:233,modifiability,paramet,parameter,233,"What do you mean? In which way is it incompatible? In matplotlib/matplotlib#9698 it’s said that. > The goal is to ultimately replace setting `savefig.transparent` by. `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:682,integrability,messag,message,682,"I was wrong, it worked well... ```python. %matplotlib inline. import scanpy as sc. import matplotlib as mpl. # 2 lines below solved the facecolor problem. mpl.rcParams['figure.facecolor'] = 'white'. sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```. ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind. Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html. - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:682,interoperability,messag,message,682,"I was wrong, it worked well... ```python. %matplotlib inline. import scanpy as sc. import matplotlib as mpl. # 2 lines below solved the facecolor problem. mpl.rcParams['figure.facecolor'] = 'white'. sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```. ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind. Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html. - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:631,modifiability,paramet,parameter,631,"I was wrong, it worked well... ```python. %matplotlib inline. import scanpy as sc. import matplotlib as mpl. # 2 lines below solved the facecolor problem. mpl.rcParams['figure.facecolor'] = 'white'. sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```. ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind. Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html. - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:493,usability,user,user-images,493,"I was wrong, it worked well... ```python. %matplotlib inline. import scanpy as sc. import matplotlib as mpl. # 2 lines below solved the facecolor problem. mpl.rcParams['figure.facecolor'] = 'white'. sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'). ```. ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind. Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html. - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/pull/474:356,deployability,version,version,356,"I've added a decorator for renaming arguments, `@deprecated_arg_names`. Should this have gone in `AnnData.utils`? Decorator example usage:. ```python. @deprecated_arg_names({""b"": ""c""}). def foo(a, c=2):. return a + c. foo(a, b=1). # __main__:1: DeprecationWarning: Keyword argument 'b' has been deprecated in favour of 'c'. 'b' will be removed in a future version. # 2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:356,integrability,version,version,356,"I've added a decorator for renaming arguments, `@deprecated_arg_names`. Should this have gone in `AnnData.utils`? Decorator example usage:. ```python. @deprecated_arg_names({""b"": ""c""}). def foo(a, c=2):. return a + c. foo(a, b=1). # __main__:1: DeprecationWarning: Keyword argument 'b' has been deprecated in favour of 'c'. 'b' will be removed in a future version. # 2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:13,modifiability,deco,decorator,13,"I've added a decorator for renaming arguments, `@deprecated_arg_names`. Should this have gone in `AnnData.utils`? Decorator example usage:. ```python. @deprecated_arg_names({""b"": ""c""}). def foo(a, c=2):. return a + c. foo(a, b=1). # __main__:1: DeprecationWarning: Keyword argument 'b' has been deprecated in favour of 'c'. 'b' will be removed in a future version. # 2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:114,modifiability,Deco,Decorator,114,"I've added a decorator for renaming arguments, `@deprecated_arg_names`. Should this have gone in `AnnData.utils`? Decorator example usage:. ```python. @deprecated_arg_names({""b"": ""c""}). def foo(a, c=2):. return a + c. foo(a, b=1). # __main__:1: DeprecationWarning: Keyword argument 'b' has been deprecated in favour of 'c'. 'b' will be removed in a future version. # 2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:356,modifiability,version,version,356,"I've added a decorator for renaming arguments, `@deprecated_arg_names`. Should this have gone in `AnnData.utils`? Decorator example usage:. ```python. @deprecated_arg_names({""b"": ""c""}). def foo(a, c=2):. return a + c. foo(a, b=1). # __main__:1: DeprecationWarning: Keyword argument 'b' has been deprecated in favour of 'c'. 'b' will be removed in a future version. # 2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:68,deployability,api,api-wrap,68,Hi! I’ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account? There’s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:68,integrability,api,api-wrap,68,Hi! I’ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account? There’s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:68,interoperability,api,api-wrap,68,Hi! I’ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account? There’s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:86,performance,time,time,86,Hi! I’ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account? There’s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:42,usability,custom,custom,42,"Right, I'd also suspect that a non-scanpy-custom solution for deprecated arguments is the way to go. For now, this is fine; we can replace it if we transition there. If @flying-sheep would go about replacing this with `legaciy_api_wrap` (after or before merging), this looks like a very good solution to me?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:223,integrability,abstract,abstract,223,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:65,interoperability,specif,specific,65,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:144,interoperability,specif,specific,144,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:223,modifiability,abstract,abstract,223,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:771,modifiability,deco,decorator,771,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:336,reliability,doe,doesn,336,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:428,reliability,doe,doesn,428,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:502,reliability,Doe,Does,502,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:817,safety,compl,complexity,817,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:817,security,compl,complexity,817,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:193,usability,user,user,193,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:609,usability,tool,tools,609,"@flying-sheep I think changing the name of a keyword argument is specific enough to warrant it's own functionality. * This allows giving a very specific warning about what should change in the user's code. * This can fully abstract away the functional change (checking if the old argument was used, and renaming), so the implementation doesn't have any code dealing with the deprecation. As far as I can tell, `legacy_api_wrap` doesn't quite cover this use case, as it deals with positional arguments. Does that sound right to you? @falexwolf If there was a commonly used, high quality library which provided tools for handling deprecations like this I wouldn't mind switching over to that. After some light googling, I didn't find one. Ultimately, it's a pretty trivial decorator, and I don't think it's adding much complexity here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:2,safety,compl,completely,2,"I completely get your point, @ivirshup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:2,security,compl,completely,2,"I completely get your point, @ivirshup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:62,deployability,api,api-wrap,62,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:92,deployability,API,API,92,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:180,deployability,API,API,180,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:62,integrability,api,api-wrap,62,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:92,integrability,API,API,92,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:180,integrability,API,API,180,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:62,interoperability,api,api-wrap,62,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:92,interoperability,API,API,92,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:180,interoperability,API,API,180,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:127,modifiability,deco,decorators,127,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:247,modifiability,deco,decorators,247,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:152,usability,clear,clearer,152,"OK, so now the question is: should this become part of legacy-api-wrap? I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py. @renamed_args(new=""old""). ```. feels more natural than. ```py. @deprecated_arg_names({""old"": ""new""}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:80,deployability,API,APIs,80,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:131,deployability,api,api,131,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:256,deployability,api,api,256,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:585,deployability,version,version,585,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:80,integrability,API,APIs,80,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:131,integrability,api,api,131,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:256,integrability,api,api,256,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:585,integrability,version,version,585,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:80,interoperability,API,APIs,80,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:131,interoperability,api,api,131,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:256,interoperability,api,api,256,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:573,interoperability,specif,specify,573,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:61,modifiability,maintain,maintaining,61,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:144,modifiability,deco,decorator,144,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:317,modifiability,deco,decorators,317,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:509,modifiability,deco,decorator,509,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:585,modifiability,version,version,585,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:61,safety,maintain,maintaining,61,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:50,usability,experien,experience,50,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:388,usability,help,help,388,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:393,usability,guid,guide,393,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:428,usability,prefer,prefer,428,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:736,usability,intuit,intuitive,736,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:67,modifiability,deco,decorator,67,"Right, there were two changes, the bigger one I've fixed with that decorator. Since the smaller breaking change is just a change of default bool, I could split that out, but I'm not sure how you want to handle that. If there was a `v1.5` branch, it could go there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:126,deployability,releas,release,126,"Thanks for clarifying this again, @ivirshup! We should have changed the default value already for 1.4. I'll add a note to the release notes and it's fine... ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/issues/475:160,security,auth,authored,160,"Thanks! > On Feb 12, 2019, at 01:14, Philipp A. <notifications@github.com> wrote:. > . > Closed #475 via 86f189e. > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/475
https://github.com/scverse/scanpy/issues/475:89,usability,Close,Closed,89,"Thanks! > On Feb 12, 2019, at 01:14, Philipp A. <notifications@github.com> wrote:. > . > Closed #475 via 86f189e. > . > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/475
https://github.com/scverse/scanpy/issues/476:134,usability,command,command,134,"@tsotnech I ran into the same issue a few months ago but found a fix by using the sc.pl.scatter() function instead. I have pasted the command here and it worked for me. from matplotlib import pyplot as plt. sc.pl.scatter(adata,color='NANOG',legend_loc='none',color_map=plt.cm.Reds,size=5,basis='umap'). Hopefully this works for you too. -Aditi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/issues/476:0,deployability,Continu,Continuous,0,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python. import scanpy as sc. import matplotlib as mpl. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=[""louvain"", ""HES4""]). sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds). ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/issues/476:177,interoperability,share,share,177,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python. import scanpy as sc. import matplotlib as mpl. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=[""louvain"", ""HES4""]). sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds). ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/issues/476:467,usability,clear,clearly,467,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python. import scanpy as sc. import matplotlib as mpl. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=[""louvain"", ""HES4""]). sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds). ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/issues/476:475,usability,document,documented,475,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python. import scanpy as sc. import matplotlib as mpl. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color=[""louvain"", ""HES4""]). sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds). ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/pull/477:57,reliability,doe,does,57,Could you please also mention what the default of `None` does? If I recall correctly:. - `color_map`: the default is `rcParams['image.cmap']`. - `palette`: the default are the colors in `rcParams['axes.prop_cycle']`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:205,modifiability,variab,variables,205,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look? ```. color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None). Color map to use for continous variables. Anything that works for `cmap`. argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,. `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used. palette : `str`, list of `str`, or `Cycler` optional (default: `None`). Colors to use for plotting categorical annotation groups. The palette can be. a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list. of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object. If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical. variable already has colors stored in `adata.uns[""{var}_colors""]`. ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:773,modifiability,variab,variable,773,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look? ```. color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None). Color map to use for continous variables. Anything that works for `cmap`. argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,. `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used. palette : `str`, list of `str`, or `Cycler` optional (default: `None`). Colors to use for plotting categorical annotation groups. The palette can be. a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list. of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object. If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical. variable already has colors stored in `adata.uns[""{var}_colors""]`. ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:76,reliability,doe,does,76,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look? ```. color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None). Color map to use for continous variables. Anything that works for `cmap`. argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,. `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used. palette : `str`, list of `str`, or `Cycler` optional (default: `None`). Colors to use for plotting categorical annotation groups. The palette can be. a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list. of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object. If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical. variable already has colors stored in `adata.uns[""{var}_colors""]`. ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:553,safety,valid,valid,553,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look? ```. color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None). Color map to use for continous variables. Anything that works for `cmap`. argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,. `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used. palette : `str`, list of `str`, or `Cycler` optional (default: `None`). Colors to use for plotting categorical annotation groups. The palette can be. a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list. of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object. If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical. variable already has colors stored in `adata.uns[""{var}_colors""]`. ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:37,usability,help,helpful,37,"Great! Yes, that’s probably all very helpful!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:36,deployability,continu,continuous,36,@flying-sheep @ivirshup The default continuous color map is usually set using `sc.set_figure_params` in the example tutorials. I am not totally sure but I think that internally the `rcParams` are modified. Maybe you can also mention this on the improved documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:196,security,modif,modified,196,@flying-sheep @ivirshup The default continuous color map is usually set using `sc.set_figure_params` in the example tutorials. I am not totally sure but I think that internally the `rcParams` are modified. Maybe you can also mention this on the improved documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:254,usability,document,documentation,254,@flying-sheep @ivirshup The default continuous color map is usually set using `sc.set_figure_params` in the example tutorials. I am not totally sure but I think that internally the `rcParams` are modified. Maybe you can also mention this on the improved documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:74,security,modif,modify,74,Sure! the docs `set_figure_params` should explain that the basically just modify the `rcParams`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:24,usability,document,document,24,"I'm of two minds how to document that. Either every plotting function gets a note saying that default values for some arguments can be changed globally with `sc.set_figure_params`, or add that note to the top of the plotting docs on the website.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:96,deployability,api,api,96,"Hmm, the docs for `set_figure_params` [already mention](https://scanpy.readthedocs.io/en/latest/api/index.html#settings) `rcParams`, but maybe it should be mentioned directly in the docs for `scanpy.plotting`, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:96,integrability,api,api,96,"Hmm, the docs for `set_figure_params` [already mention](https://scanpy.readthedocs.io/en/latest/api/index.html#settings) `rcParams`, but maybe it should be mentioned directly in the docs for `scanpy.plotting`, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:96,interoperability,api,api,96,"Hmm, the docs for `set_figure_params` [already mention](https://scanpy.readthedocs.io/en/latest/api/index.html#settings) `rcParams`, but maybe it should be mentioned directly in the docs for `scanpy.plotting`, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/issues/478:422,deployability,api,api,422,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:534,deployability,log,logging,534,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1647,energy efficiency,alloc,allocation,1647,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:422,integrability,api,api,422,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1866,integrability,sub,subscribed,1866,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:422,interoperability,api,api,422,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1658,interoperability,distribut,distribution,1658,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:795,modifiability,variab,variable,795,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:479,safety,test,testdir,479,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:534,safety,log,logging,534,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:534,security,log,logging,534,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:2063,security,auth,auth,2063,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:479,testability,test,testdir,479,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:534,testability,log,logging,534,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:671,usability,learn,learn,671,"I think the problem is the option `sort_order` which is True by default for. numerical data. This changes the ordering of the dots and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1380,usability,user,user-images,1380,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1512,usability,user,user-images,1512,"s and thus it messes. up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot. > with the following code. >. > import scanpy.api as sc. > import numpy as np. > sc.settings.figdir = ""testdir"". > sc.settings.file_format_figs = ""png"". > sc.logging.print_versions(). >. > With these libraries. > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4. > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Running the following code bit. I use some dummy variable for size. >. > somedata = sc.datasets.paul15(). > sc.pp.pca(somedata). > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). > sc.tl.leiden(somedata, resolution=0.5, random_state=42). > z = np.abs(somedata.obsm['X_pca'][:,0])**1. > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). >. > I get the following two figure as output. > [image: umapcontinuous_expr]. > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>. > [image: umapgroup_value]. > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>. >. > I would expect to see a similar size allocation/distribution but they are. > very different. I Could not really find a cause for this looking at the. > scatter plot function so it might be somewhere deeper. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/478>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1daoQkAExgeaLnSwdvtGUfQ4JFKyks5vMkwGgaJpZM4a1nTD>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:25,usability,help,help,25,"great, thank you for the help @fidelram",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/479:66,availability,error,errors,66,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:1050,deployability,API,APIs,1050,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:484,energy efficiency,current,current,484,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:131,integrability,interfac,interface,131,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:467,integrability,abstract,abstract,467,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:1050,integrability,API,APIs,1050,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:131,interoperability,interfac,interface,131,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:1050,interoperability,API,APIs,1050,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:131,modifiability,interfac,interface,131,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:467,modifiability,abstract,abstract,467,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:909,modifiability,variab,variable,909,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:66,performance,error,errors,66,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:45,reliability,pra,practice,45,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:66,safety,error,errors,66,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:1084,safety,input,input,1084,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:66,usability,error,errors,66,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:121,usability,support,supported,121,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:804,usability,close,closest,804,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:1084,usability,input,input,1084,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python. adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression"". sc.pl.pca(adata, var_key=""gex""). sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). # This also has the nice feature that it could abstract out the current `use_highly_variable` argument. ```. View based:. ```python. gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]. sc.pp.pca(gex_view) # Calculate pca on gene expression. sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]). ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:29,usability,multi-mod,multi-modal,29,Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:50,usability,tool,tool,50,Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,deployability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,integrability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,interoperability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,modifiability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,reliability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,security,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:257,testability,integr,integrated,257,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:30,usability,multi-mod,multi-modal,30,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:51,usability,tool,tool,51,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:. https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:59,interoperability,specif,specific,59,"We are trying to develop new scverse packages which handle specific modalities such as [scirpy](https://github.com/scverse/scirpy) for AIRR, [muon](https://github.com/scverse/muon) for ATAC/CITE, and [squidpy](https://github.com/scverse/squidpy) for spatial data",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:37,modifiability,pac,packages,37,"We are trying to develop new scverse packages which handle specific modalities such as [scirpy](https://github.com/scverse/scirpy) for AIRR, [muon](https://github.com/scverse/muon) for ATAC/CITE, and [squidpy](https://github.com/scverse/squidpy) for spatial data",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/480:247,availability,cluster,clusters,247,"Thanks for reporting. I will take a look. On Wed, Feb 13, 2019 at 2:19 AM tsotnech <notifications@github.com> wrote:. > Hi guys,. >. > sorry for opening so many issues, I really love using tracksplot for. > visualizing the expression in different clusters, but the small issue I. > have is that tracks actually never align well with the clusters ""timeline"". > at the bottom. especially the first and last tracks are always misaligned. > I couldn't find the list of arguments that I can pass through. > sc.pl.tracksplot command maybe just changing margins or size of the plot. > might make it better. > [image: tracksplotin_final]. > <https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png>. >. > in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline. > below. >. > Thanks a lot,. >. > Tsotne. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/480>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Zm16jiUoHyCrqHwaX6yuA2EcnT_ks5vM2gxgaJpZM4a4ZBv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:337,availability,cluster,clusters,337,"Thanks for reporting. I will take a look. On Wed, Feb 13, 2019 at 2:19 AM tsotnech <notifications@github.com> wrote:. > Hi guys,. >. > sorry for opening so many issues, I really love using tracksplot for. > visualizing the expression in different clusters, but the small issue I. > have is that tracks actually never align well with the clusters ""timeline"". > at the bottom. especially the first and last tracks are always misaligned. > I couldn't find the list of arguments that I can pass through. > sc.pl.tracksplot command maybe just changing margins or size of the plot. > might make it better. > [image: tracksplotin_final]. > <https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png>. >. > in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline. > below. >. > Thanks a lot,. >. > Tsotne. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/480>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Zm16jiUoHyCrqHwaX6yuA2EcnT_ks5vM2gxgaJpZM4a4ZBv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
